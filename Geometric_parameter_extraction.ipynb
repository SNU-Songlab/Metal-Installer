{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#Download the PDB listed in the Excel file\n",
        "import os\n",
        "import pandas as pd\n",
        "import requests\n",
        "\n",
        "# Function to download PDB file\n",
        "def download_pdb(pdb_code, save_directory):\n",
        "    download_url = f\"https://files.rcsb.org/download/{pdb_code}.pdb\"\n",
        "    response = requests.get(download_url)\n",
        "    if response.status_code == 200:\n",
        "        save_path = os.path.join(save_directory, f\"{pdb_code}.pdb\")\n",
        "        with open(save_path, \"wb\") as file:\n",
        "            file.write(response.content)\n",
        "        print(f\"Downloaded {pdb_code}.pdb\")\n",
        "        return True\n",
        "    else:\n",
        "        print(f\"Failed to download {pdb_code}.pdb\")\n",
        "        return False\n",
        "\n",
        "# Read the Excel file with the PDB codes\n",
        "excel_file = \"L:/Zn-installer_rawdata/241111_Mn_Final/Mn.xlsx\"\n",
        "df = pd.read_excel(excel_file)\n",
        "\n",
        "# Get the PDB codes from the Excel file\n",
        "pdb_codes = df[\"PDB ID\"].tolist()\n",
        "\n",
        "# Directory to save the downloaded PDB files\n",
        "pdb_directory = \"L:/Zn-installer_rawdata/241111_Mn_Final/\"\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "os.makedirs(pdb_directory, exist_ok=True)\n",
        "\n",
        "# List to store the PDB IDs for failed downloads\n",
        "failed_pdb_ids = []\n",
        "\n",
        "# Iterate over the PDB codes and download the structures\n",
        "for pdb_code in pdb_codes:\n",
        "    if not download_pdb(pdb_code, pdb_directory):\n",
        "        failed_pdb_ids.append(pdb_code)\n",
        "\n",
        "# Save the failed PDB IDs to an Excel file\n",
        "failed_df = pd.DataFrame({\"PDB ID\": failed_pdb_ids})\n",
        "failed_excel_file = \"L:/Zn-installer_rawdata/241111_Mn_Final/Mn_failed.xlsx\"\n",
        "failed_df.to_excel(failed_excel_file, index=False)\n",
        "print(f\"Failed PDB IDs saved to {failed_excel_file}\")"
      ],
      "metadata": {
        "id": "khhBwlfvY0QR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Classify the PDB Files based on the number of manganese ions in the assymetric units\n",
        "import os\n",
        "import pandas as pd\n",
        "from Bio.PDB import PDBParser\n",
        "from collections import defaultdict\n",
        "import shutil\n",
        "\n",
        "def categorize_metal_atoms(filename):\n",
        "    pdb_id = os.path.basename(filename).split('.')[0]\n",
        "    manganese_atoms = []\n",
        "    other_metal_atoms = []\n",
        "\n",
        "    parser = PDBParser(QUIET=True)\n",
        "    structure = parser.get_structure(pdb_id, filename)\n",
        "\n",
        "    # Store Mn and other metal atoms\n",
        "    for model in structure:\n",
        "        for chain in model:\n",
        "            for residue in chain:\n",
        "                for atom in residue:\n",
        "                    element_symbol = atom.element\n",
        "                    if element_symbol == 'MN':\n",
        "                        manganese_atoms.append(atom)\n",
        "                    elif element_symbol in ['ZN', 'MG', 'CO', 'CA', 'FE', 'PT', 'NA', 'K', 'LI', 'CD', 'YB', 'NI', 'PR', 'HG', 'MN']:\n",
        "                        other_metal_atoms.append(atom)\n",
        "\n",
        "    return manganese_atoms, other_metal_atoms\n",
        "\n",
        "def save_metal_atoms_to_excel(directory):\n",
        "    pdb_files = [f for f in os.listdir(directory) if f.endswith('.pdb')]\n",
        "    all_mn_atoms = []\n",
        "    all_other_metals = []\n",
        "\n",
        "    for pdb_file in pdb_files:\n",
        "        pdb_file_path = os.path.join(directory, pdb_file)\n",
        "        mn_atoms, other_metals = categorize_metal_atoms(pdb_file_path)\n",
        "        all_mn_atoms.extend([(pdb_file.split('.')[0], 'MN')] * len(mn_atoms))\n",
        "        all_other_metals.extend([(pdb_file.split('.')[0], atom.element) for atom in other_metals])\n",
        "\n",
        "    df_mn_atoms = pd.DataFrame(all_mn_atoms, columns=['PDB ID', 'Name of metal atoms'])\n",
        "    df_other_metals = pd.DataFrame(all_other_metals, columns=['PDB ID', 'Name of metal atoms'])\n",
        "\n",
        "    output_file_mn_atoms = os.path.join(directory, 'mn_atoms2.xlsx')\n",
        "    output_file_other_metals = os.path.join(directory, 'other_metals2.xlsx')\n",
        "\n",
        "    df_mn_atoms.to_excel(output_file_mn_atoms, index=False)\n",
        "    df_other_metals.to_excel(output_file_other_metals, index=False)\n",
        "\n",
        "    print(f\"MN atoms categorized have been saved to '{output_file_mn_atoms}'.\")\n",
        "    print(f\"Other metals categorized have been saved to '{output_file_other_metals}'.\")\n",
        "\n",
        "    return output_file_mn_atoms\n",
        "\n",
        "def count_and_categorize_mn_atoms(excel_file, pdb_directory):\n",
        "    df = pd.read_excel(excel_file)\n",
        "    mn_counts = defaultdict(int)\n",
        "\n",
        "    for index, row in df.iterrows():\n",
        "        pdb_id = row['PDB ID']\n",
        "        metal_atom_name = row['Name of metal atoms']\n",
        "\n",
        "        if metal_atom_name == 'MN':\n",
        "            mn_counts[pdb_id] += 1\n",
        "\n",
        "    mn_counts_dict = dict(mn_counts)\n",
        "    result_df = pd.DataFrame(list(mn_counts_dict.items()), columns=['PDB ID', 'Metal Count'])\n",
        "    output_excel = os.path.join(pdb_directory, 'mn_count_results.xlsx')\n",
        "    result_df.to_excel(output_excel, index=False)\n",
        "    print(f\"Metal atom counts have been saved to '{output_excel}'.\")\n",
        "\n",
        "    # Categorize and copy PDB files based on the number of MN atoms\n",
        "    for pdb_id, count in mn_counts_dict.items():\n",
        "        pdb_file = os.path.join(pdb_directory, f\"{pdb_id}.pdb\")\n",
        "        if not os.path.isfile(pdb_file):\n",
        "            print(f\"Warning: PDB file {pdb_file} not found.\")\n",
        "            continue\n",
        "\n",
        "        if count == 1:\n",
        "            destination_dir = os.path.join(pdb_directory, 'metal_count_1_mn2')\n",
        "        elif count > 1:\n",
        "            destination_dir = os.path.join(pdb_directory, 'metal_count_greater_than_1_mn2')\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "        os.makedirs(destination_dir, exist_ok=True)\n",
        "        shutil.copy(pdb_file, os.path.join(destination_dir, f\"{pdb_id}.pdb\"))\n",
        "        print(f\"File '{pdb_id}.pdb' copied to '{destination_dir}'.\")\n",
        "\n",
        "# Specify the directories\n",
        "pdb_directory = 'L:/Zn-installer_rawdata/241111_Mn_Final/Final_3/'\n",
        "\n",
        "# Save categorized metal atoms to Excel and get the path of the Excel file\n",
        "mn_atoms_excel_file = save_metal_atoms_to_excel(pdb_directory)\n",
        "\n",
        "# Count Mn atoms based on the Excel file and categorize PDB files\n",
        "count_and_categorize_mn_atoms(mn_atoms_excel_file, pdb_directory)"
      ],
      "metadata": {
        "id": "xLNXVrdHRtmh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Classification of mononuclear from heteronuclear proteins\n",
        "import os\n",
        "import shutil\n",
        "import pandas as pd\n",
        "from Bio.PDB import PDBParser\n",
        "\n",
        "def calculate_distance(atom1, atom2):\n",
        "    # Calculate the Euclidean distance between two atoms\n",
        "    x1, y1, z1 = atom1.coord\n",
        "    x2, y2, z2 = atom2.coord\n",
        "    distance = ((x1 - x2)**2 + (y1 - y2)**2 + (z1 - z2)**2)**0.5\n",
        "    return distance\n",
        "\n",
        "def categorize_pdb_files(directory, distance_threshold=5.0):\n",
        "    mono_hetero_results = []\n",
        "    mono_results = []\n",
        "\n",
        "    pdb_files = [f for f in os.listdir(directory) if f.endswith('.pdb')]\n",
        "    parser = PDBParser(QUIET=True)\n",
        "\n",
        "    for pdb_file in pdb_files:\n",
        "        try:\n",
        "            pdb_id = pdb_file.split('.')[0]\n",
        "            structure = parser.get_structure(pdb_id, os.path.join(directory, pdb_file))\n",
        "\n",
        "            # Store Mn and other metal atoms\n",
        "            manganese_atoms = []\n",
        "            other_metal_atoms = []\n",
        "            has_other_metals_near_manganese = False\n",
        "\n",
        "            for model in structure:\n",
        "                for chain in model:\n",
        "                    for residue in chain:\n",
        "                        for atom in residue:\n",
        "                            element_symbol = atom.element\n",
        "                            if element_symbol == 'MN':\n",
        "                                manganese_atoms.append(atom)\n",
        "                            elif element_symbol in ['ZN', 'MG', 'CO', 'CA', 'FE', 'PT', 'NA', 'K', 'LI', 'CD', 'YB', 'NI', 'PR', 'HG', 'MN']:\n",
        "                                other_metal_atoms.append(atom)\n",
        "\n",
        "            # If exactly one Mn atom, check for proximity to other metals\n",
        "            if len(manganese_atoms) == 1:\n",
        "                manganese_atom = manganese_atoms[0]\n",
        "                for metal_atom in other_metal_atoms:\n",
        "                    distance = calculate_distance(manganese_atom, metal_atom)\n",
        "                    if distance <= distance_threshold:\n",
        "                        mono_hetero_results.append(pdb_id)\n",
        "                        has_other_metals_near_manganese = True\n",
        "                        break  # No need to check further, already categorized as 'mono-hetero'\n",
        "\n",
        "            if not has_other_metals_near_manganese:\n",
        "                mono_results.append(pdb_id)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {pdb_file}: {e}\")\n",
        "\n",
        "    # Save results to separate Excel files\n",
        "    save_results(mono_hetero_results, mono_results, directory)\n",
        "\n",
        "def save_results(mono_hetero_results, mono_results, directory):\n",
        "    \"\"\"Save categorization results to Excel files and copy PDB files.\"\"\"\n",
        "    # Set up output directories\n",
        "    output_dir_mono_hetero = os.path.join(directory, 'mono_hetero2')\n",
        "    output_dir_mono = os.path.join(directory, 'mono2')\n",
        "\n",
        "    try:\n",
        "        # Create directories if they do not exist\n",
        "        os.makedirs(output_dir_mono_hetero, exist_ok=True)\n",
        "        os.makedirs(output_dir_mono, exist_ok=True)\n",
        "\n",
        "        # Save results to Excel files\n",
        "        df_mono_hetero = pd.DataFrame(mono_hetero_results, columns=['PDB ID'])\n",
        "        df_mono = pd.DataFrame(mono_results, columns=['PDB ID'])\n",
        "\n",
        "        df_mono_hetero.to_excel(os.path.join(output_dir_mono_hetero, 'mono_hetero2.xlsx'), index=False)\n",
        "        df_mono.to_excel(os.path.join(output_dir_mono, 'mono2.xlsx'), index=False)\n",
        "\n",
        "        # Copy corresponding PDB files to directories\n",
        "        for pdb_id in mono_hetero_results:\n",
        "            src_file = os.path.join(directory, f\"{pdb_id}.pdb\")\n",
        "            if os.path.isfile(src_file):\n",
        "                shutil.copy(src_file, os.path.join(output_dir_mono_hetero, f\"{pdb_id}.pdb\"))\n",
        "\n",
        "        for pdb_id in mono_results:\n",
        "            src_file = os.path.join(directory, f\"{pdb_id}.pdb\")\n",
        "            if os.path.isfile(src_file):\n",
        "                shutil.copy(src_file, os.path.join(output_dir_mono, f\"{pdb_id}.pdb\"))\n",
        "\n",
        "        print(\"PDB files have been copied to the corresponding folders.\")\n",
        "        print(f\"Results categorized as 'mono_hetero' have been saved to '{output_dir_mono_hetero}/mono_hetero2.xlsx'.\")\n",
        "        print(f\"Results categorized as 'mono' have been saved to '{output_dir_mono}/mono2.xlsx'.\")\n",
        "\n",
        "    except PermissionError as e:\n",
        "        print(f\"PermissionError: {e}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating directories or saving results: {e}\")\n",
        "\n",
        "# Specify the directory containing the PDB files\n",
        "pdb_directory = 'J:/Zn-installer_rawdata/241111_Mn_Final/Final_3/metal_count_1/'\n",
        "\n",
        "# Categorize and copy PDB files based on the presence of Mn and other metals within 5 Å\n",
        "categorize_pdb_files(pdb_directory)"
      ],
      "metadata": {
        "id": "UojNvMouY2XZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Extract the mononuclear with (M>1, extract from hetero and homonuclear proteins)\n",
        "import os\n",
        "import shutil\n",
        "import pandas as pd\n",
        "from Bio.PDB import PDBParser\n",
        "import numpy as np\n",
        "\n",
        "# Function to calculate the distance between two atoms\n",
        "def calculate_distance(atom1, atom2):\n",
        "    return np.linalg.norm(atom1.get_coord() - atom2.get_coord())\n",
        "\n",
        "# Function to find and categorize Mn atoms based on their proximity to other metal ions (including other Mn atoms)\n",
        "def categorize_pdb_files(directory, distance_threshold=5.0):\n",
        "    manganese_ion = 'MN'\n",
        "    other_metals = ['CU', 'MG', 'CO', 'CA', 'FE', 'PT', 'NA', 'K ', 'LI', 'CD', 'YB', 'NI', 'PR', 'HG', 'MN', 'ZN']  # Include Mn\n",
        "\n",
        "    all_isolated_results = []\n",
        "    mixed_results = []\n",
        "    other_results = []\n",
        "\n",
        "    pdb_files = [f for f in os.listdir(directory) if f.endswith('.pdb')]\n",
        "    parser = PDBParser(QUIET=True)\n",
        "\n",
        "    for pdb_file in pdb_files:\n",
        "        try:\n",
        "            pdb_id = pdb_file.split('.')[0]\n",
        "            structure = parser.get_structure(pdb_id, os.path.join(directory, pdb_file))\n",
        "            manganese_atoms = []\n",
        "            other_metal_atoms = []\n",
        "\n",
        "            # Collect all Mn atoms and other metal atoms in the structure\n",
        "            for model in structure:\n",
        "                for chain in model:\n",
        "                    for residue in chain:\n",
        "                        for atom in residue:\n",
        "                            element_symbol = atom.element.strip()  # Strip leading/trailing spaces to ensure proper comparison\n",
        "\n",
        "                            if element_symbol == manganese_ion:\n",
        "                                manganese_atoms.append(atom)\n",
        "                            elif element_symbol in other_metals:\n",
        "                                other_metal_atoms.append(atom)\n",
        "\n",
        "            # Flag to track isolated Mn atoms\n",
        "            isolated_mn_count = 0\n",
        "\n",
        "            # Check if each Mn atom has any other metal ions (including other Mn atoms) within the distance threshold\n",
        "            for manganese_atom in manganese_atoms:\n",
        "                is_isolated = True  # Assume the Mn atom is isolated unless proven otherwise\n",
        "                for metal_atom in (manganese_atoms + other_metal_atoms):\n",
        "                    if metal_atom != manganese_atom:  # Exclude the Mn atom itself from comparison\n",
        "                        distance = calculate_distance(manganese_atom, metal_atom)\n",
        "                        if distance <= distance_threshold:\n",
        "                            is_isolated = False  # Mn atom is not isolated\n",
        "                            break\n",
        "\n",
        "                if is_isolated:\n",
        "                    isolated_mn_count += 1\n",
        "\n",
        "            # Classify the file based on the number of isolated Mn atoms\n",
        "            if isolated_mn_count == len(manganese_atoms):\n",
        "                all_isolated_results.append(pdb_id)  # All Mn atoms are isolated\n",
        "                print(f\"{pdb_id} classified as all_isolated.\")\n",
        "            elif isolated_mn_count > 0:\n",
        "                mixed_results.append(pdb_id)  # Mix of isolated and non-isolated Mn atoms\n",
        "                print(f\"{pdb_id} classified as mixed.\")\n",
        "            else:\n",
        "                other_results.append(pdb_id)  # No isolated Mn atoms\n",
        "                print(f\"{pdb_id} classified as other.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {pdb_file}: {e}\")\n",
        "\n",
        "    # Save results\n",
        "    save_results(all_isolated_results, mixed_results, other_results, directory)\n",
        "\n",
        "# Function to save categorized results to Excel and copy PDB files\n",
        "def save_results(all_isolated_results, mixed_results, other_results, directory):\n",
        "    # Create directories for storing results\n",
        "    output_dir_all_isolated = os.path.join(directory, 'all_isolated')\n",
        "    output_dir_mixed = os.path.join(directory, 'mixed')\n",
        "    output_dir_other = os.path.join(directory, 'other')\n",
        "\n",
        "    os.makedirs(output_dir_all_isolated, exist_ok=True)\n",
        "    os.makedirs(output_dir_mixed, exist_ok=True)\n",
        "    os.makedirs(output_dir_other, exist_ok=True)\n",
        "\n",
        "    # Save results to Excel files\n",
        "    pd.DataFrame(all_isolated_results, columns=['PDB ID']).to_excel(os.path.join(output_dir_all_isolated, 'all_isolated.xlsx'), index=False)\n",
        "    pd.DataFrame(mixed_results, columns=['PDB ID']).to_excel(os.path.join(output_dir_mixed, 'mixed.xlsx'), index=False)\n",
        "    pd.DataFrame(other_results, columns=['PDB ID']).to_excel(os.path.join(output_dir_other, 'other.xlsx'), index=False)\n",
        "\n",
        "    # Copy corresponding PDB files to their respective directories\n",
        "    for pdb_id in all_isolated_results:\n",
        "        src_file = os.path.join(directory, f\"{pdb_id}.pdb\")\n",
        "        if os.path.isfile(src_file):\n",
        "            shutil.copy(src_file, output_dir_all_isolated)\n",
        "\n",
        "    for pdb_id in mixed_results:\n",
        "        src_file = os.path.join(directory, f\"{pdb_id}.pdb\")\n",
        "        if os.path.isfile(src_file):\n",
        "            shutil.copy(src_file, output_dir_mixed)\n",
        "\n",
        "    for pdb_id in other_results:\n",
        "        src_file = os.path.join(directory, f\"{pdb_id}.pdb\")\n",
        "        if os.path.isfile(src_file):\n",
        "            shutil.copy(src_file, output_dir_other)\n",
        "\n",
        "    print(f\"Results saved: {len(all_isolated_results)} in 'all_isolated', {len(mixed_results)} in 'mixed', {len(other_results)} in 'other'.\")\n",
        "\n",
        "# Main script execution\n",
        "pdb_directory = 'J:/Zn-installer_rawdata/241111_Mn_Final/Final_3/metal_count_greater_than_1/'\n",
        "categorize_pdb_files(pdb_directory)"
      ],
      "metadata": {
        "id": "gRa2ywmwY_6L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Extract the mononuclear binding sites from homo and hetero nuclear\n",
        "import os\n",
        "import shutil\n",
        "import pandas as pd\n",
        "from Bio.PDB import PDBParser, PDBIO, Select\n",
        "import numpy as np\n",
        "\n",
        "# Function to calculate the distance between two atoms\n",
        "def calculate_distance(atom1, atom2):\n",
        "    return np.linalg.norm(atom1.get_coord() - atom2.get_coord())\n",
        "\n",
        "# Function to process mixed PDB files and remove non-isolated Mn atoms\n",
        "def process_mixed_pdb_files(directory, distance_threshold=5.0):\n",
        "    manganese_ion = 'MN'\n",
        "    other_metals = ['CU', 'MG', 'CO', 'CA', 'FE', 'PT', 'NA', 'K ', 'LI', 'CD', 'YB', 'NI', 'PR', 'HG', 'MN', 'ZN']  # Include Mn\n",
        "\n",
        "    pdb_files = [f for f in os.listdir(directory) if f.endswith('.pdb')]\n",
        "    parser = PDBParser(QUIET=True)\n",
        "\n",
        "    for pdb_file in pdb_files:\n",
        "        pdb_id = pdb_file.split('.')[0]\n",
        "        pdb_file_path = os.path.join(directory, pdb_file)\n",
        "        try:\n",
        "            structure = parser.get_structure(pdb_id, pdb_file_path)\n",
        "            manganese_atoms = []\n",
        "            other_metal_atoms = []\n",
        "\n",
        "            # Collect all Mn atoms and other metal atoms in the structure\n",
        "            for model in structure:\n",
        "                for chain in model:\n",
        "                    for residue in chain:\n",
        "                        for atom in residue:\n",
        "                            element_symbol = atom.element.strip()  # Strip leading/trailing spaces to ensure proper comparison\n",
        "\n",
        "                            if element_symbol == manganese_ion:\n",
        "                                manganese_atoms.append(atom)\n",
        "                            elif element_symbol in other_metals:\n",
        "                                other_metal_atoms.append(atom)\n",
        "\n",
        "            # Collect isolated Mn atoms\n",
        "            isolated_manganese_atoms = []\n",
        "\n",
        "            for manganese_atom in manganese_atoms:\n",
        "                is_isolated = True  # Assume the Mn atom is isolated unless proven otherwise\n",
        "                for metal_atom in (manganese_atoms + other_metal_atoms):\n",
        "                    if metal_atom != manganese_atom:  # Exclude the Mn atom itself from comparison\n",
        "                        distance = calculate_distance(manganese_atom, metal_atom)\n",
        "                        if distance <= distance_threshold:\n",
        "                            is_isolated = False  # Mn atom is not isolated\n",
        "                            break\n",
        "\n",
        "                if is_isolated:\n",
        "                    isolated_manganese_atoms.append(manganese_atom)\n",
        "\n",
        "            # Save PDB file with only isolated Mn atoms\n",
        "            if isolated_manganese_atoms:\n",
        "                io = PDBIO()\n",
        "                io.set_structure(structure)\n",
        "                io.save(os.path.join(directory, f\"{pdb_id}.pdb\"), select=IsolatedMnSelector(isolated_manganese_atoms))\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {pdb_file}: {e}\")\n",
        "\n",
        "# Selector class to filter isolated Mn atoms\n",
        "class IsolatedMnSelector(Select):\n",
        "    def __init__(self, isolated_atoms):\n",
        "        self.isolated_atoms = isolated_atoms\n",
        "\n",
        "    def accept_atom(self, atom):\n",
        "        return atom in self.isolated_atoms or atom.element.strip() != 'MN'\n",
        "\n",
        "# Main script execution\n",
        "pdb_directory = 'J:/Zn-installer_rawdata/241111_Mn_Final/Final_3/metal_count_greater_than_1/mixed/'\n",
        "process_mixed_pdb_files(pdb_directory)\n"
      ],
      "metadata": {
        "id": "JN334SJt8BWD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from Bio import PDB\n",
        "import numpy as np\n",
        "import logging\n",
        "\n",
        "# --- Constants ---\n",
        "# Input/Output Configuration\n",
        "PDB_DIRECTORY = \"D:/250414_Final/Mn/2His_1Asp\"\n",
        "OUTPUT_DIR = \"D:/250414_Final/Mn/2His_1Asp\"\n",
        "# Changed prefix to indicate the format change\n",
        "OUTPUT_FILENAME_PREFIX = \"original_format_output_\"\n",
        "\n",
        "# Biochemical Constants\n",
        "METAL_ID = 'MN'\n",
        "ALLOWED_RESIDUES = {\n",
        "    'ALA', 'ARG', 'ASN', 'ASP', 'CYS', 'GLN', 'GLU', 'GLY', 'HIS',\n",
        "    'LYS', 'MET', 'PHE', 'SER', 'THR', 'TYR', 'VAL'\n",
        "}\n",
        "\n",
        "# Distance thresholds (in Angstroms) for MN coordination [min_dist, max_dist]\n",
        "# Using the dictionary provided in the script\n",
        "MN_DISTANCE_THRESHOLDS = {\n",
        "    'HIS_ND1': (0, 2.75), 'HIS_NE2': (0, 2.75), 'HIS_N': (0, 2.75), 'HIS_O': (0, 2.85),\n",
        "    'GLU_OE1': (0, 2.85), 'GLU_OE2': (0, 2.85), 'GLU_N': (0, 2.75), 'GLU_O': (0, 2.85),\n",
        "    'ASP_OD1': (0, 2.85), 'ASP_OD2': (0, 2.85), 'ASP_N': (0, 2.75), 'ASP_O': (0, 2.85),\n",
        "    'ALA_N': (0, 2.75),   'ALA_O': (0, 2.85),\n",
        "    'CYS_SG': (0, 2.75),\n",
        "    'MET_SD': (0, 2.75),\n",
        "    'ARG_NH1': (0, 2.75), 'ARG_NH2': (0, 2.75), 'ARG_NE': (0, 2.75), 'ARG_N': (0, 2.75), 'ARG_O': (0, 2.85),\n",
        "    'ASN_OD1': (0, 2.85), 'ASN_ND2': (0, 2.75), 'ASN_N': (0, 2.75), 'ASN_O': (0, 2.85),\n",
        "    'GLN_OE1': (0, 2.85), 'GLN_NE2': (0, 2.75), 'GLN_N': (0, 2.75), 'GLN_O': (0, 2.85),\n",
        "    'GLY_N': (0, 2.75),   'GLY_O': (0, 2.85),\n",
        "    'LYS_NZ': (0, 2.75),  'LYS_N': (0, 2.75), 'LYS_O': (0, 2.85),\n",
        "    'SER_OG': (0, 2.85),  'SER_N': (0, 2.75), 'SER_O': (0, 2.85),\n",
        "    'THR_OG1': (0, 2.85), 'THR_N': (0, 2.75), 'THR_O': (0, 2.85),\n",
        "    'TYR_OH': (0, 2.85),  'TYR_N': (0, 2.75), 'TYR_O': (0, 2.85),\n",
        "}\n",
        "\n",
        "# Define base columns matching the desired \"original\" output structure (image)\n",
        "# 8 columns before binding atoms start\n",
        "BASE_OUTPUT_COLUMNS = [\n",
        "    'Entry ID', 'PDB ID', 'Metal Chain ID', 'Metal Residue number', 'Metal',\n",
        "    'Chain ID', 'Residue number', 'Residue name' # These refer to the specific ligand in the row\n",
        "]\n",
        "\n",
        "# --- Helper Functions (Keep as they are in the provided script) ---\n",
        "\n",
        "def calculate_distance(atom1, atom2):\n",
        "    \"\"\"Calculates the Euclidean distance between two Bio.PDB.Atom objects.\"\"\"\n",
        "    try:\n",
        "        coord1 = atom1.get_coord()\n",
        "        coord2 = atom2.get_coord()\n",
        "        diff = coord1 - coord2\n",
        "        return np.sqrt(np.sum(diff * diff))\n",
        "    except Exception as e:\n",
        "        # It's often better to log the pdb_id here if possible, but it's not passed directly\n",
        "        logging.error(f\"Error calculating distance between {atom1} and {atom2}: {e}\")\n",
        "        return float('inf') # Return infinity if coordinates are bad\n",
        "\n",
        "def get_distance_threshold(residue_name, atom_name, thresholds):\n",
        "    \"\"\"\n",
        "    Gets the distance threshold tuple (min, max) for a given residue and atom.\n",
        "    Handles specific cases like ASP/GLU carboxylates and CYS/MET sulfurs based on provided dict keys.\n",
        "    \"\"\"\n",
        "    # This function attempts to match the logic needed for the provided MN_DISTANCE_THRESHOLDS dict\n",
        "    # Check for specific atom keys first\n",
        "    specific_key = f\"{residue_name}_{atom_name}\"\n",
        "    if specific_key in thresholds:\n",
        "        return thresholds[specific_key]\n",
        "\n",
        "    # Handle potential combined carboxylate/amide oxygens if specific keys aren't present\n",
        "    # Note: The provided dict HAS specific keys like GLU_OE1, GLU_OE2, so this fallback might not be needed\n",
        "    # if residue_name == 'GLU' and atom_name in ['OE1', 'OE2'] and 'GLU_OE' in thresholds:\n",
        "    #     return thresholds['GLU_OE']\n",
        "    # if residue_name == 'ASP' and atom_name in ['OD1', 'OD2'] and 'ASP_OD' in thresholds:\n",
        "    #     return thresholds['ASP_OD']\n",
        "\n",
        "    # Handle specific sulfur atoms (assuming keys like CYS_SG, MET_SD exist)\n",
        "    if residue_name == 'CYS' and atom_name == 'SG' and 'CYS_SG' in thresholds:\n",
        "         return thresholds['CYS_SG']\n",
        "    if residue_name == 'MET' and atom_name == 'SD' and 'MET_SD' in thresholds:\n",
        "         return thresholds['MET_SD']\n",
        "\n",
        "    # Fallback if no specific or grouped key is found\n",
        "    return thresholds.get(specific_key, (0, 0))\n",
        "\n",
        "\n",
        "# --- MODIFIED Core Logic Function ---\n",
        "# Renamed to avoid confusion, but uses the same internal logic as the provided script's find_coordination_sites\n",
        "# The CHANGE is in how the final data is structured and returned.\n",
        "def find_coordination_sites_for_original_format(structure, pdb_id, metal_atom_name, thresholds, allowed_ligands):\n",
        "    \"\"\"\n",
        "    Identifies metal coordination sites using the provided script's logic,\n",
        "    but formats the output as one row per metal-ligand pair.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary where keys are coordination numbers and values are lists\n",
        "        of tuples. Each tuple represents ONE metal-ligand interaction:\n",
        "        (entry_id, pdb_code, metal_chain, metal_resnum, metal_name,\n",
        "         ligand_chain, ligand_resnum, ligand_name, atom1, atom2, ...)\n",
        "    \"\"\"\n",
        "    # This dictionary will store {coord_num: [list_of_metal_ligand_tuples]}\n",
        "    coordination_sites_by_number = {}\n",
        "\n",
        "    # Iterate through the structure to find metal ions\n",
        "    for model in structure:\n",
        "        for chain in model:\n",
        "            for metal_residue in chain:\n",
        "                if metal_residue.get_resname() == metal_atom_name:\n",
        "                    try:\n",
        "                        metal_atom = metal_residue[metal_atom_name]\n",
        "                    except KeyError:\n",
        "                        logging.warning(f\"Metal residue {metal_residue.id} in {pdb_id} \"\n",
        "                                        f\"doesn't contain a '{metal_atom_name}' atom. Skipping.\")\n",
        "                        continue\n",
        "\n",
        "                    metal_chain_id = metal_residue.get_parent().id\n",
        "                    metal_res_num = metal_residue.id[1]\n",
        "                    metal_name = metal_residue.get_resname()\n",
        "                    # Use .get() for safer header access, fallback to pdb_id\n",
        "                    pdb_code = structure.header.get('idcode', pdb_id)\n",
        "\n",
        "                    # Store info about ligands coordinating *this specific* metal ion\n",
        "                    # Using a dictionary keyed by residue ID to handle duplicates across models easily\n",
        "                    coordinating_ligands_details = {} # {(chain, resnum): {ligand_info..., \"binding_atoms\": [...]}}\n",
        "\n",
        "                    # Now, iterate through all potential ligands for this metal ion\n",
        "                    for ligand_model in structure:\n",
        "                        for ligand_chain in ligand_model:\n",
        "                            for ligand_residue in ligand_chain:\n",
        "                                # Skip if it's the metal itself or not an allowed residue type\n",
        "                                if ligand_residue == metal_residue or ligand_residue.get_resname() not in allowed_ligands:\n",
        "                                    continue\n",
        "\n",
        "                                coordinating_atoms_in_ligand = [] # Atoms in *this* ligand coordinating the *current* metal\n",
        "                                ligand_res_name = ligand_residue.get_resname()\n",
        "                                ligand_chain_id = ligand_chain.id\n",
        "                                ligand_res_num = ligand_residue.id[1]\n",
        "                                residue_unique_id = (ligand_chain_id, ligand_res_num)\n",
        "\n",
        "                                # Check each atom in the potential ligand residue\n",
        "                                for ligand_atom in ligand_residue:\n",
        "                                    ligand_atom_name = ligand_atom.get_id()\n",
        "                                    min_dist, max_dist = get_distance_threshold(ligand_res_name, ligand_atom_name, thresholds)\n",
        "\n",
        "                                    if max_dist <= 0: continue # Skip if no valid threshold\n",
        "\n",
        "                                    distance = calculate_distance(metal_atom, ligand_atom)\n",
        "\n",
        "                                    if min_dist <= distance <= max_dist:\n",
        "                                        coordinating_atoms_in_ligand.append(ligand_atom_name)\n",
        "\n",
        "                                # If any atom in this ligand coordinated, store/update its info\n",
        "                                if coordinating_atoms_in_ligand:\n",
        "                                    if residue_unique_id not in coordinating_ligands_details:\n",
        "                                        coordinating_ligands_details[residue_unique_id] = {\n",
        "                                            \"ligand_chain_id\": ligand_chain_id,\n",
        "                                            \"ligand_res_num\": ligand_res_num,\n",
        "                                            \"ligand_res_name\": ligand_res_name,\n",
        "                                            \"binding_atoms\": set(coordinating_atoms_in_ligand) # Use a set initially\n",
        "                                        }\n",
        "                                    else:\n",
        "                                        # Add any newly found coordinating atoms for this residue\n",
        "                                        coordinating_ligands_details[residue_unique_id][\"binding_atoms\"].update(coordinating_atoms_in_ligand)\n",
        "\n",
        "                    # --- Data Formatting Change Starts Here ---\n",
        "                    # After checking all ligands for this metal ion, determine coordination number\n",
        "                    coordination_number = len(coordinating_ligands_details)\n",
        "\n",
        "                    if coordination_number > 0:\n",
        "                        # Create the list of output tuples (one per ligand) for this metal\n",
        "                        output_rows_for_this_metal = []\n",
        "                        for ligand_details in coordinating_ligands_details.values():\n",
        "                            # Sort binding atoms alphabetically for consistent output\n",
        "                            sorted_binding_atoms = tuple(sorted(list(ligand_details[\"binding_atoms\"])))\n",
        "\n",
        "                            # Construct the tuple for this metal-ligand pair row\n",
        "                            row_tuple = (\n",
        "                                pdb_id,                   # Entry ID (from filename)\n",
        "                                pdb_code,                 # PDB ID (from header)\n",
        "                                metal_chain_id,           # Metal Chain ID\n",
        "                                metal_res_num,            # Metal Residue number\n",
        "                                metal_name,               # Metal Name\n",
        "                                ligand_details[\"ligand_chain_id\"],    # Ligand Chain ID\n",
        "                                ligand_details[\"ligand_res_num\"],     # Ligand Residue number\n",
        "                                ligand_details[\"ligand_res_name\"]     # Ligand Residue name\n",
        "                            ) + sorted_binding_atoms      # Add tuple of binding atoms\n",
        "\n",
        "                            output_rows_for_this_metal.append(row_tuple)\n",
        "\n",
        "                        # Store this list of rows under the coordination number\n",
        "                        if coordination_number not in coordination_sites_by_number:\n",
        "                            coordination_sites_by_number[coordination_number] = []\n",
        "                        coordination_sites_by_number[coordination_number].extend(output_rows_for_this_metal)\n",
        "                    # --- Data Formatting Change Ends Here ---\n",
        "\n",
        "    # Return the dictionary structured for the desired output format\n",
        "    return coordination_sites_by_number\n",
        "\n",
        "\n",
        "# --- MODIFIED Saving Function ---\n",
        "# Renamed function to clarify its purpose\n",
        "def save_coordination_data_as_original_format(all_coordination_data, output_dir, file_prefix, base_columns):\n",
        "    \"\"\"\n",
        "    Saves the aggregated coordination data using the original format\n",
        "    (one row per metal-ligand pair, dynamic binding atom columns).\n",
        "    \"\"\"\n",
        "    if not all_coordination_data:\n",
        "        logging.warning(\"No coordination data found to save.\")\n",
        "        return\n",
        "\n",
        "    for coordination_number, sites_list in all_coordination_data.items():\n",
        "        if not sites_list:\n",
        "            logging.info(f\"No sites found for coordination number {coordination_number}. Skipping file.\")\n",
        "            continue\n",
        "\n",
        "        logging.info(f\"Processing data for coordination number {coordination_number}...\")\n",
        "\n",
        "        # Remove exact duplicate rows before saving (often desirable)\n",
        "        # Sort first by all elements in tuple for consistent non-duplicate selection\n",
        "        unique_sites_list = sorted(list(set(sites_list)))\n",
        "\n",
        "        # Determine the maximum number of binding atoms listed for any single ligand in this group\n",
        "        num_base_cols = len(base_columns) # Should be 8\n",
        "        max_binding_atoms = 0\n",
        "        if unique_sites_list:\n",
        "            try:\n",
        "                # Calculate max length of the binding atom part of the tuples\n",
        "                max_binding_atoms = max(len(site_tuple) - num_base_cols for site_tuple in unique_sites_list)\n",
        "            except ValueError:\n",
        "                 logging.error(f\"Inconsistent tuple length found for coord_num {coordination_number}. Check data.\")\n",
        "                 max_binding_atoms = 0 # Default\n",
        "\n",
        "        # Define columns: base columns + Binding atomX columns\n",
        "        columns = list(base_columns)\n",
        "        columns.extend([f'Binding atom{i}' for i in range(1, max_binding_atoms + 1)])\n",
        "\n",
        "        # Create DataFrame using the unique list and defined columns\n",
        "        # .fillna(np.nan) handles ligands with fewer binding atoms than the max\n",
        "        try:\n",
        "             df_coordination = pd.DataFrame(unique_sites_list, columns=columns).fillna(np.nan)\n",
        "        except Exception as e:\n",
        "             # Log error and fallback to default columns if specific columns fail\n",
        "             logging.error(f\"Error creating DataFrame for coord num {coordination_number} with specific columns: {e}. Using default columns.\")\n",
        "             df_coordination = pd.DataFrame(unique_sites_list)\n",
        "\n",
        "\n",
        "        # Define output path and save\n",
        "        output_filename = f\"{file_prefix}{coordination_number}.xlsx\"\n",
        "        output_excel_path = os.path.join(output_dir, output_filename)\n",
        "\n",
        "        try:\n",
        "            df_coordination.to_excel(output_excel_path, index=False)\n",
        "            logging.info(f\"Saved data for coordination number {coordination_number} to {output_excel_path}\")\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Failed to save Excel file {output_excel_path}: {e}\")\n",
        "\n",
        "# --- Main Execution (Calls the modified functions) ---\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function to parse PDB files, find coordination sites, and save results in original format.\"\"\"\n",
        "    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "    aggregated_coordination_info = {} # Stores {coord_num: [list_of_metal_ligand_tuples]}\n",
        "    parser = PDB.PDBParser(QUIET=True)\n",
        "\n",
        "    logging.info(f\"Starting PDB analysis in directory: {PDB_DIRECTORY}\")\n",
        "\n",
        "    processed_files = 0\n",
        "    for filename in os.listdir(PDB_DIRECTORY):\n",
        "        if filename.lower().endswith('.pdb'):\n",
        "            pdb_filepath = os.path.join(PDB_DIRECTORY, filename)\n",
        "            pdb_id = filename.split('.')[0]\n",
        "            logging.info(f\"Processing {filename} (ID: {pdb_id})...\")\n",
        "\n",
        "            try:\n",
        "                structure = parser.get_structure(pdb_id, pdb_filepath)\n",
        "\n",
        "                # === Call the function that formats data for the original output ===\n",
        "                coordination_info = find_coordination_sites_for_original_format(\n",
        "                    structure,\n",
        "                    pdb_id,\n",
        "                    METAL_ID,\n",
        "                    MN_DISTANCE_THRESHOLDS,\n",
        "                    ALLOWED_RESIDUES\n",
        "                )\n",
        "\n",
        "                # Aggregate results (coordination_info is already grouped by coord number)\n",
        "                for coord_num, sites_list in coordination_info.items():\n",
        "                    if coord_num not in aggregated_coordination_info:\n",
        "                        aggregated_coordination_info[coord_num] = []\n",
        "                    # sites_list contains tuples formatted for one row per metal-ligand pair\n",
        "                    aggregated_coordination_info[coord_num].extend(sites_list)\n",
        "\n",
        "                processed_files += 1\n",
        "\n",
        "            except Exception as e:\n",
        "                logging.error(f\"Could not process file {filename}. Error: {e}\", exc_info=True)\n",
        "\n",
        "    logging.info(f\"Finished processing {processed_files} PDB files.\")\n",
        "\n",
        "    logging.info(\"Saving aggregated results to Excel files (Original Row Format)...\")\n",
        "    # === Call the saving function designed for the original format ===\n",
        "    save_coordination_data_as_original_format(\n",
        "        aggregated_coordination_info,\n",
        "        OUTPUT_DIR,\n",
        "        OUTPUT_FILENAME_PREFIX,\n",
        "        BASE_OUTPUT_COLUMNS\n",
        "    )\n",
        "\n",
        "    logging.info('Script execution completed.')\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "imQ71jLJZWvf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xRz66TRzSHq",
        "outputId": "17ea217f-0c3a-48fb-c5e8-dd35dcf7b8b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ FASTA file saved to: /content/2His_1Glu_coordination_3_pdb_list.fasta\n"
          ]
        }
      ],
      "source": [
        "#Fasta file ()\n",
        "import pandas as pd\n",
        "import requests\n",
        "\n",
        "# Load Excel file with PDB IDs\n",
        "input_file = \"/content/2His_1Glu_PDB_list.xlsx\"  # Update with correct path\n",
        "df = pd.read_excel(input_file)\n",
        "\n",
        "# Extract PDB IDs\n",
        "pdb_ids = df[\"PDB ID\"].dropna().unique().tolist()\n",
        "\n",
        "# Function to download FASTA from RCSB\n",
        "def download_fasta(pdb_id):\n",
        "    url = f\"https://www.rcsb.org/fasta/entry/{pdb_id}\"\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        return response.text\n",
        "    else:\n",
        "        print(f\"❌ Failed to download {pdb_id}\")\n",
        "        return None\n",
        "\n",
        "# Download FASTA sequences\n",
        "fasta_file = \"/content/2His_1Glu_coordination_3_pdb_list.fasta\"  # Update path\n",
        "with open(fasta_file, \"w\") as fasta_output:\n",
        "    for pdb_id in pdb_ids:\n",
        "        fasta_data = download_fasta(pdb_id)\n",
        "        if fasta_data:\n",
        "            fasta_output.write(f\">{pdb_id}\\n{fasta_data}\\n\")\n",
        "\n",
        "print(f\"✅ FASTA file saved to: {fasta_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wnOBx0f-ioJX",
        "outputId": "851a74b3-abd4-4a92-8be6-d897498aa589"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ File saved: 3His_sequence_parsed2.xlsx\n"
          ]
        }
      ],
      "source": [
        "#Only extract the sequence of metal-binding chain\n",
        "import pandas as pd\n",
        "from Bio import SeqIO\n",
        "\n",
        "fasta_path = \"/content/2His_1Glu_coordination_3_pdb_list.fasta\"  # path to your FASTA file\n",
        "\n",
        "records = []\n",
        "for record in SeqIO.parse(fasta_path, \"fasta\"):\n",
        "    full_header = record.description\n",
        "    pdb_id = full_header.split(\"_\")[0]\n",
        "\n",
        "    try:\n",
        "        chain_info = full_header.split(\"|\")[1].strip()\n",
        "        if chain_info.startswith(\"Chain \"):\n",
        "            chain_id = chain_info.replace(\"Chain \", \"\")\n",
        "        elif chain_info.startswith(\"Chains \"):\n",
        "            chain_id = chain_info.replace(\"Chains \", \"\")\n",
        "        else:\n",
        "            chain_id = None\n",
        "    except IndexError:\n",
        "        chain_id = None\n",
        "\n",
        "    sequence = str(record.seq)\n",
        "    if chain_id is not None:\n",
        "        for single_chain in [c.strip() for c in chain_id.split(\",\")]:\n",
        "            records.append((pdb_id, single_chain, sequence))\n",
        "\n",
        "df = pd.DataFrame(records, columns=[\"PDB ID\", \"Chain ID\", \"Sequence\"])\n",
        "df.to_excel(\"/content/2His_1Glu_coordination_3_pdb_list_parsed.xlsx\", index=False)\n",
        "print(\"✅ File saved: 3His_sequence_parsed2.xlsx\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EmPtXm38iYaZ"
      },
      "outputs": [],
      "source": [
        "#Only extract the sequence of metal-binding chain\n",
        "import pandas as pd\n",
        "\n",
        "# Load both Excel files\n",
        "file1_path = \"/content/2His_1Glu_coordination_3_pdb_list_parsed.xlsx\"\n",
        "file2_path = \"/content/2His_1Glu_pdb_chain_ID.xlsx\"\n",
        "\n",
        "df_main = pd.read_excel(file1_path)\n",
        "df_match = pd.read_excel(file2_path)\n",
        "\n",
        "# Merge on 'PDB ID' and 'Chain ID'\n",
        "merged_df = pd.merge(df_main, df_match, on=['PDB ID', 'Chain ID'], how='inner')\n",
        "\n",
        "# Save the result\n",
        "output_path = \"/content/2His_1Glu_coordination_3_pdb_list_sequences.xlsx\"\n",
        "merged_df.to_excel(output_path, index=False)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iGUd27PpmZJn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0O1MJHz76pUd"
      },
      "outputs": [],
      "source": [
        "#Global sequence alignment\n",
        "import pandas as pd\n",
        "from Bio import pairwise2\n",
        "from Bio.Align import substitution_matrices\n",
        "import time\n",
        "\n",
        "# Input file path\n",
        "input_file = '/content/2His_1Glu_coordination_3_pdb_list_sequences.xlsx'\n",
        "\n",
        "# Read Excel file\n",
        "df = pd.read_excel(input_file)\n",
        "df[\"Sequence\"] = df[\"Sequence\"].astype(str)\n",
        "\n",
        "unique_sequences = []\n",
        "excluded_sequences = []\n",
        "\n",
        "start_time = time.time()\n",
        "threshold = 30.0  # Identity threshold (%)\n",
        "\n",
        "# Load BLOSUM62 substitution matrix correctly\n",
        "blosum62 = substitution_matrices.load(\"BLOSUM62\")\n",
        "\n",
        "for i, row in df.iterrows():\n",
        "    sequence = row[\"Sequence\"]\n",
        "    pdb_id = row[\"PDB ID\"]\n",
        "    is_unique = True\n",
        "\n",
        "    for unique_pdb_id, unique_sequence in unique_sequences:\n",
        "        # Perform global alignment using BLOSUM62\n",
        "        alignments = pairwise2.align.globalds(sequence, unique_sequence, blosum62, -10, -0.5, one_alignment_only=True)\n",
        "\n",
        "        aligned_seq1, aligned_seq2, score, start, end = alignments[0]\n",
        "\n",
        "        alignment_length = len(aligned_seq1)\n",
        "        identical = sum(a == b for a, b in zip(aligned_seq1, aligned_seq2) if a != '-' and b != '-')\n",
        "        percent_identity = (identical / alignment_length) * 100\n",
        "\n",
        "        if percent_identity >= threshold:\n",
        "            is_unique = False\n",
        "            excluded_sequences.append((pdb_id, sequence))\n",
        "            print(f\"PDB ID {pdb_id} excluded (identity: {percent_identity:.2f}%) with {unique_pdb_id}\")\n",
        "            break\n",
        "\n",
        "    if is_unique:\n",
        "        unique_sequences.append((pdb_id, sequence))\n",
        "        print(f\"PDB ID {pdb_id} is unique and saved.\")\n",
        "\n",
        "    elapsed_time = time.time() - start_time\n",
        "    remaining_time = (elapsed_time / (i + 1)) * (len(df) - i - 1)\n",
        "    print(f\"Progress: {i+1}/{len(df)} | Elapsed: {elapsed_time:.2f}s | Remaining: {remaining_time:.2f}s\")\n",
        "\n",
        "# Save results\n",
        "df_unique = pd.DataFrame(unique_sequences, columns=[\"PDB ID\", \"Sequence\"])\n",
        "df_excluded = pd.DataFrame(excluded_sequences, columns=[\"PDB ID\", \"Sequence\"])\n",
        "\n",
        "df_unique.to_excel(\"/content/2His_1Glu_unique_30.xlsx\", index=False)\n",
        "df_excluded.to_excel(\"/content/2His_1Glu_blo_exclude_30.xlsx\", index=False)\n",
        "\n",
        "print(\"Sequence identity analysis completed.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Copy the PDB file based on unique_sequence\n",
        "import pandas as pd\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "# Paths to input Excel files\n",
        "input_file_unique = \"L:/Zn-installer_rawdata/241020_Mn_Final/Final_3/Final_3.xlsx\"\n",
        "\n",
        "# Path to the source directory containing PDB files\n",
        "source_dir = \"L:/Zn-installer_rawdata/241020_Mn_Final/metal_count_greater_than_1/mixed/\"\n",
        "\n",
        "# Paths to the destination directories\n",
        "destination_dir_unique = \"L:/Zn-installer_rawdata/241020_Mn_Final/Final_3/\"\n",
        "\n",
        "# Create the destination directories if they do not exist\n",
        "os.makedirs(destination_dir_unique, exist_ok=True)\n",
        "\n",
        "# Function to copy PDB files based on a DataFrame\n",
        "def copy_pdb_files(df, destination_dir):\n",
        "    pdb_ids = df['PDB ID'].tolist()\n",
        "    for pdb_id in pdb_ids:\n",
        "        source_file = os.path.join(source_dir, f\"{pdb_id}.pdb\")\n",
        "        destination_file = os.path.join(destination_dir, f\"{pdb_id}.pdb\")\n",
        "        if os.path.exists(source_file):\n",
        "            shutil.copy(source_file, destination_file)\n",
        "            print(f\"Copied: {pdb_id}.pdb to {destination_dir}\")\n",
        "        else:\n",
        "            print(f\"File not found: {pdb_id}.pdb\")\n",
        "\n",
        "# Read Excel files using pandas\n",
        "df_unique = pd.read_excel(input_file_unique)\n",
        "\n",
        "\n",
        "# Copy PDB files for unique sequences\n",
        "copy_pdb_files(df_unique, destination_dir_unique)"
      ],
      "metadata": {
        "id": "3yq0wqv3yQHd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#structure alignment (If necessary)\n",
        "import os\n",
        "import itertools\n",
        "import pandas as pd\n",
        "from pymol2 import PyMOL\n",
        "\n",
        "# Change this to your local folder with PDBs\n",
        "pdb_folder = r\"/content/drive/MyDrive/All/Zn_2His_1Glu\"  # <-- change this\n",
        "pdb_files = [f for f in os.listdir(pdb_folder) if f.endswith(\".pdb\")]\n",
        "\n",
        "all_rmsd_path = os.path.join(pdb_folder, \"/content/drive/MyDrive/250413_Final/2His_1Glu_pairwise_rmsd.xlsx\")\n",
        "similar_rmsd_path = os.path.join(pdb_folder, \"/content/drive/MyDrive/250413_Final/2His_1Glu_similar_structures.xlsx\")\n",
        "\n",
        "results = []\n",
        "similar_results = []\n",
        "\n",
        "# ✅ Start PyMOL instance properly\n",
        "with PyMOL() as pymol:\n",
        "    pymol.start()\n",
        "    cmd = pymol.cmd\n",
        "\n",
        "    # Load each structure\n",
        "    for pdb_file in pdb_files:\n",
        "        full_path = os.path.join(pdb_folder, pdb_file)\n",
        "        obj_name = os.path.splitext(pdb_file)[0]\n",
        "        cmd.load(full_path, obj_name)\n",
        "\n",
        "    # Align pairwise\n",
        "    for pdb1, pdb2 in itertools.combinations(pdb_files, 2):\n",
        "        obj1 = os.path.splitext(pdb1)[0]\n",
        "        obj2 = os.path.splitext(pdb2)[0]\n",
        "        try:\n",
        "            rmsd = cmd.align(obj1, obj2)[0]\n",
        "            print(f\"{pdb1} vs {pdb2} -> RMSD = {rmsd:.3f}\")\n",
        "            row = {\n",
        "                \"Structure 1\": pdb1,\n",
        "                \"Structure 2\": pdb2,\n",
        "                \"RMSD (Å)\": round(rmsd, 3)\n",
        "            }\n",
        "            results.append(row)\n",
        "            if rmsd < 1.0:\n",
        "                similar_results.append(row)\n",
        "        except Exception as e:\n",
        "            print(f\"Failed: {pdb1} vs {pdb2} — {e}\")\n",
        "\n",
        "# Save results\n",
        "df_all = pd.DataFrame(results)\n",
        "df_similar = pd.DataFrame(similar_results)\n",
        "df_all.to_excel(all_rmsd_path, index=False)\n",
        "df_similar.to_excel(similar_rmsd_path, index=False)\n",
        "\n",
        "# ✅ Extract unique representative PDBs by clustering similar ones\n",
        "unique_pdbs = set(pdb_files)  # start with all PDBs\n",
        "\n",
        "# Create groups of similar structures\n",
        "similar_groups = []\n",
        "used = set()\n",
        "\n",
        "for _, row in df_similar.iterrows():\n",
        "    a, b = row[\"Structure 1\"], row[\"Structure 2\"]\n",
        "    found = False\n",
        "    for group in similar_groups:\n",
        "        if a in group or b in group:\n",
        "            group.update([a, b])\n",
        "            found = True\n",
        "            break\n",
        "    if not found:\n",
        "        similar_groups.append(set([a, b]))\n",
        "    used.update([a, b])\n",
        "\n",
        "# Remove all but one from each similar group\n",
        "for group in similar_groups:\n",
        "    group = list(group)\n",
        "    # Keep the first one and remove the others\n",
        "    for pdb in group[1:]:\n",
        "        if pdb in unique_pdbs:\n",
        "            unique_pdbs.remove(pdb)\n",
        "\n",
        "# Save unique list\n",
        "unique_pdb_list = sorted(list(unique_pdbs))\n",
        "df_unique = pd.DataFrame({\"Unique PDB\": unique_pdb_list})\n",
        "unique_pdb_path = os.path.join(pdb_folder, \"/content/drive/MyDrive/250413_Final/2His_1Glu_unique_pdb.xlsx\")\n",
        "df_unique.to_excel(unique_pdb_path, index=False)\n",
        "\n",
        "print(f\"✅ Unique PDBs saved to: {unique_pdb_path}\")\n"
      ],
      "metadata": {
        "id": "mhSoV3His3Kk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from Bio import PDB\n",
        "import numpy as np\n",
        "import logging\n",
        "\n",
        "# --- Constants ---\n",
        "# Input/Output Configuration\n",
        "PDB_DIRECTORY = \"D:/250414_Final/Mn/2His_1Asp\"\n",
        "OUTPUT_DIR = \"D:/250414_Final/Mn/2His_1Asp\"\n",
        "# Changed prefix to indicate the format change\n",
        "OUTPUT_FILENAME_PREFIX = \"original_format_output_\"\n",
        "\n",
        "# Biochemical Constants\n",
        "METAL_ID = 'MN'\n",
        "ALLOWED_RESIDUES = {\n",
        "    'ALA', 'ARG', 'ASN', 'ASP', 'CYS', 'GLN', 'GLU', 'GLY', 'HIS',\n",
        "    'LYS', 'MET', 'PHE', 'SER', 'THR', 'TYR', 'VAL'\n",
        "}\n",
        "\n",
        "# Distance thresholds (in Angstroms) for MN coordination [min_dist, max_dist]\n",
        "# Using the dictionary provided in the script\n",
        "MN_DISTANCE_THRESHOLDS = {\n",
        "    'HIS_ND1': (0, 2.75), 'HIS_NE2': (0, 2.75), 'HIS_N': (0, 2.75), 'HIS_O': (0, 2.85),\n",
        "    'GLU_OE1': (0, 2.85), 'GLU_OE2': (0, 2.85), 'GLU_N': (0, 2.75), 'GLU_O': (0, 2.85),\n",
        "    'ASP_OD1': (0, 2.85), 'ASP_OD2': (0, 2.85), 'ASP_N': (0, 2.75), 'ASP_O': (0, 2.85),\n",
        "    'ALA_N': (0, 2.75),   'ALA_O': (0, 2.85),\n",
        "    'CYS_SG': (0, 2.75),\n",
        "    'MET_SD': (0, 2.75),\n",
        "    'ARG_NH1': (0, 2.75), 'ARG_NH2': (0, 2.75), 'ARG_NE': (0, 2.75), 'ARG_N': (0, 2.75), 'ARG_O': (0, 2.85),\n",
        "    'ASN_OD1': (0, 2.85), 'ASN_ND2': (0, 2.75), 'ASN_N': (0, 2.75), 'ASN_O': (0, 2.85),\n",
        "    'GLN_OE1': (0, 2.85), 'GLN_NE2': (0, 2.75), 'GLN_N': (0, 2.75), 'GLN_O': (0, 2.85),\n",
        "    'GLY_N': (0, 2.75),   'GLY_O': (0, 2.85),\n",
        "    'LYS_NZ': (0, 2.75),  'LYS_N': (0, 2.75), 'LYS_O': (0, 2.85),\n",
        "    'SER_OG': (0, 2.85),  'SER_N': (0, 2.75), 'SER_O': (0, 2.85),\n",
        "    'THR_OG1': (0, 2.85), 'THR_N': (0, 2.75), 'THR_O': (0, 2.85),\n",
        "    'TYR_OH': (0, 2.85),  'TYR_N': (0, 2.75), 'TYR_O': (0, 2.85),\n",
        "}\n",
        "\n",
        "# Define base columns matching the desired \"original\" output structure (image)\n",
        "# 8 columns before binding atoms start\n",
        "BASE_OUTPUT_COLUMNS = [\n",
        "    'Entry ID', 'PDB ID', 'Metal Chain ID', 'Metal Residue number', 'Metal',\n",
        "    'Chain ID', 'Residue number', 'Residue name' # These refer to the specific ligand in the row\n",
        "]\n",
        "\n",
        "# --- Helper Functions (Keep as they are in the provided script) ---\n",
        "\n",
        "def calculate_distance(atom1, atom2):\n",
        "    \"\"\"Calculates the Euclidean distance between two Bio.PDB.Atom objects.\"\"\"\n",
        "    try:\n",
        "        coord1 = atom1.get_coord()\n",
        "        coord2 = atom2.get_coord()\n",
        "        diff = coord1 - coord2\n",
        "        return np.sqrt(np.sum(diff * diff))\n",
        "    except Exception as e:\n",
        "        # It's often better to log the pdb_id here if possible, but it's not passed directly\n",
        "        logging.error(f\"Error calculating distance between {atom1} and {atom2}: {e}\")\n",
        "        return float('inf') # Return infinity if coordinates are bad\n",
        "\n",
        "def get_distance_threshold(residue_name, atom_name, thresholds):\n",
        "    \"\"\"\n",
        "    Gets the distance threshold tuple (min, max) for a given residue and atom.\n",
        "    Handles specific cases like ASP/GLU carboxylates and CYS/MET sulfurs based on provided dict keys.\n",
        "    \"\"\"\n",
        "    # This function attempts to match the logic needed for the provided MN_DISTANCE_THRESHOLDS dict\n",
        "    # Check for specific atom keys first\n",
        "    specific_key = f\"{residue_name}_{atom_name}\"\n",
        "    if specific_key in thresholds:\n",
        "        return thresholds[specific_key]\n",
        "\n",
        "    # Handle potential combined carboxylate/amide oxygens if specific keys aren't present\n",
        "    # Note: The provided dict HAS specific keys like GLU_OE1, GLU_OE2, so this fallback might not be needed\n",
        "    # if residue_name == 'GLU' and atom_name in ['OE1', 'OE2'] and 'GLU_OE' in thresholds:\n",
        "    #     return thresholds['GLU_OE']\n",
        "    # if residue_name == 'ASP' and atom_name in ['OD1', 'OD2'] and 'ASP_OD' in thresholds:\n",
        "    #     return thresholds['ASP_OD']\n",
        "\n",
        "    # Handle specific sulfur atoms (assuming keys like CYS_SG, MET_SD exist)\n",
        "    if residue_name == 'CYS' and atom_name == 'SG' and 'CYS_SG' in thresholds:\n",
        "         return thresholds['CYS_SG']\n",
        "    if residue_name == 'MET' and atom_name == 'SD' and 'MET_SD' in thresholds:\n",
        "         return thresholds['MET_SD']\n",
        "\n",
        "    # Fallback if no specific or grouped key is found\n",
        "    return thresholds.get(specific_key, (0, 0))\n",
        "\n",
        "\n",
        "# --- MODIFIED Core Logic Function ---\n",
        "# Renamed to avoid confusion, but uses the same internal logic as the provided script's find_coordination_sites\n",
        "# The CHANGE is in how the final data is structured and returned.\n",
        "def find_coordination_sites_for_original_format(structure, pdb_id, metal_atom_name, thresholds, allowed_ligands):\n",
        "    \"\"\"\n",
        "    Identifies metal coordination sites using the provided script's logic,\n",
        "    but formats the output as one row per metal-ligand pair.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary where keys are coordination numbers and values are lists\n",
        "        of tuples. Each tuple represents ONE metal-ligand interaction:\n",
        "        (entry_id, pdb_code, metal_chain, metal_resnum, metal_name,\n",
        "         ligand_chain, ligand_resnum, ligand_name, atom1, atom2, ...)\n",
        "    \"\"\"\n",
        "    # This dictionary will store {coord_num: [list_of_metal_ligand_tuples]}\n",
        "    coordination_sites_by_number = {}\n",
        "\n",
        "    # Iterate through the structure to find metal ions\n",
        "    for model in structure:\n",
        "        for chain in model:\n",
        "            for metal_residue in chain:\n",
        "                if metal_residue.get_resname() == metal_atom_name:\n",
        "                    try:\n",
        "                        metal_atom = metal_residue[metal_atom_name]\n",
        "                    except KeyError:\n",
        "                        logging.warning(f\"Metal residue {metal_residue.id} in {pdb_id} \"\n",
        "                                        f\"doesn't contain a '{metal_atom_name}' atom. Skipping.\")\n",
        "                        continue\n",
        "\n",
        "                    metal_chain_id = metal_residue.get_parent().id\n",
        "                    metal_res_num = metal_residue.id[1]\n",
        "                    metal_name = metal_residue.get_resname()\n",
        "                    # Use .get() for safer header access, fallback to pdb_id\n",
        "                    pdb_code = structure.header.get('idcode', pdb_id)\n",
        "\n",
        "                    # Store info about ligands coordinating *this specific* metal ion\n",
        "                    # Using a dictionary keyed by residue ID to handle duplicates across models easily\n",
        "                    coordinating_ligands_details = {} # {(chain, resnum): {ligand_info..., \"binding_atoms\": [...]}}\n",
        "\n",
        "                    # Now, iterate through all potential ligands for this metal ion\n",
        "                    for ligand_model in structure:\n",
        "                        for ligand_chain in ligand_model:\n",
        "                            for ligand_residue in ligand_chain:\n",
        "                                # Skip if it's the metal itself or not an allowed residue type\n",
        "                                if ligand_residue == metal_residue or ligand_residue.get_resname() not in allowed_ligands:\n",
        "                                    continue\n",
        "\n",
        "                                coordinating_atoms_in_ligand = [] # Atoms in *this* ligand coordinating the *current* metal\n",
        "                                ligand_res_name = ligand_residue.get_resname()\n",
        "                                ligand_chain_id = ligand_chain.id\n",
        "                                ligand_res_num = ligand_residue.id[1]\n",
        "                                residue_unique_id = (ligand_chain_id, ligand_res_num)\n",
        "\n",
        "                                # Check each atom in the potential ligand residue\n",
        "                                for ligand_atom in ligand_residue:\n",
        "                                    ligand_atom_name = ligand_atom.get_id()\n",
        "                                    min_dist, max_dist = get_distance_threshold(ligand_res_name, ligand_atom_name, thresholds)\n",
        "\n",
        "                                    if max_dist <= 0: continue # Skip if no valid threshold\n",
        "\n",
        "                                    distance = calculate_distance(metal_atom, ligand_atom)\n",
        "\n",
        "                                    if min_dist <= distance <= max_dist:\n",
        "                                        coordinating_atoms_in_ligand.append(ligand_atom_name)\n",
        "\n",
        "                                # If any atom in this ligand coordinated, store/update its info\n",
        "                                if coordinating_atoms_in_ligand:\n",
        "                                    if residue_unique_id not in coordinating_ligands_details:\n",
        "                                        coordinating_ligands_details[residue_unique_id] = {\n",
        "                                            \"ligand_chain_id\": ligand_chain_id,\n",
        "                                            \"ligand_res_num\": ligand_res_num,\n",
        "                                            \"ligand_res_name\": ligand_res_name,\n",
        "                                            \"binding_atoms\": set(coordinating_atoms_in_ligand) # Use a set initially\n",
        "                                        }\n",
        "                                    else:\n",
        "                                        # Add any newly found coordinating atoms for this residue\n",
        "                                        coordinating_ligands_details[residue_unique_id][\"binding_atoms\"].update(coordinating_atoms_in_ligand)\n",
        "\n",
        "                    # --- Data Formatting Change Starts Here ---\n",
        "                    # After checking all ligands for this metal ion, determine coordination number\n",
        "                    coordination_number = len(coordinating_ligands_details)\n",
        "\n",
        "                    if coordination_number > 0:\n",
        "                        # Create the list of output tuples (one per ligand) for this metal\n",
        "                        output_rows_for_this_metal = []\n",
        "                        for ligand_details in coordinating_ligands_details.values():\n",
        "                            # Sort binding atoms alphabetically for consistent output\n",
        "                            sorted_binding_atoms = tuple(sorted(list(ligand_details[\"binding_atoms\"])))\n",
        "\n",
        "                            # Construct the tuple for this metal-ligand pair row\n",
        "                            row_tuple = (\n",
        "                                pdb_id,                   # Entry ID (from filename)\n",
        "                                pdb_code,                 # PDB ID (from header)\n",
        "                                metal_chain_id,           # Metal Chain ID\n",
        "                                metal_res_num,            # Metal Residue number\n",
        "                                metal_name,               # Metal Name\n",
        "                                ligand_details[\"ligand_chain_id\"],    # Ligand Chain ID\n",
        "                                ligand_details[\"ligand_res_num\"],     # Ligand Residue number\n",
        "                                ligand_details[\"ligand_res_name\"]     # Ligand Residue name\n",
        "                            ) + sorted_binding_atoms      # Add tuple of binding atoms\n",
        "\n",
        "                            output_rows_for_this_metal.append(row_tuple)\n",
        "\n",
        "                        # Store this list of rows under the coordination number\n",
        "                        if coordination_number not in coordination_sites_by_number:\n",
        "                            coordination_sites_by_number[coordination_number] = []\n",
        "                        coordination_sites_by_number[coordination_number].extend(output_rows_for_this_metal)\n",
        "                    # --- Data Formatting Change Ends Here ---\n",
        "\n",
        "    # Return the dictionary structured for the desired output format\n",
        "    return coordination_sites_by_number\n",
        "\n",
        "\n",
        "# --- MODIFIED Saving Function ---\n",
        "# Renamed function to clarify its purpose\n",
        "def save_coordination_data_as_original_format(all_coordination_data, output_dir, file_prefix, base_columns):\n",
        "    \"\"\"\n",
        "    Saves the aggregated coordination data using the original format\n",
        "    (one row per metal-ligand pair, dynamic binding atom columns).\n",
        "    \"\"\"\n",
        "    if not all_coordination_data:\n",
        "        logging.warning(\"No coordination data found to save.\")\n",
        "        return\n",
        "\n",
        "    for coordination_number, sites_list in all_coordination_data.items():\n",
        "        if not sites_list:\n",
        "            logging.info(f\"No sites found for coordination number {coordination_number}. Skipping file.\")\n",
        "            continue\n",
        "\n",
        "        logging.info(f\"Processing data for coordination number {coordination_number}...\")\n",
        "\n",
        "        # Remove exact duplicate rows before saving (often desirable)\n",
        "        # Sort first by all elements in tuple for consistent non-duplicate selection\n",
        "        unique_sites_list = sorted(list(set(sites_list)))\n",
        "\n",
        "        # Determine the maximum number of binding atoms listed for any single ligand in this group\n",
        "        num_base_cols = len(base_columns) # Should be 8\n",
        "        max_binding_atoms = 0\n",
        "        if unique_sites_list:\n",
        "            try:\n",
        "                # Calculate max length of the binding atom part of the tuples\n",
        "                max_binding_atoms = max(len(site_tuple) - num_base_cols for site_tuple in unique_sites_list)\n",
        "            except ValueError:\n",
        "                 logging.error(f\"Inconsistent tuple length found for coord_num {coordination_number}. Check data.\")\n",
        "                 max_binding_atoms = 0 # Default\n",
        "\n",
        "        # Define columns: base columns + Binding atomX columns\n",
        "        columns = list(base_columns)\n",
        "        columns.extend([f'Binding atom{i}' for i in range(1, max_binding_atoms + 1)])\n",
        "\n",
        "        # Create DataFrame using the unique list and defined columns\n",
        "        # .fillna(np.nan) handles ligands with fewer binding atoms than the max\n",
        "        try:\n",
        "             df_coordination = pd.DataFrame(unique_sites_list, columns=columns).fillna(np.nan)\n",
        "        except Exception as e:\n",
        "             # Log error and fallback to default columns if specific columns fail\n",
        "             logging.error(f\"Error creating DataFrame for coord num {coordination_number} with specific columns: {e}. Using default columns.\")\n",
        "             df_coordination = pd.DataFrame(unique_sites_list)\n",
        "\n",
        "\n",
        "        # Define output path and save\n",
        "        output_filename = f\"{file_prefix}{coordination_number}.xlsx\"\n",
        "        output_excel_path = os.path.join(output_dir, output_filename)\n",
        "\n",
        "        try:\n",
        "            df_coordination.to_excel(output_excel_path, index=False)\n",
        "            logging.info(f\"Saved data for coordination number {coordination_number} to {output_excel_path}\")\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Failed to save Excel file {output_excel_path}: {e}\")\n",
        "\n",
        "# --- Main Execution (Calls the modified functions) ---\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function to parse PDB files, find coordination sites, and save results in original format.\"\"\"\n",
        "    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "    aggregated_coordination_info = {} # Stores {coord_num: [list_of_metal_ligand_tuples]}\n",
        "    parser = PDB.PDBParser(QUIET=True)\n",
        "\n",
        "    logging.info(f\"Starting PDB analysis in directory: {PDB_DIRECTORY}\")\n",
        "\n",
        "    processed_files = 0\n",
        "    for filename in os.listdir(PDB_DIRECTORY):\n",
        "        if filename.lower().endswith('.pdb'):\n",
        "            pdb_filepath = os.path.join(PDB_DIRECTORY, filename)\n",
        "            pdb_id = filename.split('.')[0]\n",
        "            logging.info(f\"Processing {filename} (ID: {pdb_id})...\")\n",
        "\n",
        "            try:\n",
        "                structure = parser.get_structure(pdb_id, pdb_filepath)\n",
        "\n",
        "                # === Call the function that formats data for the original output ===\n",
        "                coordination_info = find_coordination_sites_for_original_format(\n",
        "                    structure,\n",
        "                    pdb_id,\n",
        "                    METAL_ID,\n",
        "                    MN_DISTANCE_THRESHOLDS,\n",
        "                    ALLOWED_RESIDUES\n",
        "                )\n",
        "\n",
        "                # Aggregate results (coordination_info is already grouped by coord number)\n",
        "                for coord_num, sites_list in coordination_info.items():\n",
        "                    if coord_num not in aggregated_coordination_info:\n",
        "                        aggregated_coordination_info[coord_num] = []\n",
        "                    # sites_list contains tuples formatted for one row per metal-ligand pair\n",
        "                    aggregated_coordination_info[coord_num].extend(sites_list)\n",
        "\n",
        "                processed_files += 1\n",
        "\n",
        "            except Exception as e:\n",
        "                logging.error(f\"Could not process file {filename}. Error: {e}\", exc_info=True)\n",
        "\n",
        "    logging.info(f\"Finished processing {processed_files} PDB files.\")\n",
        "\n",
        "    logging.info(\"Saving aggregated results to Excel files (Original Row Format)...\")\n",
        "    # === Call the saving function designed for the original format ===\n",
        "    save_coordination_data_as_original_format(\n",
        "        aggregated_coordination_info,\n",
        "        OUTPUT_DIR,\n",
        "        OUTPUT_FILENAME_PREFIX,\n",
        "        BASE_OUTPUT_COLUMNS\n",
        "    )\n",
        "\n",
        "    logging.info('Script execution completed.')\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "KS9f6DARhOoT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#change the format of excel file\n",
        "import pandas as pd\n",
        "\n",
        "# Load the Excel file\n",
        "coordination_df = pd.read_excel('L:/Zn-installer_rawdata/241020_Mn_Final/Final_3/new_3.xlsx')\n",
        "\n",
        "# Remove extra single quotes from column names\n",
        "coordination_df.columns = coordination_df.columns.str.replace(\"'\", \"\")\n",
        "\n",
        "# Prepare the data for horizontal arrangement based on PDB_ID, Metal Chain ID, and Metal Residue number\n",
        "grouped = coordination_df.groupby(['Entry ID', 'Metal Chain ID', 'Metal Residue number'])\n",
        "\n",
        "# Create a new DataFrame to hold the horizontally arranged data\n",
        "horizontal_data = []\n",
        "\n",
        "for name, group in grouped:\n",
        "    row = list(name)\n",
        "    for _, data in group.iterrows():\n",
        "        row.extend([\n",
        "            data['Chain ID'], data['Residue number'], data['Residue name'], data['Binding atom1']\n",
        "        ])\n",
        "    horizontal_data.append(row)\n",
        "\n",
        "# Determine the column names for the new DataFrame\n",
        "max_columns = max(len(row) for row in horizontal_data)\n",
        "columns = ['Entry ID', 'Metal Chain ID', 'Metal Residue number'] + \\\n",
        "    [item for i in range((max_columns - 3) // 4) for item in [f'Chain ID{i+1}', f'Residue_number{i+1}', f'Residue_name{i+1}', f'Binding atom{i+1}']]\n",
        "\n",
        "# Create the horizontally arranged DataFrame\n",
        "horizontal_df = pd.DataFrame(horizontal_data, columns=columns)\n",
        "\n",
        "# Save the resulting DataFrame to a new Excel file\n",
        "horizontal_df.to_excel('L:/Zn-installer_rawdata/241020_Mn_Final/Final_3/new_3_format.xlsx', index=False)"
      ],
      "metadata": {
        "id": "KMsiYSi2w7sZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate the geometric parameter\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from Bio.PDB import PDBParser\n",
        "from Bio.PDB.PDBExceptions import PDBConstructionWarning\n",
        "import warnings\n",
        "from math import acos, degrees\n",
        "\n",
        "# Load the Excel file\n",
        "excel_file = 'L:/Zn-installer_rawdata/241111_Mn_Final//Final/new_3_format.xlsx'\n",
        "df = pd.read_excel(excel_file)\n",
        "\n",
        "# Specify the directory containing PDB files\n",
        "pdb_directory = 'L:/Zn-installer_rawdata/241111_Mn_Final//Final/'\n",
        "\n",
        "def parse_pdb_id(pdb_id):\n",
        "    return pdb_id.strip()\n",
        "\n",
        "def calculate_distance(coord1, coord2):\n",
        "    return np.linalg.norm(coord1 - coord2)\n",
        "\n",
        "def calculate_angle(vector1, vector2):\n",
        "    \"\"\"\n",
        "    Calculate the angle between two vectors.\n",
        "    \"\"\"\n",
        "    unit_vector1 = vector1 / np.linalg.norm(vector1)\n",
        "    unit_vector2 = vector2 / np.linalg.norm(vector2)\n",
        "    dot_product = np.dot(unit_vector1, unit_vector2)\n",
        "    angle = degrees(acos(dot_product))\n",
        "    return angle\n",
        "\n",
        "# Add columns for distances and angles\n",
        "for i in range(1, 4):\n",
        "    df[f'Calpha_X_{i}'] = None\n",
        "    df[f'Calpha_Y_{i}'] = None\n",
        "    df[f'Calpha_Z_{i}'] = None\n",
        "    df[f'Cbeta_X_{i}'] = None\n",
        "    df[f'Cbeta_Y_{i}'] = None\n",
        "    df[f'Cbeta_Z_{i}'] = None\n",
        "    df[f'Angle_{i}'] = None\n",
        "\n",
        "for i in range(3):\n",
        "    for j in range(i + 1, 3):\n",
        "        df[f'Calpha_Distance_{i+1}_{j+1}'] = None\n",
        "        df[f'Cbeta_Distance_{i+1}_{j+1}'] = None\n",
        "        df[f'Angle_{i+1}_{j+1}'] = None\n",
        "\n",
        "# Calculate distances, angles, and save alpha and beta carbon coordinates\n",
        "for index, row in df.iterrows():\n",
        "    pdb_id = parse_pdb_id(row['Entry ID'])\n",
        "    chain_ids = [row['Chain ID1'], row['Chain ID2'], row['Chain ID3']]\n",
        "    residues = [row['Residue_number1'], row['Residue_number2'], row['Residue_number3']]\n",
        "\n",
        "    pdb_filename = f\"{pdb_directory}/{pdb_id}.pdb\"\n",
        "    with warnings.catch_warnings():\n",
        "        warnings.simplefilter(\"ignore\", PDBConstructionWarning)\n",
        "        parser = PDBParser(QUIET=True)\n",
        "        structure = parser.get_structure(pdb_id, pdb_filename)\n",
        "\n",
        "    model = structure[0]\n",
        "    residue_coords = []\n",
        "\n",
        "    for i in range(3):\n",
        "        chain_id = chain_ids[i]\n",
        "        residue_number = int(residues[i])\n",
        "        try:\n",
        "            chain = model[chain_id]\n",
        "            residue = chain[residue_number]\n",
        "\n",
        "            ca_coord = np.array(residue['CA'].get_coord())\n",
        "            cb_coord = np.array(residue['CB'].get_coord())\n",
        "\n",
        "            df.at[index, f'Calpha_X_{i+1}'] = ca_coord[0]\n",
        "            df.at[index, f'Calpha_Y_{i+1}'] = ca_coord[1]\n",
        "            df.at[index, f'Calpha_Z_{i+1}'] = ca_coord[2]\n",
        "\n",
        "            df.at[index, f'Cbeta_X_{i+1}'] = cb_coord[0]\n",
        "            df.at[index, f'Cbeta_Y_{i+1}'] = cb_coord[1]\n",
        "            df.at[index, f'Cbeta_Z_{i+1}'] = cb_coord[2]\n",
        "\n",
        "            residue_coords.append((ca_coord, cb_coord))\n",
        "        except KeyError:\n",
        "            residue_coords.append((None, None))\n",
        "            continue\n",
        "\n",
        "    for i in range(3):\n",
        "        for j in range(i + 1, 3):\n",
        "            ca_coord_i, cb_coord_i = residue_coords[i]\n",
        "            ca_coord_j, cb_coord_j = residue_coords[j]\n",
        "\n",
        "            if ca_coord_i is not None and ca_coord_j is not None:\n",
        "                alpha_distance = calculate_distance(ca_coord_i, ca_coord_j)\n",
        "                df.at[index, f'Calpha_Distance_{i+1}_{j+1}'] = alpha_distance\n",
        "\n",
        "            if cb_coord_i is not None and cb_coord_j is not None:\n",
        "                beta_distance = calculate_distance(cb_coord_i, cb_coord_j)\n",
        "                df.at[index, f'Cbeta_Distance_{i+1}_{j+1}'] = beta_distance\n",
        "\n",
        "            if ca_coord_i is not None and cb_coord_i is not None and ca_coord_j is not None and cb_coord_j is not None:\n",
        "                vector_i = ca_coord_j - ca_coord_i\n",
        "                vector_j = cb_coord_j - cb_coord_i\n",
        "                angle = calculate_angle(vector_i, vector_j)\n",
        "                df.at[index, f'Angle_{i+1}_{j+1}'] = angle\n",
        "\n",
        "# Save the updated DataFrame to a new Excel file\n",
        "df.to_excel('L:/Zn-installer_rawdata/241111_Mn_Final//Final/Geometric_parameters.xlsx', index=False)"
      ],
      "metadata": {
        "id": "_gi4yJoPf3xv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2D_Final (density map)\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from Bio.PDB import PDBParser, is_aa\n",
        "from math import acos, degrees\n",
        "\n",
        "def calculate_distance(coord1, coord2):\n",
        "    return np.linalg.norm(coord1 - coord2)\n",
        "\n",
        "def calculate_angle(coord1, coord2, coord3):\n",
        "    # Calculate vector from coord2 to coord1 and coord2 to coord3\n",
        "    v1 = coord1 - coord2\n",
        "    v2 = coord3 - coord2\n",
        "    # Calculate cosine of the angle using dot product\n",
        "    cosine_angle = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
        "    # Ensure the cosine value is within the valid range due to floating-point errors\n",
        "    cosine_angle = np.clip(cosine_angle, -1.0, 1.0)\n",
        "    # Return the angle in degrees\n",
        "    return degrees(acos(cosine_angle))\n",
        "\n",
        "def extract_zn_coordinates(pdb_directory, pdb_entry, metal_chain_id, metal_residue_number):\n",
        "    # Initialize PDB parser\n",
        "    parser = PDBParser(QUIET=True)\n",
        "\n",
        "    # Build PDB file path\n",
        "    pdb_file = os.path.join(pdb_directory, f\"{pdb_entry}.pdb\")\n",
        "\n",
        "    if not os.path.exists(pdb_file):\n",
        "        print(f\"PDB file for {pdb_entry} not found in the directory.\")\n",
        "        return None\n",
        "\n",
        "    # Parse the PDB file\n",
        "    structure = parser.get_structure(pdb_entry, pdb_file)\n",
        "\n",
        "    # Iterate over all chains and residues to find the Zn atom\n",
        "    for chain in structure[0]:\n",
        "        if chain.id == metal_chain_id:\n",
        "            for residue in chain:\n",
        "                if residue.id[1] == metal_residue_number and residue.resname == 'FE':\n",
        "                    # Extract the Zn atom coordinates\n",
        "                    for atom in residue:\n",
        "                        if atom.element == 'FE':\n",
        "                            return atom.coord\n",
        "    return None\n",
        "\n",
        "def extract_coordinates(structure, chain_id, residue_number, atom_name):\n",
        "    for chain in structure[0]:\n",
        "        if chain.id == chain_id:\n",
        "            for residue in chain:\n",
        "                if residue.id[1] == residue_number and is_aa(residue):\n",
        "                    if atom_name in residue:\n",
        "                        return residue[atom_name].coord\n",
        "    return None\n",
        "\n",
        "def process_pdb_file(pdb_directory, pdb_entry, metal_chain_id, metal_residue_number, residues_info):\n",
        "    parser = PDBParser(QUIET=True)\n",
        "    pdb_file = os.path.join(pdb_directory, f\"{pdb_entry}.pdb\")\n",
        "\n",
        "    if not os.path.exists(pdb_file):\n",
        "        print(f\"PDB file for {pdb_entry} not found in the directory.\")\n",
        "        return None\n",
        "\n",
        "    structure = parser.get_structure(pdb_entry, pdb_file)\n",
        "\n",
        "    # Extract Zn coordinates using the improved method\n",
        "    zn_coords = extract_zn_coordinates(pdb_directory, pdb_entry, metal_chain_id, metal_residue_number)\n",
        "    if zn_coords is None:\n",
        "        print(f\"Zn atom not found in {pdb_entry}.\")\n",
        "        return None\n",
        "\n",
        "    ca_zn_distances = []\n",
        "    cb_zn_distances = []\n",
        "    ca_zn_cb_angles = []\n",
        "\n",
        "    for residue_info in residues_info:\n",
        "        chain_id, residue_number = residue_info['chain_id'], residue_info['residue_number']\n",
        "        ca_coords = extract_coordinates(structure, chain_id, residue_number, 'CA')\n",
        "        cb_coords = extract_coordinates(structure, chain_id, residue_number, 'CB')\n",
        "\n",
        "        if ca_coords is not None and cb_coords is not None:\n",
        "            # Calculate distances\n",
        "            ca_zn_distance = calculate_distance(ca_coords, zn_coords)\n",
        "            cb_zn_distance = calculate_distance(cb_coords, zn_coords)\n",
        "\n",
        "            # Calculate angle between CA-Zn-CB\n",
        "            ca_zn_cb_angle = calculate_angle(ca_coords, zn_coords, cb_coords)\n",
        "\n",
        "            ca_zn_distances.append(ca_zn_distance)\n",
        "            cb_zn_distances.append(cb_zn_distance)\n",
        "            ca_zn_cb_angles.append(ca_zn_cb_angle)\n",
        "        else:\n",
        "            print(f\"CA or CB atom not found for residue {residue_number} in chain {chain_id} of {pdb_entry}.\")\n",
        "\n",
        "    if len(ca_zn_distances) == 3 and len(cb_zn_distances) == 3 and len(ca_zn_cb_angles) == 3:\n",
        "        return {\n",
        "            'Entry ID': pdb_entry,\n",
        "            'Calpha_Zn_Dist1': ca_zn_distances[0],\n",
        "            'Calpha_Zn_Dist2': ca_zn_distances[1],\n",
        "            'Calpha_Zn_Dist3': ca_zn_distances[2],\n",
        "            'Cbeta_Zn_Dist1': cb_zn_distances[0],\n",
        "            'Cbeta_Zn_Dist2': cb_zn_distances[1],\n",
        "            'Cbeta_Zn_Dist3': cb_zn_distances[2],\n",
        "            'CA-Zn-CB Angle_1': ca_zn_cb_angles[0],\n",
        "            'CA-Zn-CB Angle_2': ca_zn_cb_angles[1],\n",
        "            'CA-Zn-CB Angle_3': ca_zn_cb_angles[2]\n",
        "        }\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "def calculate_and_trim_probability_map(df, calpha_cols, cbeta_cols, calpha_bins, cbeta_bins, npy_file='prob_map.npy', excel_file='precomputed_prob_map.xlsx'):\n",
        "    # Concatenate all Calpha and Cbeta distances into single series\n",
        "    all_calpha_distances = pd.concat([df[calpha_col] for calpha_col in calpha_cols])\n",
        "    all_cbeta_distances = pd.concat([df[cbeta_col] for cbeta_col in cbeta_cols])\n",
        "\n",
        "    # Create a 2D histogram to count occurrences in each bin\n",
        "    hist, calpha_bins_edges, cbeta_bins_edges = np.histogram2d(all_calpha_distances, all_cbeta_distances, bins=[calpha_bins, cbeta_bins])\n",
        "\n",
        "    # Normalize the histogram to get probabilities\n",
        "    prob_map = hist / np.sum(hist)\n",
        "\n",
        "    # Create a DataFrame for the probability map with all bins included, even if counts are zero\n",
        "    prob_map_df = pd.DataFrame(prob_map, index=calpha_bins_edges[:-1], columns=cbeta_bins_edges[:-1])\n",
        "\n",
        "    # Set the index and column names to match the expected output format\n",
        "    prob_map_df.index = prob_map_df.index[:len(prob_map_df)]\n",
        "    prob_map_df.columns = prob_map_df.columns[:len(prob_map_df.columns)]\n",
        "\n",
        "    # Reset the index and save the formatted DataFrame\n",
        "    formatted_prob_map_df = prob_map_df.reset_index()\n",
        "    formatted_prob_map_df.columns = ['Unnamed: 0'] + list(formatted_prob_map_df.columns[1:])\n",
        "\n",
        "    # Save the trimmed probability map to an Excel file\n",
        "    formatted_prob_map_df.to_excel(excel_file, index=False)\n",
        "    print(f\"Trimmed probability map saved to {excel_file}\")\n",
        "\n",
        "    # Save the probability map as a .npy file for later use\n",
        "    np.save(npy_file, prob_map)\n",
        "    print(f\"Probability map saved as {npy_file}\")\n",
        "\n",
        "    return formatted_prob_map_df\n",
        "\n",
        "def main():\n",
        "    pdb_directory = 'D:/250414_Final/FE/2His_1Asp/'  # Update this path to the directory where PDB files are stored\n",
        "    excel_file_path = 'D:/250414_Final/FE/Fe_2His_1Asp_geometric_parameters.xlsx'\n",
        "\n",
        "    # Load the Excel sheet data\n",
        "    sheet_data = pd.read_excel(excel_file_path, sheet_name='Sheet1')\n",
        "\n",
        "    all_results = []\n",
        "\n",
        "    for index, row in sheet_data.iterrows():\n",
        "        pdb_entry = row['Entry ID']\n",
        "        metal_chain_id = row['Metal Chain ID']\n",
        "        metal_residue_number = row['Metal Residue number']\n",
        "\n",
        "        residues_info = [\n",
        "            {'chain_id': row['Chain ID1'], 'residue_number': row['Residue_number1']},\n",
        "            {'chain_id': row['Chain ID2'], 'residue_number': row['Residue_number2']},\n",
        "            {'chain_id': row['Chain ID3'], 'residue_number': row['Residue_number3']}\n",
        "        ]\n",
        "\n",
        "        # Process each PDB file\n",
        "        result = process_pdb_file(pdb_directory, pdb_entry, metal_chain_id, metal_residue_number, residues_info)\n",
        "        if result:\n",
        "            all_results.append(result)\n",
        "\n",
        "    # Convert results to DataFrame and save\n",
        "    results_df = pd.DataFrame(all_results)\n",
        "    print(results_df)\n",
        "    results_df.to_excel('D:/250414_Final/FE/2His_1Asp_distance_angle.xlsx', index=False)\n",
        "\n",
        "    # Calculate and save the probability map\n",
        "    calpha_cols = ['Calpha_Zn_Dist1', 'Calpha_Zn_Dist2', 'Calpha_Zn_Dist3']\n",
        "    cbeta_cols = ['Cbeta_Zn_Dist1', 'Cbeta_Zn_Dist2', 'Cbeta_Zn_Dist3']\n",
        "    calpha_bins = np.arange(4.6, 7.0, 0.2).tolist()\n",
        "    cbeta_bins = np.arange(3.6, 6.1, 0.2).tolist()\n",
        "    npy_file = 'D:/250414_Final/FE/Fe_2His_1Asp_distance_angle_0.2.npy'\n",
        "    excel_file = 'D:/250414_Final/FE/Fe_2His_1Asp_distance_distance_angle_0.2.xlsx'\n",
        "    calculate_and_trim_probability_map(results_df, calpha_cols, cbeta_cols, calpha_bins, cbeta_bins, npy_file, excel_file)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "bBqHzcU0L2d4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#3D_Final\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from Bio.PDB import PDBParser, is_aa\n",
        "from math import acos, degrees\n",
        "\n",
        "# Helper functions to calculate distances and angles\n",
        "def calculate_distance(coord1, coord2):\n",
        "    return np.linalg.norm(coord1 - coord2)\n",
        "\n",
        "def calculate_angle(coord1, coord2, coord3):\n",
        "    v1 = coord1 - coord2\n",
        "    v2 = coord3 - coord2\n",
        "    cosine_angle = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
        "    cosine_angle = np.clip(cosine_angle, -1.0, 1.0)\n",
        "    return degrees(acos(cosine_angle))\n",
        "\n",
        "# Load Zn coordinates\n",
        "def extract_zn_coordinates(pdb_directory, pdb_entry, metal_chain_id, metal_residue_number):\n",
        "    parser = PDBParser(QUIET=True)\n",
        "    pdb_file = os.path.join(pdb_directory, f\"{pdb_entry}.pdb\")\n",
        "    if not os.path.exists(pdb_file):\n",
        "        print(f\"PDB file for {pdb_entry} not found.\")\n",
        "        return None\n",
        "\n",
        "    structure = parser.get_structure(pdb_entry, pdb_file)\n",
        "    for chain in structure[0]:\n",
        "        if chain.id == metal_chain_id:\n",
        "            for residue in chain:\n",
        "                if residue.id[1] == metal_residue_number and residue.resname == 'FE':\n",
        "                    for atom in residue:\n",
        "                        if atom.element == 'FE':\n",
        "                            return atom.coord\n",
        "    return None\n",
        "\n",
        "# Load atom coordinates\n",
        "def extract_coordinates(structure, chain_id, residue_number, atom_name):\n",
        "    for chain in structure[0]:\n",
        "        if chain.id == chain_id:\n",
        "            for residue in chain:\n",
        "                if residue.id[1] == residue_number and is_aa(residue):\n",
        "                    if atom_name in residue:\n",
        "                        return residue[atom_name].coord\n",
        "    return None\n",
        "\n",
        "# Function to create the 3D density map\n",
        "def calculate_3d_density_map_from_data(df, calpha_cols, cbeta_cols, angle_cols, calpha_bins, cbeta_bins, angle_bins, npy_file='3d_prob_map.npy', excel_file='3d_precomputed_prob_map.xlsx'):\n",
        "    # Concatenate all CA-Zn distances, CB-Zn distances, and angles\n",
        "    all_calpha_distances = pd.concat([df[calpha_col] for calpha_col in calpha_cols], ignore_index=True)\n",
        "    all_cbeta_distances = pd.concat([df[cbeta_col] for cbeta_col in cbeta_cols], ignore_index=True)\n",
        "    all_angles = pd.concat([df[angle_col] for angle_col in angle_cols], ignore_index=True)\n",
        "\n",
        "    # Ensure all data series are of the same length\n",
        "    if len(all_calpha_distances) == len(all_cbeta_distances) == len(all_angles):\n",
        "        # Create a 3D histogram\n",
        "        hist, edges = np.histogramdd(\n",
        "            (all_calpha_distances, all_cbeta_distances, all_angles),\n",
        "            bins=[calpha_bins, cbeta_bins, angle_bins]\n",
        "        )\n",
        "\n",
        "        # Normalize to get probability map\n",
        "        prob_map = hist / np.sum(hist)\n",
        "\n",
        "        # Verify that the sum of the probabilities equals 1\n",
        "        prob_sum = np.sum(prob_map)\n",
        "        if np.isclose(prob_sum, 1.0):\n",
        "            print(f\"Verification passed: Sum of probabilities = {prob_sum:.6f}\")\n",
        "        else:\n",
        "            print(f\"Warning: Sum of probabilities is not 1 (actual: {prob_sum:.6f})\")\n",
        "\n",
        "        # Flatten the 3D array and get bin centers for DataFrame\n",
        "        prob_map_flat = prob_map.flatten()\n",
        "        calpha_centers = 0.5 * (edges[0][:-1] + edges[0][1:])\n",
        "        cbeta_centers = 0.5 * (edges[1][:-1] + edges[1][1:])\n",
        "        angle_centers = 0.5 * (edges[2][:-1] + edges[2][1:])\n",
        "\n",
        "        # Create DataFrame with multi-index for bins\n",
        "        index = pd.MultiIndex.from_product([calpha_centers, cbeta_centers, angle_centers], names=['Calpha_Zn_Dist', 'Cbeta_Zn_Dist', 'CA-Zn-CB_Angle'])\n",
        "        prob_map_df = pd.DataFrame(prob_map_flat, index=index, columns=['Probability']).reset_index()\n",
        "\n",
        "        # Save the DataFrame to an Excel file\n",
        "        prob_map_df.to_excel(excel_file, index=False)\n",
        "        print(f\"3D probability map saved to {excel_file}\")\n",
        "\n",
        "        # Save the 3D probability map as a .npy file for further use\n",
        "        np.save(npy_file, prob_map)\n",
        "        print(f\"3D probability map saved as {npy_file}\")\n",
        "\n",
        "        return prob_map_df\n",
        "    else:\n",
        "        print(\"Data series lengths for distances and angles do not match.\")\n",
        "        return None\n",
        "\n",
        "# Function to automatically generate bins with edges\n",
        "def generate_bins_with_auto_edge(start, stop, step):\n",
        "    \"\"\"\n",
        "    Generate bins including edges and automatically adjust the last bin edge if needed.\n",
        "    \"\"\"\n",
        "    bins = np.arange(start, stop, step).tolist()\n",
        "    if bins[-1] < stop:\n",
        "        bins.append(stop)\n",
        "    return bins\n",
        "\n",
        "# Define bin ranges for distances and angles using the automatic function\n",
        "calpha_bins = generate_bins_with_auto_edge(4.6, 7.0, 0.2)  # Automatically includes 7.1\n",
        "cbeta_bins = generate_bins_with_auto_edge(3.6, 6.1, 0.2)   # Automatically includes 6.1\n",
        "angle_bins = generate_bins_with_auto_edge(0, 20, 0.5)      # Automatically includes 20.0\n",
        "\n",
        "# Load calculated distances and angles from the Excel file\n",
        "calculated_data_path = 'D:/250414_Final/FE/2His_1Asp_distance_angle.xlsx'  # Update this path as needed\n",
        "calculated_df = pd.read_excel(calculated_data_path)\n",
        "\n",
        "# Columns with calculated data\n",
        "calpha_cols = ['Calpha_Zn_Dist1', 'Calpha_Zn_Dist2', 'Calpha_Zn_Dist3']\n",
        "cbeta_cols = ['Cbeta_Zn_Dist1', 'Cbeta_Zn_Dist2', 'Cbeta_Zn_Dist3']\n",
        "angle_cols = ['CA-Zn-CB Angle_1', 'CA-Zn-CB Angle_2', 'CA-Zn-CB Angle_3']\n",
        "\n",
        "# Output files for the probability map\n",
        "npy_file ='D:/250414_Final/FE/2His_1Asp_prob_map_adjusted3.npy'\n",
        "excel_file = 'D:/250414_Final/FE/2His_1Asp_0.2_distance_angle_0.5.xlsx'\n",
        "\n",
        "# Generate and save the 3D probability map\n",
        "calculate_3d_density_map_from_data(\n",
        "    calculated_df, calpha_cols, cbeta_cols, angle_cols,\n",
        "    calpha_bins, cbeta_bins, angle_bins,\n",
        "    npy_file=npy_file,\n",
        "    excel_file=excel_file\n",
        ")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Cz0vEl6aZta5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}