{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# @markdown # Step 1A: Install Conda in Colab\n",
        "# @markdown Please make sure to select a runtime with **High-RAM** before running this step\n",
        "!pip -q install -U condacolab\n",
        "import condacolab\n",
        "condacolab.install()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "YPFCSbYcJOsI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown # Step 1B: Match your environment to Colab’s Python (3.12)\n",
        "\n",
        "import os, sys\n",
        "\n",
        "# 1) Remove any existing pin file (it can force the wrong Python)\n",
        "pin_file = \"/usr/local/conda-meta/pinned\"\n",
        "if os.path.exists(pin_file):\n",
        "    os.remove(pin_file)\n",
        "\n",
        "# 2) Make Conda's base match the running kernel (3.12) and install deps\n",
        "!mamba install -y -c conda-forge \"python=3.12\" \"pymol-open-source=3.1.0=py312*\" biopython tqdm\n",
        "\n",
        "# 3) Quick smoke test\n",
        "print(\"Python:\", sys.version.split()[0])\n",
        "try:\n",
        "    import Bio, tqdm\n",
        "    print(\"Biopython:\", Bio.__version__, \"| tqdm:\", tqdm.__version__)\n",
        "except Exception as e:\n",
        "    print(\"Biopython/tqdm import error:\", e)\n",
        "\n",
        "try:\n",
        "    import pymol\n",
        "    pymol.finish_launching(['pymol','-cq'])\n",
        "    from pymol import cmd\n",
        "    print(\"PyMOL OK. Version:\", cmd.get_version())\n",
        "except Exception as e:\n",
        "    print(\"PyMOL import/launch error:\", e)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "IBXYqLhBHmVp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown # Step 1C: Install all neccessary packages\n",
        "from Bio.PDB import PDBParser, Selection, NeighborSearch\n",
        "from Bio.PDB.Polypeptide import is_aa\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Optional: PyMOL headless (only if you need cmd)\n",
        "try:\n",
        "    import pymol\n",
        "    pymol.finish_launching(['pymol','-cq'])\n",
        "    from pymol import cmd\n",
        "except Exception as e:\n",
        "    print(\"PyMOL not available yet:\", e)\n",
        "\n",
        "# Optional: Torch\n",
        "try:\n",
        "    import torch\n",
        "except ImportError:\n",
        "    print(\"torch not found. Install with: pip install torch  (or: mamba install -y -c pytorch pytorch)\")\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "HGNke-geJZd2"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "6Q3_F6_hbkUp"
      },
      "outputs": [],
      "source": [
        "#@markdown # Step 2: Prepare PDB Files for Metal-Installer\n",
        "\n",
        "# Import the necessary modules\n",
        "import pymol2\n",
        "\n",
        "#@markdown **Note:** Specify the paths to the input and output PDB files below.\n",
        "\n",
        "#@markdown ### Enter the path to your input PDB file:\n",
        "pdb_file_path = \"/content/3tis.pdb\"  #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ### Enter the path to your output PDB file:\n",
        "Output_file_path = \"/content/3tis_alanine.pdb\"  #@param {type:\"string\"}\n",
        "\n",
        "# Create an instance of the PyMOL session\n",
        "with pymol2.PyMOL() as pymol:\n",
        "    # Initialize PyMOL\n",
        "    pymol.cmd.reinitialize()\n",
        "\n",
        "    # Load the structure file\n",
        "    pymol.cmd.load(pdb_file_path)\n",
        "\n",
        "    # Remove ligands, metal ions, and water\n",
        "    pymol.cmd.remove(\"resn HOH\")  # Remove water molecules\n",
        "    pymol.cmd.remove(\"hetatm\")  # Remove heteroatoms (metals, ligands)\n",
        "\n",
        "    # Identify glycine residues\n",
        "    glycine_residues = pymol.cmd.get_model(\"resn GLY\").atom\n",
        "\n",
        "    # Loop through glycine residues\n",
        "    for atom in glycine_residues:\n",
        "        residue_num = atom.resi\n",
        "        chain = atom.chain\n",
        "        # Construct the selection string in the format \"resi X and chain Y\"\n",
        "        selection_str = f\"resi {residue_num} and chain {chain}\"\n",
        "        # Apply the mutation using the mutagenesis command\n",
        "        pymol.cmd.wizard(\"mutagenesis\")\n",
        "        pymol.cmd.refresh_wizard()\n",
        "        pymol.cmd.get_wizard().do_select(selection_str)\n",
        "        pymol.cmd.get_wizard().set_mode(\"ALA\")\n",
        "        pymol.cmd.get_wizard().apply()\n",
        "        pymol.cmd.delete(selection_str)  # Delete the original residue to avoid clashes\n",
        "\n",
        "    # Save the mutated structure\n",
        "    pymol.cmd.save(Output_file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown # Step 3A: Filter using geometric parameters (select one representative structure for multimers)\n",
        "# pip install scipy pytz pandas openpyxl BioPython\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from Bio.PDB import PDBParser # For PDB parsing\n",
        "from Bio.PDB.Residue import Residue # For type hinting if needed\n",
        "from Bio.PDB.Chain import Chain # For type hinting if needed\n",
        "import itertools\n",
        "import requests\n",
        "import traceback\n",
        "from scipy.spatial import KDTree\n",
        "import datetime\n",
        "import pytz\n",
        "from typing import List, Optional, Tuple # For type hinting\n",
        "\n",
        "# --- Configuration ---\n",
        "# Specify the SINGLE PDB file path\n",
        "Target_pdb_file = \"/content/3tis_alanine.pdb\" # @param {type:\"string\"}\n",
        "\n",
        "# Specify Specific Residue Number\n",
        "# @markdown ### Find triad containing at least one following residue number.\n",
        "# @markdown ######  Specify a residue of interest (enter 0 for no prior selection).\n",
        "Specific_residue_number = 100  # @param {type:\"integer\"}\n",
        "# @markdown ### Optional range filter (select cases that include at least one specified residue)\n",
        "Use_range_filter = False   # @param {type:\"boolean\"}\n",
        "Starting_residue_number = 100  # @param {type:\"integer\"} #starting residue\n",
        "Last_residue_number = 150    # @param {type:\"integer\"} #last residue number\n",
        "# --- Specify Exact Output Excel File Path ---\n",
        "Output_excel_file_path_result1 = \"/content/3tis_alanine_3His.xlsx\"# @param {type:\"string\"}\n",
        "\n",
        "# --- Determine Output Directory from Output File Path ---\n",
        "output_directory = os.path.dirname(Output_excel_file_path_result1)\n",
        "if output_directory:\n",
        "    os.makedirs(output_directory, exist_ok=True)\n",
        "    print(f\"➡️ Output directory: {output_directory}\")\n",
        "else:\n",
        "    output_directory = \".\"\n",
        "    print(f\"➡️ Output directory: Current directory\")\n",
        "\n",
        "print(f\"➡️ Output Excel file with coordinates will be saved as: {Output_excel_file_path_result1}\")\n",
        "\n",
        "if Specific_residue_number != 0:\n",
        "    print(f\"⚠️ Filtering by residue number '{Specific_residue_number}'.\")\n",
        "\n",
        "\n",
        "# --- Download threshold configuration ---\n",
        "# Thresholds file will be saved in the same directory as the output Excel file\n",
        "base_url = \"https://raw.githubusercontent.com/SNU-Songlab/Metal-Installer-code/main/Threshold\"\n",
        "Metal = 'Zn'  # @param [\"Zn\", \"Mn\", \"Cu\", \"Fe\"]\n",
        "Ligands = '3His_only_for_Zn_Cu'  # @param [\"3His_only_for_Zn_Cu\", \"2His_1Asp_only_for_Zn_Mn_Fe\", \"2His_1Glu_only_for_Zn_Mn_Fe\", \"2His_1Cys_only_for_Cu\"]\n",
        "Threshold = '1'  # @param [\"1\", \"2\", \"3\", \"4\",\"5\"]\n",
        "thresholds_url = f\"{base_url}/{Metal}/{Ligands}/{Threshold}.xlsx\"\n",
        "thresholds_file = os.path.join(output_directory, f\"thresholds_{Metal}_{Ligands}_R{Threshold}.xlsx\")\n",
        "\n",
        "print(f\"⬇️ Downloading threshold set '{Ligands}' for '{Metal}' (Threshold {Threshold}) from: {thresholds_url}\")\n",
        "print(f\"⚠️ NOTE: These thresholds will be applied to any found triad (regardless of residue type).\")\n",
        "\n",
        "response = requests.get(thresholds_url)\n",
        "if response.status_code == 200:\n",
        "    with open(thresholds_file, \"wb\") as file:\n",
        "        file.write(response.content)\n",
        "    print(f\"✅ Thresholds downloaded successfully to {thresholds_file}\")\n",
        "else:\n",
        "    raise ValueError(f\"❌ Failed to download thresholds from {thresholds_url}. Status code: {response.status_code}\")\n",
        "\n",
        "# --- Load thresholds ---\n",
        "print(\"⚙️ Loading thresholds...\")\n",
        "try:\n",
        "    thresholds_df = pd.read_excel(thresholds_file, sheet_name=\"Sheet1\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"❌ Error: Thresholds file not found at {thresholds_file}. Cannot load thresholds.\")\n",
        "    raise # Re-raise the error to stop execution\n",
        "thresholds = {\n",
        "    row[\"Parameter\"]: (row[\"Min\"], row[\"Max\"])\n",
        "    for _, row in thresholds_df.iterrows()\n",
        "    if pd.notna(row[\"Min\"]) and pd.notna(row[\"Max\"])\n",
        "}\n",
        "required_thresholds = [\"alpha_distance_range\", \"beta_distance_range\", \"ratio_threshold_range\", \"pie_threshold_range\"]\n",
        "if not all(key in thresholds for key in required_thresholds):\n",
        "    missing = [key for key in required_thresholds if key not in thresholds]\n",
        "    raise ValueError(f\"❌ Missing required thresholds in the downloaded file: {missing}\")\n",
        "\n",
        "# Only define if present, otherwise geometry checks later will fail if needed\n",
        "alpha_distance_range = thresholds.get(\"alpha_distance_range\")\n",
        "beta_distance_range = thresholds.get(\"beta_distance_range\")\n",
        "ratio_threshold_range = thresholds.get(\"ratio_threshold_range\")\n",
        "pie_threshold_range = thresholds.get(\"pie_threshold_range\")\n",
        "\n",
        "\n",
        "print(\"📊 Thresholds loaded:\")\n",
        "if all(t is not None for t in [alpha_distance_range, beta_distance_range, ratio_threshold_range, pie_threshold_range]):\n",
        "    for key, value in thresholds.items():\n",
        "        print(f\"    - {key}: Min={value[0]}, Max={value[1]}\")\n",
        "else:\n",
        "    print(\"    ⚠️ Warning: Could not load all required thresholds.\")\n",
        "\n",
        "\n",
        "# --- Helper Functions ---\n",
        "def calculate_pie(v1, v2):\n",
        "    \"\"\"Calculates the angle (in degrees) between two vectors.\"\"\"\n",
        "    dot = np.dot(v1, v2)\n",
        "    norm = np.linalg.norm(v1) * np.linalg.norm(v2)\n",
        "    if norm == 0: return np.nan\n",
        "    angle_rad = np.arccos(np.clip(dot / norm, -1.0, 1.0))\n",
        "    return np.degrees(angle_rad)\n",
        "\n",
        "def extract_coordinates(chain: Optional[Chain], res_id: int, atom_name: str) -> List[Optional[float]]:\n",
        "    \"\"\"Safely extracts coordinates for a given atom in a residue.\"\"\"\n",
        "    if chain is None: return [np.nan, np.nan, np.nan]\n",
        "    try:\n",
        "        residue_key = (' ', res_id, ' ')\n",
        "        if residue_key not in chain:\n",
        "            if res_id not in chain: return [np.nan, np.nan, np.nan]\n",
        "            else: residue = chain[res_id]\n",
        "        else: residue = chain[residue_key]\n",
        "        if atom_name not in residue: return [np.nan, np.nan, np.nan]\n",
        "        coord = residue[atom_name].coord\n",
        "        if coord is None or len(coord) != 3: return [np.nan, np.nan, np.nan]\n",
        "        return [float(c) for c in coord]\n",
        "    except Exception: return [np.nan, np.nan, np.nan]\n",
        "\n",
        "# --- Helper function for Deduplication --- ADDED\n",
        "def standardize_residue_identity(row):\n",
        "    \"\"\"Creates a sorted tuple of residue numbers for unique identification.\"\"\"\n",
        "    try:\n",
        "        # Extract numbers, attempt conversion to int, use placeholder for errors/NaNs\n",
        "        res_nums = [\n",
        "            int(row['Coord_residue_number1']) if pd.notna(row['Coord_residue_number1']) else -9999,\n",
        "            int(row['Coord_residue_number2']) if pd.notna(row['Coord_residue_number2']) else -9999,\n",
        "            int(row['Coord_residue_number3']) if pd.notna(row['Coord_residue_number3']) else -9999,\n",
        "        ]\n",
        "        # Only return tuple if all 3 numbers were valid (not placeholder)\n",
        "        return tuple(sorted(res_nums)) if -9999 not in res_nums else None\n",
        "    except (TypeError, ValueError):\n",
        "        return None # Indicate error or invalid format\n",
        "\n",
        "# --- Main Processing Function ---\n",
        "def process_pdb_file(pdb_file_path, output_excel_path):\n",
        "    \"\"\"\n",
        "    Processes a single PDB file. Finds triads meeting spatial, optional number,\n",
        "    and geometric criteria. Extracts CA/CB coordinates. Applies REDUNDANCY REMOVAL.\n",
        "    Saves combined results.\n",
        "    \"\"\" # <-- DOCSTRING UPDATED\n",
        "    pdb_name = os.path.basename(pdb_file_path)\n",
        "    # <-- UPDATED PRINT STATEMENT\n",
        "    print(f\"🔄 Processing: {pdb_name} (Number Filter: {Specific_residue_number if Specific_residue_number != 0 else 'OFF'}, Redundancy Removal: ON)\")\n",
        "\n",
        "    # --- Check if thresholds loaded ---\n",
        "    if not all(t is not None for t in [alpha_distance_range, beta_distance_range, ratio_threshold_range, pie_threshold_range]):\n",
        "        print(f\"    ❌ Cannot proceed: Required geometric thresholds were not loaded successfully.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        parser = PDBParser(QUIET=True)\n",
        "        structure = parser.get_structure(\"protein\", pdb_file_path)\n",
        "        if len(structure) > 1:\n",
        "            print(f\"    ⚠️ Warning: Multiple models found in {pdb_name}. Using only the first model (ID: {structure[0].id}).\")\n",
        "        model = structure[0]\n",
        "        chains_dict = {chain.id: chain for chain in model}\n",
        "        if not chains_dict:\n",
        "            print(f\"    ❌ Error: No chains found in model {model.id} of {pdb_name}.\")\n",
        "            return\n",
        "\n",
        "        all_residues_full = [res for chain in chains_dict.values() for res in chain if res.get_id()[0] == \" \"]\n",
        "        print(f\"    Found {len(all_residues_full)} standard residues.\")\n",
        "\n",
        "        # --- KDTree Pre-filtering ---\n",
        "        residues_for_tree = [res for res in all_residues_full if res.has_id(\"CA\")]\n",
        "        if len(residues_for_tree) < 3:\n",
        "            print(f\"    ⚠️ Skipping {pdb_name}: Not enough residues (<3) with CA atoms.\")\n",
        "            return\n",
        "        coords_ca = np.array([res[\"CA\"].coord for res in residues_for_tree])\n",
        "        residue_map = residues_for_tree\n",
        "        kdtree = KDTree(coords_ca)\n",
        "        max_dist = alpha_distance_range[1] * 1.1\n",
        "        pairs = kdtree.query_pairs(r=max_dist)\n",
        "        potential_triad_indices = set()\n",
        "        for i, j in pairs:\n",
        "            indices_k_near_i = kdtree.query_ball_point(coords_ca[i], r=max_dist)\n",
        "            indices_k_near_j = kdtree.query_ball_point(coords_ca[j], r=max_dist)\n",
        "            common_neighbors = set(indices_k_near_i).intersection(indices_k_near_j)\n",
        "            for k in common_neighbors:\n",
        "                if k != i and k != j:\n",
        "                    triad_indices = tuple(sorted((i, j, k)))\n",
        "                    potential_triad_indices.add(triad_indices)\n",
        "        print(f\"    Generated {len(potential_triad_indices)} unique potential spatial triads.\")\n",
        "\n",
        "        # --- Map Indices to Residues ---\n",
        "        all_spatial_triads = []\n",
        "        for idx_i, idx_j, idx_k in potential_triad_indices:\n",
        "            if all(idx < len(residue_map) for idx in [idx_i, idx_j, idx_k]):\n",
        "                comb = (residue_map[idx_i], residue_map[idx_j], residue_map[idx_k])\n",
        "                all_spatial_triads.append(comb)\n",
        "            else: print(f\"    ⚠️ Warning: KDTree index out of bounds. Skipping triad indices {(idx_i, idx_j, idx_k)}.\")\n",
        "        print(f\"    Mapped {len(all_spatial_triads)} spatial triads.\")\n",
        "\n",
        "        # --- Apply Residue NUMBER Filter ---\n",
        "        triads_meeting_number_criteria = []\n",
        "\n",
        "        if Specific_residue_number != 0:\n",
        "            print(f\"    🔍 Filtering by specific residue number: {Specific_residue_number}\")\n",
        "            for comb in all_spatial_triads:\n",
        "                if any(res.get_id()[1] == Specific_residue_number for res in comb):\n",
        "                    triads_meeting_number_criteria.append(comb)\n",
        "            print(f\"    ✅ {len(triads_meeting_number_criteria)} triads matched the specific residue number.\")\n",
        "        elif Use_range_filter:\n",
        "            print(f\"    🔍 Filtering by residue number range: {Starting_residue_number} to {Last_residue_number}\")\n",
        "            for comb in all_spatial_triads:\n",
        "                if any(Starting_residue_number <= res.get_id()[1] <= Last_residue_number for res in comb):\n",
        "                    triads_meeting_number_criteria.append(comb)\n",
        "            print(f\"    ✅ {len(triads_meeting_number_criteria)} triads matched the residue number range.\")\n",
        "        else:\n",
        "            print(f\"    🔍 No residue number filtering applied.\")\n",
        "            triads_meeting_number_criteria = all_spatial_triads\n",
        "\n",
        "        # --- Final list for geometric checks ---\n",
        "        final_triads_to_process = triads_meeting_number_criteria\n",
        "        print(f\"    Proceeding to detailed geometric checks for {len(final_triads_to_process)} triads.\")\n",
        "\n",
        "        # --- Detailed Geometric Filtering ---\n",
        "        def get_triad_type(comb):\n",
        "            chains = [res.get_full_id()[2] for res in comb]\n",
        "            return \"intra\" if len(set(chains)) == 1 else \"inter\"\n",
        "        results = []\n",
        "        for comb in final_triads_to_process:\n",
        "            try:\n",
        "                if not all(res.has_id(\"CA\") and res.has_id(\"CB\") for res in comb): continue\n",
        "                alpha_distances, beta_distances = [], []\n",
        "                valid_distances = True\n",
        "                for res1, res2 in itertools.combinations(comb, 2):\n",
        "                    d_ca = np.linalg.norm(res1[\"CA\"].coord - res2[\"CA\"].coord)\n",
        "                    d_cb = np.linalg.norm(res1[\"CB\"].coord - res2[\"CB\"].coord)\n",
        "                    if not (alpha_distance_range[0] <= d_ca <= alpha_distance_range[1] and \\\n",
        "                            beta_distance_range[0] <= d_cb <= beta_distance_range[1]):\n",
        "                        valid_distances = False; break\n",
        "                    alpha_distances.append(d_ca); beta_distances.append(d_cb)\n",
        "                if valid_distances and len(alpha_distances) == 3:\n",
        "                    row = {\"PDB_ID\": pdb_name, \"Triad_Type\": get_triad_type(comb)}\n",
        "                    for i, res in enumerate(comb):\n",
        "                        full_id = res.get_full_id()\n",
        "                        row[f\"Coord_chain_id_number{i+1}\"] = full_id[2]\n",
        "                        row[f\"Coord_residue_number{i+1}\"] = res.get_id()[1]\n",
        "                        row[f\"Coord_residue_name{i+1}\"] = res.get_resname()\n",
        "                    for i in range(3):\n",
        "                        row[f\"Alpha Distance {i+1}\"] = alpha_distances[i]\n",
        "                        row[f\"Beta Distance {i+1}\"] = beta_distances[i]\n",
        "                    results.append(row)\n",
        "            except Exception: continue # Skip triad on any error during check\n",
        "\n",
        "        print(f\"    Found {len(results)} triads passing initial distance filters.\")\n",
        "        df = pd.DataFrame(results)\n",
        "\n",
        "        # --- Apply Ratio Filter ---\n",
        "        if not df.empty:\n",
        "            def pass_ratio(row):\n",
        "                try:\n",
        "                    for i in range(3):\n",
        "                        if row[f\"Beta Distance {i+1}\"] is None or row[f\"Beta Distance {i+1}\"] == 0: return False\n",
        "                        ratio = row[f\"Alpha Distance {i+1}\"] / row[f\"Beta Distance {i+1}\"]\n",
        "                        if not (ratio_threshold_range[0] <= ratio <= ratio_threshold_range[1]): return False\n",
        "                    return True\n",
        "                except Exception: return False\n",
        "            df_ratio = df[df.apply(pass_ratio, axis=1)].copy()\n",
        "            print(f\"    Found {len(df_ratio)} triads passing ratio filter.\")\n",
        "        else: df_ratio = pd.DataFrame()\n",
        "\n",
        "        # --- Apply Pie Angle Filter ---\n",
        "        if not df_ratio.empty:\n",
        "            def compute_pie(row):\n",
        "                try:\n",
        "                    chain1 = chains_dict.get(row['Coord_chain_id_number1']); chain2 = chains_dict.get(row['Coord_chain_id_number2']); chain3 = chains_dict.get(row['Coord_chain_id_number3'])\n",
        "                    if not all([chain1, chain2, chain3]): return pd.Series([np.nan]*3, index=[\"Pie_1_2\", \"Pie_1_3\", \"Pie_2_3\"])\n",
        "                    res1 = chain1[(' ', row['Coord_residue_number1'], ' ')] if (' ', row['Coord_residue_number1'], ' ') in chain1 else None\n",
        "                    res2 = chain2[(' ', row['Coord_residue_number2'], ' ')] if (' ', row['Coord_residue_number2'], ' ') in chain2 else None\n",
        "                    res3 = chain3[(' ', row['Coord_residue_number3'], ' ')] if (' ', row['Coord_residue_number3'], ' ') in chain3 else None\n",
        "                    if not all([res1, res2, res3]): return pd.Series([np.nan]*3, index=[\"Pie_1_2\", \"Pie_1_3\", \"Pie_2_3\"])\n",
        "                    res_objs = [res1, res2, res3]; angles = []\n",
        "                    for i, j in [(0,1), (0,2), (1,2)]:\n",
        "                        if not (res_objs[i].has_id(\"CA\") and res_objs[i].has_id(\"CB\") and res_objs[j].has_id(\"CA\") and res_objs[j].has_id(\"CB\")): angles.append(np.nan); continue\n",
        "                        v_ca = res_objs[j][\"CA\"].coord - res_objs[i][\"CA\"].coord; v_cb = res_objs[j][\"CB\"].coord - res_objs[i][\"CB\"].coord\n",
        "                        angles.append(calculate_pie(v_ca, v_cb))\n",
        "                    while len(angles) < 3: angles.append(np.nan)\n",
        "                    return pd.Series(angles, index=[\"Pie_1_2\", \"Pie_1_3\", \"Pie_2_3\"])\n",
        "                except Exception: return pd.Series([np.nan]*3, index=[\"Pie_1_2\", \"Pie_1_3\", \"Pie_2_3\"])\n",
        "            pie_results = df_ratio.apply(compute_pie, axis=1)\n",
        "            df_ratio[[\"Pie_1_2\", \"Pie_1_3\", \"Pie_2_3\"]] = pie_results\n",
        "            for col in [\"Pie_1_2\", \"Pie_1_3\", \"Pie_2_3\"]:\n",
        "                df_ratio[f\"{col}_Filter\"] = df_ratio[col].apply(lambda x: pie_threshold_range[0] < x < pie_threshold_range[1] if pd.notnull(x) else False)\n",
        "            df_ratio['Pie_Filter'] = df_ratio[[f'{col}_Filter' for col in ['Pie_1_2', 'Pie_1_3', 'Pie_2_3']]].all(axis=1)\n",
        "            df_final = df_ratio[df_ratio['Pie_Filter']].copy()\n",
        "            print(f\"    Found {len(df_final)} triads passing pie angle filter.\")\n",
        "        else: df_final = pd.DataFrame()\n",
        "\n",
        "        # --- Coordinate Extraction ---\n",
        "        df_with_coords = pd.DataFrame()\n",
        "        if not df_final.empty:\n",
        "            print(f\"    Extracting CA and CB coordinates for {len(df_final)} final triads...\")\n",
        "            ca_coords, cb_coords = [], []\n",
        "            for idx, row in df_final.iterrows():\n",
        "                chain1 = chains_dict.get(row['Coord_chain_id_number1']); chain2 = chains_dict.get(row['Coord_chain_id_number2']); chain3 = chains_dict.get(row['Coord_chain_id_number3'])\n",
        "                ca1 = extract_coordinates(chain1, row['Coord_residue_number1'], 'CA'); ca2 = extract_coordinates(chain2, row['Coord_residue_number2'], 'CA'); ca3 = extract_coordinates(chain3, row['Coord_residue_number3'], 'CA')\n",
        "                ca_coords.append([*ca1, *ca2, *ca3])\n",
        "                cb1 = extract_coordinates(chain1, row['Coord_residue_number1'], 'CB'); cb2 = extract_coordinates(chain2, row['Coord_residue_number2'], 'CB'); cb3 = extract_coordinates(chain3, row['Coord_residue_number3'], 'CB')\n",
        "                cb_coords.append([*cb1, *cb2, *cb3])\n",
        "            ca_cols = ['CA1_X', 'CA1_Y', 'CA1_Z', 'CA2_X', 'CA2_Y', 'CA2_Z', 'CA3_X', 'CA3_Y', 'CA3_Z']\n",
        "            cb_cols = ['CB1_X', 'CB1_Y', 'CB1_Z', 'CB2_X', 'CB2_Y', 'CB2_Z', 'CB3_X', 'CB3_Y', 'CB3_Z']\n",
        "            df_ca = pd.DataFrame(ca_coords, columns=ca_cols); df_cb = pd.DataFrame(cb_coords, columns=cb_cols)\n",
        "            df_with_coords = pd.concat([df_final.reset_index(drop=True), df_ca, df_cb], axis=1)\n",
        "            print(f\"    Coordinate extraction complete.\")\n",
        "        else:\n",
        "            print(\"    No triads passed all filters, skipping coordinate extraction.\")\n",
        "            df_with_coords = df_final\n",
        "\n",
        "        # --- *** Deduplication Step *** --- ADDED\n",
        "        if not df_with_coords.empty:\n",
        "            print(f\"    Applying deduplication based on residue number combinations...\")\n",
        "            initial_rows_dedupe = len(df_with_coords)\n",
        "            res_num_cols_for_dedupe = ['Coord_residue_number1', 'Coord_residue_number2', 'Coord_residue_number3']\n",
        "\n",
        "            if not all(col in df_with_coords.columns for col in res_num_cols_for_dedupe):\n",
        "                 print(\"      Warning: Skipping deduplication - required residue number columns missing.\")\n",
        "            else:\n",
        "                 df_with_coords['temp_triad_identity'] = df_with_coords.apply(standardize_residue_identity, axis=1)\n",
        "                 df_with_coords.dropna(subset=['temp_triad_identity'], inplace=True)\n",
        "                 rows_after_dropna = len(df_with_coords)\n",
        "                 if rows_after_dropna < initial_rows_dedupe: print(f\"      Removed {initial_rows_dedupe - rows_after_dropna} rows with invalid residue numbers before deduplication.\")\n",
        "                 # Keep the first occurrence (no score column here to sort by)\n",
        "                 df_with_coords.drop_duplicates(subset=['temp_triad_identity'], keep='first', inplace=True)\n",
        "                 df_with_coords.drop(columns=['temp_triad_identity'], inplace=True)\n",
        "                 print(f\"      DataFrame reduced to {len(df_with_coords)} rows after deduplication.\")\n",
        "        else:\n",
        "             print(\"    Skipping deduplication as no triads passed previous filters.\")\n",
        "        # --- *** End Deduplication Step *** ---\n",
        "\n",
        "\n",
        "        # --- Output ---\n",
        "        # Save the potentially deduplicated df_with_coords\n",
        "        if not df_with_coords.empty:\n",
        "            print(f\"    Saving final results with coordinates...\")\n",
        "            with pd.ExcelWriter(output_excel_path) as writer:\n",
        "                num_suffix = f\"_Num{Specific_residue_number}\" if Specific_residue_number != 0 else \"_ANY_Num\"\n",
        "                sheet_name = f\"Final_Coords{num_suffix}\" # Keep sheet name relatively short\n",
        "                df_with_coords.to_excel(writer, sheet_name=sheet_name[:31], index=False) # Limit sheet name length\n",
        "            print(f\"✅ Finished: {pdb_name}. Results with coordinates saved to {output_excel_path} (Unique triads found after filters & dedupe: {len(df_with_coords)})\") # Updated count description\n",
        "        else:\n",
        "            print(f\"✅ Finished: {pdb_name}. No triads passed all filters. No output file created.\")\n",
        "\n",
        "    except FileNotFoundError: print(f\"❌ Error: Input PDB file not found at {pdb_file_path}\")\n",
        "    except Exception as e: print(f\"❌ An unexpected error occurred while processing {pdb_name}: {e}\"); traceback.print_exc()\n",
        "\n",
        "\n",
        "# --- Run Processing for the Single PDB File ---\n",
        "if __name__ == \"__main__\":\n",
        "    try: kst = pytz.timezone('Asia/Seoul'); current_time_kst = datetime.datetime.now(kst)\n",
        "    except ImportError: kst = None; current_time_kst = datetime.datetime.now()\n",
        "\n",
        "    print(f\"\\n--- Starting Single File Prescreening & Coordinate Extraction ---\")\n",
        "    print(f\"Current Time: {current_time_kst.strftime('%Y-%m-%d %H:%M:%S %Z%z')}\")\n",
        "    print(f\"Input PDB File: {Target_pdb_file}\")\n",
        "    print(f\"Specific Number Filter: {Specific_residue_number if Specific_residue_number != 0 else 'OFF'}\")\n",
        "    print(f\"Threshold Set Used: {Metal} / {Ligands} / Range {Threshold }\")\n",
        "    print(f\"Outputting results to file: {Output_excel_file_path_result1}\")\n",
        "    print(\"Redundancy Removal: ON\") #<-- UPDATED\n",
        "\n",
        "    if not os.path.isfile(Target_pdb_file):\n",
        "        print(f\"⚠️ Error: Input PDB file not found at '{Target_pdb_file}'. Please check the path.\")\n",
        "    else:\n",
        "        process_pdb_file(Target_pdb_file, Output_excel_file_path_result1)\n",
        "\n",
        "    print(\"\\n🎉 Processing finished.\")"
      ],
      "metadata": {
        "id": "HGRkfIJd_lC8",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown  # Step 3B: Filter using geometric parameters (select all for multimers).\n",
        "# pip install scipy pytz pandas openpyxl BioPython\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from Bio.PDB import PDBParser # For PDB parsing\n",
        "from Bio.PDB.Residue import Residue # For type hinting if needed\n",
        "from Bio.PDB.Chain import Chain # For type hinting if needed\n",
        "import itertools\n",
        "import requests\n",
        "import traceback\n",
        "from scipy.spatial import KDTree\n",
        "import datetime\n",
        "import pytz\n",
        "from typing import List, Optional, Tuple # For type hinting\n",
        "\n",
        "# --- Configuration ---\n",
        "# Specify the SINGLE PDB file path\n",
        "Target_pdb_file = \"/content/3tis_alanine.pdb\" # @param {type:\"string\"}\n",
        "\n",
        "# Specify Specific Residue Number\n",
        "# @markdown ### Find triad containing at least one following residue number.\n",
        "# @markdown ######  Specify a residue of interest (enter 0 for no prior selection).\n",
        "Specific_residue_number = 301  # @param {type:\"integer\"}\n",
        "# @markdown ### Optional range filter (select cases that include at least one specified residue)\n",
        "Use_range_filter = False   # @param {type:\"boolean\"}\n",
        "Starting_residue_number = 100  # @param {type:\"integer\"} #starting residue\n",
        "Last_residue_number = 180    # @param {type:\"integer\"} #last residue number\n",
        "# --- Specify Exact Output Excel File Path ---\n",
        "Output_excel_file_path_result1 = \"/content/3tis_alanine_3His.xlsx\"# @param {type:\"string\"}\n",
        "\n",
        "# --- Determine Output Directory from Output File Path ---\n",
        "output_directory = os.path.dirname(Output_excel_file_path_result1)\n",
        "if output_directory:\n",
        "    os.makedirs(output_directory, exist_ok=True)\n",
        "    print(f\"➡️ Output directory: {output_directory}\")\n",
        "else:\n",
        "    output_directory = \".\"\n",
        "    print(f\"➡️ Output directory: Current directory\")\n",
        "\n",
        "print(f\"➡️ Output Excel file with coordinates will be saved as: {Output_excel_file_path_result1}\")\n",
        "\n",
        "if Specific_residue_number != 0:\n",
        "    print(f\"⚠️ Filtering by residue number '{Specific_residue_number}'.\")\n",
        "\n",
        "\n",
        "# --- Download threshold configuration ---\n",
        "# Thresholds file will be saved in the same directory as the output Excel file\n",
        "base_url = \"https://raw.githubusercontent.com/SNU-Songlab/Metal-Installer-code/main/Threshold\"\n",
        "Metal = 'Cu'  # @param [\"Zn\", \"Mn\", \"Cu\", \"Fe\"]\n",
        "Ligands = \"3His_only_for_Zn_Cu\"  # @param [\"3His_only_for_Zn_Cu\", \"2His_1Asp_only_for_Zn_Mn_Fe\", \"2His_1Glu_only_for_Zn_Mn_Fe\", \"2His_1Cys_only_for_Cu\"]\n",
        "Threshold = '3'  # @param [\"1\", \"2\", \"3\", \"4\",\"5\"]\n",
        "thresholds_url = f\"{base_url}/{Metal}/{Ligands}/{Threshold}.xlsx\"\n",
        "thresholds_file = os.path.join(output_directory, f\"thresholds_{Metal}_{Ligands}_R{Threshold}.xlsx\")\n",
        "\n",
        "print(f\"⬇️ Downloading threshold set '{Ligands}' for '{Metal}' (Threshold {Threshold}) from: {thresholds_url}\")\n",
        "print(f\"⚠️ NOTE: These thresholds will be applied to any found triad (regardless of residue type).\")\n",
        "\n",
        "response = requests.get(thresholds_url)\n",
        "if response.status_code == 200:\n",
        "    with open(thresholds_file, \"wb\") as file:\n",
        "        file.write(response.content)\n",
        "    print(f\"✅ Thresholds downloaded successfully to {thresholds_file}\")\n",
        "else:\n",
        "    raise ValueError(f\"❌ Failed to download thresholds from {thresholds_url}. Status code: {response.status_code}\")\n",
        "\n",
        "# --- Load thresholds ---\n",
        "print(\"⚙️ Loading thresholds...\")\n",
        "try:\n",
        "    thresholds_df = pd.read_excel(thresholds_file, sheet_name=\"Sheet1\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"❌ Error: Thresholds file not found at {thresholds_file}. Cannot load thresholds.\")\n",
        "    raise # Re-raise the error to stop execution\n",
        "thresholds = {\n",
        "    row[\"Parameter\"]: (row[\"Min\"], row[\"Max\"])\n",
        "    for _, row in thresholds_df.iterrows()\n",
        "    if pd.notna(row[\"Min\"]) and pd.notna(row[\"Max\"])\n",
        "}\n",
        "required_thresholds = [\"alpha_distance_range\", \"beta_distance_range\", \"ratio_threshold_range\", \"pie_threshold_range\"]\n",
        "if not all(key in thresholds for key in required_thresholds):\n",
        "    missing = [key for key in required_thresholds if key not in thresholds]\n",
        "    raise ValueError(f\"❌ Missing required thresholds in the downloaded file: {missing}\")\n",
        "\n",
        "# Only define if present, otherwise geometry checks later will fail if needed\n",
        "alpha_distance_range = thresholds.get(\"alpha_distance_range\")\n",
        "beta_distance_range = thresholds.get(\"beta_distance_range\")\n",
        "ratio_threshold_range = thresholds.get(\"ratio_threshold_range\")\n",
        "pie_threshold_range = thresholds.get(\"pie_threshold_range\")\n",
        "\n",
        "\n",
        "print(\"📊 Thresholds loaded:\")\n",
        "if all(t is not None for t in [alpha_distance_range, beta_distance_range, ratio_threshold_range, pie_threshold_range]):\n",
        "    for key, value in thresholds.items():\n",
        "        print(f\"    - {key}: Min={value[0]}, Max={value[1]}\")\n",
        "else:\n",
        "    print(\"    ⚠️ Warning: Could not load all required thresholds.\")\n",
        "\n",
        "\n",
        "# --- Helper Functions ---\n",
        "def calculate_pie(v1, v2):\n",
        "    \"\"\"Calculates the angle (in degrees) between two vectors.\"\"\"\n",
        "    dot = np.dot(v1, v2)\n",
        "    norm = np.linalg.norm(v1) * np.linalg.norm(v2)\n",
        "    if norm == 0: return np.nan\n",
        "    angle_rad = np.arccos(np.clip(dot / norm, -1.0, 1.0))\n",
        "    return np.degrees(angle_rad)\n",
        "\n",
        "def extract_coordinates(chain: Optional[Chain], res_id: int, atom_name: str) -> List[Optional[float]]:\n",
        "    \"\"\"Safely extracts coordinates for a given atom in a residue.\"\"\"\n",
        "    if chain is None: return [np.nan, np.nan, np.nan]\n",
        "    try:\n",
        "        residue_key = (' ', res_id, ' ')\n",
        "        if residue_key not in chain:\n",
        "            if res_id not in chain: return [np.nan, np.nan, np.nan]\n",
        "            else: residue = chain[res_id]\n",
        "        else: residue = chain[residue_key]\n",
        "        if atom_name not in residue: return [np.nan, np.nan, np.nan]\n",
        "        coord = residue[atom_name].coord\n",
        "        if coord is None or len(coord) != 3: return [np.nan, np.nan, np.nan]\n",
        "        return [float(c) for c in coord]\n",
        "    except Exception: return [np.nan, np.nan, np.nan]\n",
        "\n",
        "# --- Helper function for Deduplication --- ADDED\n",
        "def standardize_residue_identity(row):\n",
        "    \"\"\"Creates a sorted tuple of residue numbers for unique identification.\"\"\"\n",
        "    try:\n",
        "        # Extract numbers, attempt conversion to int, use placeholder for errors/NaNs\n",
        "        res_nums = [\n",
        "            int(row['Coord_residue_number1']) if pd.notna(row['Coord_residue_number1']) else -9999,\n",
        "            int(row['Coord_residue_number2']) if pd.notna(row['Coord_residue_number2']) else -9999,\n",
        "            int(row['Coord_residue_number3']) if pd.notna(row['Coord_residue_number3']) else -9999,\n",
        "        ]\n",
        "        # Only return tuple if all 3 numbers were valid (not placeholder)\n",
        "        return tuple(sorted(res_nums)) if -9999 not in res_nums else None\n",
        "    except (TypeError, ValueError):\n",
        "        return None # Indicate error or invalid format\n",
        "\n",
        "# --- Main Processing Function ---\n",
        "def process_pdb_file(pdb_file_path, output_excel_path):\n",
        "    \"\"\"\n",
        "    Processes a single PDB file. Finds triads meeting spatial, optional number,\n",
        "    and geometric criteria. Extracts CA/CB coordinates. Applies REDUNDANCY REMOVAL.\n",
        "    Saves combined results.\n",
        "    \"\"\" # <-- DOCSTRING UPDATED\n",
        "    pdb_name = os.path.basename(pdb_file_path)\n",
        "    # <-- UPDATED PRINT STATEMENT\n",
        "    print(f\"🔄 Processing: {pdb_name} (Number Filter: {Specific_residue_number if Specific_residue_number != 0 else 'OFF'}, Redundancy Removal: ON)\")\n",
        "\n",
        "    # --- Check if thresholds loaded ---\n",
        "    if not all(t is not None for t in [alpha_distance_range, beta_distance_range, ratio_threshold_range, pie_threshold_range]):\n",
        "        print(f\"    ❌ Cannot proceed: Required geometric thresholds were not loaded successfully.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        parser = PDBParser(QUIET=True)\n",
        "        structure = parser.get_structure(\"protein\", pdb_file_path)\n",
        "        if len(structure) > 1:\n",
        "            print(f\"    ⚠️ Warning: Multiple models found in {pdb_name}. Using only the first model (ID: {structure[0].id}).\")\n",
        "        model = structure[0]\n",
        "        chains_dict = {chain.id: chain for chain in model}\n",
        "        if not chains_dict:\n",
        "            print(f\"    ❌ Error: No chains found in model {model.id} of {pdb_name}.\")\n",
        "            return\n",
        "\n",
        "        all_residues_full = [res for chain in chains_dict.values() for res in chain if res.get_id()[0] == \" \"]\n",
        "        print(f\"    Found {len(all_residues_full)} standard residues.\")\n",
        "\n",
        "        # --- KDTree Pre-filtering ---\n",
        "        residues_for_tree = [res for res in all_residues_full if res.has_id(\"CA\")]\n",
        "        if len(residues_for_tree) < 3:\n",
        "            print(f\"    ⚠️ Skipping {pdb_name}: Not enough residues (<3) with CA atoms.\")\n",
        "            return\n",
        "        coords_ca = np.array([res[\"CA\"].coord for res in residues_for_tree])\n",
        "        residue_map = residues_for_tree\n",
        "        kdtree = KDTree(coords_ca)\n",
        "        max_dist = alpha_distance_range[1] * 1.1\n",
        "        pairs = kdtree.query_pairs(r=max_dist)\n",
        "        potential_triad_indices = set()\n",
        "        for i, j in pairs:\n",
        "            indices_k_near_i = kdtree.query_ball_point(coords_ca[i], r=max_dist)\n",
        "            indices_k_near_j = kdtree.query_ball_point(coords_ca[j], r=max_dist)\n",
        "            common_neighbors = set(indices_k_near_i).intersection(indices_k_near_j)\n",
        "            for k in common_neighbors:\n",
        "                if k != i and k != j:\n",
        "                    triad_indices = tuple(sorted((i, j, k)))\n",
        "                    potential_triad_indices.add(triad_indices)\n",
        "        print(f\"    Generated {len(potential_triad_indices)} unique potential spatial triads.\")\n",
        "\n",
        "        # --- Map Indices to Residues ---\n",
        "        all_spatial_triads = []\n",
        "        for idx_i, idx_j, idx_k in potential_triad_indices:\n",
        "            if all(idx < len(residue_map) for idx in [idx_i, idx_j, idx_k]):\n",
        "                comb = (residue_map[idx_i], residue_map[idx_j], residue_map[idx_k])\n",
        "                all_spatial_triads.append(comb)\n",
        "            else: print(f\"    ⚠️ Warning: KDTree index out of bounds. Skipping triad indices {(idx_i, idx_j, idx_k)}.\")\n",
        "        print(f\"    Mapped {len(all_spatial_triads)} spatial triads.\")\n",
        "\n",
        "        # --- Apply Residue NUMBER Filter ---\n",
        "        triads_meeting_number_criteria = []\n",
        "\n",
        "        if Specific_residue_number != 0:\n",
        "            print(f\"    🔍 Filtering by specific residue number: {Specific_residue_number}\")\n",
        "            for comb in all_spatial_triads:\n",
        "                if any(res.get_id()[1] == Specific_residue_number for res in comb):\n",
        "                    triads_meeting_number_criteria.append(comb)\n",
        "            print(f\"    ✅ {len(triads_meeting_number_criteria)} triads matched the specific residue number.\")\n",
        "        elif Use_range_filter:\n",
        "            print(f\"    🔍 Filtering by residue number range: {Starting_residue_number} to {Last_residue_number}\")\n",
        "            for comb in all_spatial_triads:\n",
        "                if any(Starting_residue_number <= res.get_id()[1] <= Last_residue_number for res in comb):\n",
        "                    triads_meeting_number_criteria.append(comb)\n",
        "            print(f\"    ✅ {len(triads_meeting_number_criteria)} triads matched the residue number range.\")\n",
        "        else:\n",
        "            print(f\"    🔍 No residue number filtering applied.\")\n",
        "            triads_meeting_number_criteria = all_spatial_triads\n",
        "\n",
        "        # --- Final list for geometric checks ---\n",
        "        final_triads_to_process = triads_meeting_number_criteria\n",
        "        print(f\"    Proceeding to detailed geometric checks for {len(final_triads_to_process)} triads.\")\n",
        "\n",
        "        # --- Detailed Geometric Filtering ---\n",
        "        def get_triad_type(comb):\n",
        "            chains = [res.get_full_id()[2] for res in comb]\n",
        "            return \"intra\" if len(set(chains)) == 1 else \"inter\"\n",
        "        results = []\n",
        "        for comb in final_triads_to_process:\n",
        "            try:\n",
        "                if not all(res.has_id(\"CA\") and res.has_id(\"CB\") for res in comb): continue\n",
        "                alpha_distances, beta_distances = [], []\n",
        "                valid_distances = True\n",
        "                for res1, res2 in itertools.combinations(comb, 2):\n",
        "                    d_ca = np.linalg.norm(res1[\"CA\"].coord - res2[\"CA\"].coord)\n",
        "                    d_cb = np.linalg.norm(res1[\"CB\"].coord - res2[\"CB\"].coord)\n",
        "                    if not (alpha_distance_range[0] <= d_ca <= alpha_distance_range[1] and \\\n",
        "                            beta_distance_range[0] <= d_cb <= beta_distance_range[1]):\n",
        "                        valid_distances = False; break\n",
        "                    alpha_distances.append(d_ca); beta_distances.append(d_cb)\n",
        "                if valid_distances and len(alpha_distances) == 3:\n",
        "                    row = {\"PDB_ID\": pdb_name, \"Triad_Type\": get_triad_type(comb)}\n",
        "                    for i, res in enumerate(comb):\n",
        "                        full_id = res.get_full_id()\n",
        "                        row[f\"Coord_chain_id_number{i+1}\"] = full_id[2]\n",
        "                        row[f\"Coord_residue_number{i+1}\"] = res.get_id()[1]\n",
        "                        row[f\"Coord_residue_name{i+1}\"] = res.get_resname()\n",
        "                    for i in range(3):\n",
        "                        row[f\"Alpha Distance {i+1}\"] = alpha_distances[i]\n",
        "                        row[f\"Beta Distance {i+1}\"] = beta_distances[i]\n",
        "                    results.append(row)\n",
        "            except Exception: continue # Skip triad on any error during check\n",
        "\n",
        "        print(f\"    Found {len(results)} triads passing initial distance filters.\")\n",
        "        df = pd.DataFrame(results)\n",
        "\n",
        "        # --- Apply Ratio Filter ---\n",
        "        if not df.empty:\n",
        "            def pass_ratio(row):\n",
        "                try:\n",
        "                    for i in range(3):\n",
        "                        if row[f\"Beta Distance {i+1}\"] is None or row[f\"Beta Distance {i+1}\"] == 0: return False\n",
        "                        ratio = row[f\"Alpha Distance {i+1}\"] / row[f\"Beta Distance {i+1}\"]\n",
        "                        if not (ratio_threshold_range[0] <= ratio <= ratio_threshold_range[1]): return False\n",
        "                    return True\n",
        "                except Exception: return False\n",
        "            df_ratio = df[df.apply(pass_ratio, axis=1)].copy()\n",
        "            print(f\"    Found {len(df_ratio)} triads passing ratio filter.\")\n",
        "        else: df_ratio = pd.DataFrame()\n",
        "\n",
        "        # --- Apply Pie Angle Filter ---\n",
        "        if not df_ratio.empty:\n",
        "            def compute_pie(row):\n",
        "                try:\n",
        "                    chain1 = chains_dict.get(row['Coord_chain_id_number1']); chain2 = chains_dict.get(row['Coord_chain_id_number2']); chain3 = chains_dict.get(row['Coord_chain_id_number3'])\n",
        "                    if not all([chain1, chain2, chain3]): return pd.Series([np.nan]*3, index=[\"Pie_1_2\", \"Pie_1_3\", \"Pie_2_3\"])\n",
        "                    res1 = chain1[(' ', row['Coord_residue_number1'], ' ')] if (' ', row['Coord_residue_number1'], ' ') in chain1 else None\n",
        "                    res2 = chain2[(' ', row['Coord_residue_number2'], ' ')] if (' ', row['Coord_residue_number2'], ' ') in chain2 else None\n",
        "                    res3 = chain3[(' ', row['Coord_residue_number3'], ' ')] if (' ', row['Coord_residue_number3'], ' ') in chain3 else None\n",
        "                    if not all([res1, res2, res3]): return pd.Series([np.nan]*3, index=[\"Pie_1_2\", \"Pie_1_3\", \"Pie_2_3\"])\n",
        "                    res_objs = [res1, res2, res3]; angles = []\n",
        "                    for i, j in [(0,1), (0,2), (1,2)]:\n",
        "                        if not (res_objs[i].has_id(\"CA\") and res_objs[i].has_id(\"CB\") and res_objs[j].has_id(\"CA\") and res_objs[j].has_id(\"CB\")): angles.append(np.nan); continue\n",
        "                        v_ca = res_objs[j][\"CA\"].coord - res_objs[i][\"CA\"].coord; v_cb = res_objs[j][\"CB\"].coord - res_objs[i][\"CB\"].coord\n",
        "                        angles.append(calculate_pie(v_ca, v_cb))\n",
        "                    while len(angles) < 3: angles.append(np.nan)\n",
        "                    return pd.Series(angles, index=[\"Pie_1_2\", \"Pie_1_3\", \"Pie_2_3\"])\n",
        "                except Exception: return pd.Series([np.nan]*3, index=[\"Pie_1_2\", \"Pie_1_3\", \"Pie_2_3\"])\n",
        "            pie_results = df_ratio.apply(compute_pie, axis=1)\n",
        "            df_ratio[[\"Pie_1_2\", \"Pie_1_3\", \"Pie_2_3\"]] = pie_results\n",
        "            for col in [\"Pie_1_2\", \"Pie_1_3\", \"Pie_2_3\"]:\n",
        "                df_ratio[f\"{col}_Filter\"] = df_ratio[col].apply(lambda x: pie_threshold_range[0] < x < pie_threshold_range[1] if pd.notnull(x) else False)\n",
        "            df_ratio['Pie_Filter'] = df_ratio[[f'{col}_Filter' for col in ['Pie_1_2', 'Pie_1_3', 'Pie_2_3']]].all(axis=1)\n",
        "            df_final = df_ratio[df_ratio['Pie_Filter']].copy()\n",
        "            print(f\"    Found {len(df_final)} triads passing pie angle filter.\")\n",
        "        else: df_final = pd.DataFrame()\n",
        "\n",
        "        # --- Coordinate Extraction ---\n",
        "        df_with_coords = pd.DataFrame()\n",
        "        if not df_final.empty:\n",
        "            print(f\"    Extracting CA and CB coordinates for {len(df_final)} final triads...\")\n",
        "            ca_coords, cb_coords = [], []\n",
        "            for idx, row in df_final.iterrows():\n",
        "                chain1 = chains_dict.get(row['Coord_chain_id_number1']); chain2 = chains_dict.get(row['Coord_chain_id_number2']); chain3 = chains_dict.get(row['Coord_chain_id_number3'])\n",
        "                ca1 = extract_coordinates(chain1, row['Coord_residue_number1'], 'CA'); ca2 = extract_coordinates(chain2, row['Coord_residue_number2'], 'CA'); ca3 = extract_coordinates(chain3, row['Coord_residue_number3'], 'CA')\n",
        "                ca_coords.append([*ca1, *ca2, *ca3])\n",
        "                cb1 = extract_coordinates(chain1, row['Coord_residue_number1'], 'CB'); cb2 = extract_coordinates(chain2, row['Coord_residue_number2'], 'CB'); cb3 = extract_coordinates(chain3, row['Coord_residue_number3'], 'CB')\n",
        "                cb_coords.append([*cb1, *cb2, *cb3])\n",
        "            ca_cols = ['CA1_X', 'CA1_Y', 'CA1_Z', 'CA2_X', 'CA2_Y', 'CA2_Z', 'CA3_X', 'CA3_Y', 'CA3_Z']\n",
        "            cb_cols = ['CB1_X', 'CB1_Y', 'CB1_Z', 'CB2_X', 'CB2_Y', 'CB2_Z', 'CB3_X', 'CB3_Y', 'CB3_Z']\n",
        "            df_ca = pd.DataFrame(ca_coords, columns=ca_cols); df_cb = pd.DataFrame(cb_coords, columns=cb_cols)\n",
        "            df_with_coords = pd.concat([df_final.reset_index(drop=True), df_ca, df_cb], axis=1)\n",
        "            print(f\"    Coordinate extraction complete.\")\n",
        "        else:\n",
        "            print(\"    No triads passed all filters, skipping coordinate extraction.\")\n",
        "            df_with_coords = df_final\n",
        "        # --- Output ---\n",
        "        # Save the potentially deduplicated df_with_coords\n",
        "        if not df_with_coords.empty:\n",
        "            print(f\"    Saving final results with coordinates...\")\n",
        "            with pd.ExcelWriter(output_excel_path) as writer:\n",
        "                num_suffix = f\"_Num{Specific_residue_number}\" if Specific_residue_number != 0 else \"_ANY_Num\"\n",
        "                sheet_name = f\"Final_Coords{num_suffix}\" # Keep sheet name relatively short\n",
        "                df_with_coords.to_excel(writer, sheet_name=sheet_name[:31], index=False) # Limit sheet name length\n",
        "            print(f\"✅ Finished: {pdb_name}. Results with coordinates saved to {output_excel_path} (Unique triads found after filters & dedupe: {len(df_with_coords)})\") # Updated count description\n",
        "        else:\n",
        "            print(f\"✅ Finished: {pdb_name}. No triads passed all filters. No output file created.\")\n",
        "\n",
        "    except FileNotFoundError: print(f\"❌ Error: Input PDB file not found at {pdb_file_path}\")\n",
        "    except Exception as e: print(f\"❌ An unexpected error occurred while processing {pdb_name}: {e}\"); traceback.print_exc()\n",
        "\n",
        "\n",
        "# --- Run Processing for the Single PDB File ---\n",
        "if __name__ == \"__main__\":\n",
        "    try: kst = pytz.timezone('Asia/Seoul'); current_time_kst = datetime.datetime.now(kst)\n",
        "    except ImportError: kst = None; current_time_kst = datetime.datetime.now()\n",
        "\n",
        "    print(f\"\\n--- Starting Single File Prescreening & Coordinate Extraction ---\")\n",
        "    print(f\"Current Time: {current_time_kst.strftime('%Y-%m-%d %H:%M:%S %Z%z')}\")\n",
        "    print(f\"Input PDB File: {Target_pdb_file}\")\n",
        "    print(f\"Specific Number Filter: {Specific_residue_number if Specific_residue_number != 0 else 'OFF'}\")\n",
        "    print(f\"Threshold Set Used: {Metal} / {Ligands} / Range {Threshold }\")\n",
        "    print(f\"Outputting results to file: {Output_excel_file_path_result1}\")\n",
        "    print(\"Redundancy Removal: ON\") #<-- UPDATED\n",
        "\n",
        "    if not os.path.isfile(Target_pdb_file):\n",
        "        print(f\"⚠️ Error: Input PDB file not found at '{Target_pdb_file}'. Please check the path.\")\n",
        "    else:\n",
        "        process_pdb_file(Target_pdb_file, Output_excel_file_path_result1)\n",
        "\n",
        "    print(\"\\n🎉 Processing finished.\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "8AQOHud9GN5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown # Step 4: Filter using probability density map analysis\n",
        "# pip install scipy pandas openpyxl requests BioPython numpy pytz\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import requests\n",
        "import traceback  # Import traceback for better error printing\n",
        "import multiprocessing  # Import multiprocessing\n",
        "from Bio.PDB import PDBParser  # PDBParser is still needed for structure loading\n",
        "from scipy.spatial import KDTree  # Import KDTree\n",
        "import time  # For timing if desired\n",
        "import datetime\n",
        "import pytz\n",
        "from typing import List, Optional, Tuple  # For type hinting\n",
        "from Bio.PDB.Chain import Chain  # For type hinting if needed\n",
        "\n",
        "# --- Configuration and Setup ---\n",
        "# Define paths for the SINGLE input coordinate Excel file and the corresponding PDB file\n",
        "\n",
        "Input_result1_excel_file = '/content/3tis_alanine_3His.xlsx'  # @param {type:\"string\"}\n",
        "Input_pdb_file = '/content/3tis_alanine.pdb'  # @param {type:\"string\"}\n",
        "Output_result2_excel_file = '/content/3tis_alanine_3His_result.xlsx'  # @param {type:\"string\"}\n",
        "\n",
        "# Create output directory if it doesn't exist\n",
        "output_result_directory = os.path.dirname(Output_result2_excel_file)\n",
        "if output_result_directory:\n",
        "    os.makedirs(output_result_directory, exist_ok=True)\n",
        "    print(f\"➡️ Output directory: {output_result_directory}\")\n",
        "else:\n",
        "    # If no directory specified, output files go in the current directory\n",
        "    output_result_directory = \".\"\n",
        "    print(f\"➡️ Output directory: Current directory\")\n",
        "\n",
        "# Check if input files exist\n",
        "if not os.path.isfile(Input_result1_excel_file):\n",
        "    raise FileNotFoundError(f\"Input coordinate Excel file not found: {Input_result1_excel_file}\")\n",
        "if not os.path.isfile(Input_pdb_file):\n",
        "    raise FileNotFoundError(f\"Input PDB file not found: {Input_pdb_file}\")\n",
        "\n",
        "# Define local file paths for downloaded data\n",
        "prob_map_file = os.path.join(output_result_directory, 'map.xlsx')\n",
        "thresholds_file = os.path.join(output_result_directory, 'threshold.xlsx')\n",
        "\n",
        "# --- Download Data from GitHub ---\n",
        "base_url = \"https://raw.githubusercontent.com/SNU-Songlab/Metal-Installer-code/main/probability/\"\n",
        "Metal = 'Zn'  # @param [\"Zn\", \"Mn\", \"Cu\", \"Fe\"]\n",
        "Combinations = '3His_only_for_Zn_Cu'  # @param [\"3His_only_for_Zn_Cu\", \"2His_1Asp_only_for_Zn_Mn_Fe\", \"2His_1Glu_only_for_Zn_Mn_Fe\", \"2His_1Cys_only_for_Cu\"]\n",
        "map_url = f\"{base_url}/{Metal}/{Combinations}/map.xlsx\"\n",
        "thresholds_url = f\"{base_url}/{Metal}/{Combinations}/threshold.xlsx\"\n",
        "\n",
        "print(f\"Downloading probability map from: {map_url}\")\n",
        "response_map = requests.get(map_url)\n",
        "if response_map.status_code == 200:\n",
        "    with open(prob_map_file, 'wb') as file:\n",
        "        file.write(response_map.content)\n",
        "    print(f\"Downloaded map data to {prob_map_file}\")\n",
        "else:\n",
        "    raise ValueError(f\"Failed to download map file from {map_url}. Status code: {response_map.status_code}\")\n",
        "\n",
        "print(f\"Downloading thresholds from: {thresholds_url}\")\n",
        "response_thresh = requests.get(thresholds_url)\n",
        "if response_thresh.status_code == 200:\n",
        "    with open(thresholds_file, 'wb') as file:\n",
        "        file.write(response_thresh.content)\n",
        "    print(f\"Downloaded thresholds data to {thresholds_file}\")\n",
        "else:\n",
        "    raise ValueError(f\"Failed to download thresholds file from {thresholds_url}. Status code: {response_thresh.status_code}\")\n",
        "\n",
        "# --- Load and Process Data (Load ONCE in the main process) ---\n",
        "print(\"\\n--- Loading Shared Data ---\")\n",
        "# Load PDB Structure\n",
        "pdb_id = os.path.splitext(os.path.basename(Input_pdb_file))[0]\n",
        "print(f\"Loading PDB structure: {pdb_id}...\")\n",
        "pdb_parser = PDBParser(QUIET=True)\n",
        "try:\n",
        "    structure = pdb_parser.get_structure(pdb_id, Input_pdb_file)\n",
        "    print(f\"Loaded structure.\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error loading PDB file {Input_pdb_file}: {e}\")\n",
        "    raise  # Stop if structure cannot be loaded\n",
        "\n",
        "# Load Thresholds\n",
        "print(\"Loading thresholds...\")\n",
        "try:\n",
        "    thresholds_df = pd.read_excel(thresholds_file, sheet_name='Sheet1')\n",
        "except FileNotFoundError:\n",
        "    print(f\"❌ Error: Thresholds file not found at {thresholds_file}\")\n",
        "    raise\n",
        "\n",
        "thresholds = {}\n",
        "for _, row in thresholds_df.iterrows():\n",
        "    parameter = row['Parameter']\n",
        "    min_value = row['Min']\n",
        "    max_value = row['Max']\n",
        "    if pd.notna(min_value) and pd.notna(max_value):\n",
        "        thresholds[parameter] = (min_value, max_value)\n",
        "\n",
        "required_keys = ['ca_distances_calc', 'cb_distances_calc', 'ratio', 'angle']\n",
        "if not all(key in thresholds for key in required_keys):\n",
        "    missing_keys = [key for key in required_keys if key not in thresholds]\n",
        "    raise KeyError(f\"Missing key(s) {missing_keys} in thresholds file.\")\n",
        "print(\"Thresholds loaded.\")\n",
        "\n",
        "# Load and Process Probability Map\n",
        "print(\"Loading probability map...\")\n",
        "try:\n",
        "    df_precomputed_prob_map = pd.read_excel(prob_map_file)\n",
        "except FileNotFoundError:\n",
        "    print(f\"❌ Error: Probability map file not found at {prob_map_file}\")\n",
        "    raise\n",
        "\n",
        "print(\"Processing probability map...\")\n",
        "map_req_cols = ['Calpha_Zn_Dist', 'Cbeta_Zn_Dist', 'CA-Zn-CB_Angle', 'Probability']\n",
        "if not all(col in df_precomputed_prob_map.columns for col in map_req_cols):\n",
        "    missing_map_cols = [col for col in map_req_cols if col not in df_precomputed_prob_map.columns]\n",
        "    raise ValueError(f\"Missing required columns in map file: {missing_map_cols}\")\n",
        "\n",
        "ca_bins = np.sort(df_precomputed_prob_map['Calpha_Zn_Dist'].unique())\n",
        "cb_bins = np.sort(df_precomputed_prob_map['Cbeta_Zn_Dist'].unique())\n",
        "angle_bins = np.sort(df_precomputed_prob_map['CA-Zn-CB_Angle'].unique())\n",
        "\n",
        "try:\n",
        "    pivoted_prob_map = df_precomputed_prob_map.pivot_table(\n",
        "        index='Calpha_Zn_Dist', columns=['Cbeta_Zn_Dist', 'CA-Zn-CB_Angle'], values='Probability', fill_value=0\n",
        "    )\n",
        "    expected_shape = (len(ca_bins), len(cb_bins) * len(angle_bins))\n",
        "    if pivoted_prob_map.shape == expected_shape:\n",
        "        prob_map_3d = pivoted_prob_map.values.reshape((len(ca_bins), len(cb_bins), len(angle_bins)))\n",
        "        print(\"Probability map processed into 3D array.\")\n",
        "    else:\n",
        "        raise ValueError(f\"Pivoted map shape {pivoted_prob_map.shape} doesn't match expected shape {expected_shape} for reshaping.\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error processing probability map: {e}\")\n",
        "    raise\n",
        "\n",
        "# Load Input Coordinate Data\n",
        "print(f\"Loading input coordinate data from: {Input_result1_excel_file}...\")\n",
        "try:\n",
        "    df_sites = pd.read_excel(Input_result1_excel_file)\n",
        "    df_sites.columns = [c.strip() for c in df_sites.columns]\n",
        "except FileNotFoundError:\n",
        "    print(f\"❌ Error: Input coordinate file not found at {Input_result1_excel_file}\")\n",
        "    raise\n",
        "\n",
        "if df_sites.empty:\n",
        "    print(\"⚠️ Input coordinate file is empty. Nothing to process.\")\n",
        "    exit()\n",
        "# Ensure PDB_ID column exists or add it based on filename\n",
        "if 'PDB_ID' not in df_sites.columns:\n",
        "    df_sites['PDB_ID'] = pdb_id\n",
        "\n",
        "print(f\"Loaded {len(df_sites)} candidate sites.\")\n",
        "\n",
        "# --- Helper Function Definitions ---\n",
        "def calculate_ratio(current_point, ca_xyz, cb_xyz):\n",
        "    ca_distances = np.linalg.norm(ca_xyz - current_point, axis=1)\n",
        "    cb_distances = np.linalg.norm(cb_xyz - current_point, axis=1)\n",
        "    ratios = np.divide(ca_distances, cb_distances, out=np.full_like(ca_distances, np.inf), where=cb_distances!=0)\n",
        "    return ratios\n",
        "\n",
        "def calculate_angles(zn_coords, ca_coords_triplet, cb_coords_triplet):\n",
        "    angles = []\n",
        "    for i in range(3):\n",
        "        v_ca = ca_coords_triplet[i] - zn_coords\n",
        "        v_cb = cb_coords_triplet[i] - zn_coords\n",
        "        norm_v_ca = np.linalg.norm(v_ca)\n",
        "        norm_v_cb = np.linalg.norm(v_cb)\n",
        "        if norm_v_ca == 0 or norm_v_cb == 0:\n",
        "            angles.append(np.nan)  # Use NaN for undefined angles\n",
        "            continue\n",
        "        cos_theta = np.clip(np.dot(v_ca, v_cb) / (norm_v_ca * norm_v_cb), -1.0, 1.0)\n",
        "        angle_rad = np.arccos(cos_theta)\n",
        "        angles.append(np.degrees(angle_rad))\n",
        "    while len(angles) < 3:\n",
        "        angles.append(np.nan)\n",
        "    return angles\n",
        "\n",
        "def score_zn_predictions(ca_distances, cb_distances, angles, prob_map_3d, ca_bins, cb_bins, angle_bins):\n",
        "    if np.isnan(ca_distances).any() or np.isnan(cb_distances).any() or np.isnan(angles).any():\n",
        "        return 0.0\n",
        "    ca_bin_indices = np.clip(np.digitize(ca_distances, ca_bins[1:], right=True), 0, len(ca_bins)-1)\n",
        "    cb_bin_indices = np.clip(np.digitize(cb_distances, cb_bins[1:], right=True), 0, len(cb_bins)-1)\n",
        "    angle_bin_indices = np.clip(np.digitize(angles, angle_bins[1:], right=True), 0, len(angle_bins)-1)\n",
        "    probabilities = []\n",
        "    valid = True\n",
        "    try:\n",
        "        probs = prob_map_3d[ca_bin_indices, cb_bin_indices, angle_bin_indices]\n",
        "        if np.any(probs <= 0):\n",
        "            valid = False\n",
        "        else:\n",
        "            probabilities = probs\n",
        "    except IndexError:\n",
        "        valid = False\n",
        "    except Exception:\n",
        "        valid = False\n",
        "    final_score = np.prod(probabilities) if valid and len(probabilities) == 3 else 0.0\n",
        "    return final_score\n",
        "\n",
        "def define_excluded_triads(triad_res_nums, structure):\n",
        "    excluded_residues = set()\n",
        "    for res in triad_res_nums:\n",
        "        if res is not None and isinstance(res, tuple):\n",
        "            excluded_residues.add((str(res[0]), int(res[1])))\n",
        "    return excluded_residues\n",
        "\n",
        "def build_kdtree_excluding_triads(structure, triad_residues):\n",
        "    non_excluded_coords = []\n",
        "    included_ids = []\n",
        "    excluded_details = []\n",
        "    missing_excluded = set(triad_residues)\n",
        "    all_residues = []\n",
        "    for model in structure:\n",
        "        for chain in model:\n",
        "            for residue in chain:\n",
        "                res_id = (chain.id, residue.id[1])\n",
        "                all_residues.append((chain.id, residue.id[1], residue.resname))\n",
        "                if res_id in triad_residues:\n",
        "                    excluded_details.append((chain.id, residue.id[1], residue.resname))\n",
        "                    if res_id in missing_excluded:\n",
        "                        missing_excluded.remove(res_id)\n",
        "                    continue\n",
        "                for atom in residue.get_atoms():\n",
        "                    non_excluded_coords.append(atom.coord)\n",
        "                    included_ids.append((chain.id, residue.id[1], residue.resname, atom.get_name()))\n",
        "    print(\"\\n========== Triad Residue Debugging ==========\")\n",
        "    print(f\"Requested for exclusion: {sorted(triad_residues)}\")\n",
        "    print(f\"Actually excluded residues (found in structure): {excluded_details}\")\n",
        "    if missing_excluded:\n",
        "        print(f\"⚠️ These triad residues were NOT found in structure and thus NOT excluded: {sorted(missing_excluded)}\")\n",
        "    print(f\"Total excluded residues: {len(excluded_details)}\")\n",
        "    print(f\"Total included atoms: {len(non_excluded_coords)}\")\n",
        "    print(f\"First 10 included atoms: {included_ids[:10]}\")\n",
        "    print(\"=============================================\\n\")\n",
        "    return KDTree(np.array(non_excluded_coords, dtype=np.float64)) if non_excluded_coords else None, included_ids\n",
        "\n",
        "def proximity_filter_kdtree(kdtree, zn_candidate, exclusion_radius=2.5, atom_lookup=None):\n",
        "    if kdtree is None:\n",
        "        return True\n",
        "    try:\n",
        "        indices_nearby = kdtree.query_ball_point(zn_candidate, r=exclusion_radius)\n",
        "        if len(indices_nearby) > 0:\n",
        "            print(f\"\\n❗Candidate {zn_candidate} is within {exclusion_radius}Å of {len(indices_nearby)} atom(s):\")\n",
        "            if atom_lookup is not None:\n",
        "                for i in indices_nearby[:5]:  # print up to 5\n",
        "                    print(\"  Nearby atom:\", atom_lookup[i])\n",
        "        return len(indices_nearby) == 0\n",
        "    except Exception as e:\n",
        "        print(f\"Error in proximity_filter_kdtree: {e}\")\n",
        "        return False\n",
        "\n",
        "\n",
        "def estimate_zn_iterative(\n",
        "    ca_coords_site_flat,\n",
        "    cb_coords_site_flat,\n",
        "    site_info,\n",
        "    structure_local,\n",
        "    thresholds_local,\n",
        "    prob_map_3d_local, ca_bins_local, cb_bins_local, angle_bins_local,\n",
        "    grid_resolution=0.2\n",
        "):\n",
        "    try:\n",
        "        ca_coords_numeric = pd.to_numeric(np.asarray(ca_coords_site_flat), errors='coerce')\n",
        "        cb_coords_numeric = pd.to_numeric(np.asarray(cb_coords_site_flat), errors='coerce')\n",
        "        if np.isnan(ca_coords_numeric).any() or np.isnan(cb_coords_numeric).any():\n",
        "            return \"no metal\", 0, [np.nan, np.nan, np.nan]\n",
        "        if ca_coords_numeric.shape != (9,) or cb_coords_numeric.shape != (9,):\n",
        "            return \"no metal\", 0, [np.nan, np.nan, np.nan]\n",
        "        ca_xyz = ca_coords_numeric.astype(np.float64).reshape(3, 3)\n",
        "        cb_xyz = cb_coords_numeric.astype(np.float64).reshape(3, 3)\n",
        "    except (ValueError, TypeError):\n",
        "        return \"no metal\", 0, [np.nan, np.nan, np.nan]\n",
        "\n",
        "    if structure_local is None:\n",
        "        return \"no metal\", 0, [np.nan, np.nan, np.nan]\n",
        "\n",
        "    # Build the triad exclusion set using your actual column names!\n",
        "    triad_residues = []\n",
        "    for i in [1, 2, 3]:\n",
        "        chain = site_info.get(f'Coord_chain_id_number{i}')\n",
        "        resnum = site_info.get(f'Coord_residue_number{i}')\n",
        "        if chain is not None and resnum is not None and pd.notna(chain) and pd.notna(resnum):\n",
        "            triad_residues.append((str(chain).strip(), int(resnum)))\n",
        "    triad_residues_set = set(triad_residues)\n",
        "\n",
        "    # Use this set for exclusion in KDTree\n",
        "    kdtree, atom_lookup = build_kdtree_excluding_triads(structure_local, triad_residues_set)\n",
        "\n",
        "    shared_x_min, shared_x_max = -np.inf, np.inf\n",
        "    shared_y_min, shared_y_max = -np.inf, np.inf\n",
        "    shared_z_min, shared_z_max = -np.inf, np.inf\n",
        "    buffer_dist = max(thresholds_local['ca_distances_calc'][1], thresholds_local['cb_distances_calc'][1])\n",
        "    for j in range(3):\n",
        "        x_min_j, x_max_j = ca_xyz[j, 0] - buffer_dist, ca_xyz[j, 0] + buffer_dist\n",
        "        y_min_j, y_max_j = ca_xyz[j, 1] - buffer_dist, ca_xyz[j, 1] + buffer_dist\n",
        "        z_min_j, z_max_j = ca_xyz[j, 2] - buffer_dist, ca_xyz[j, 2] + buffer_dist\n",
        "        shared_x_min, shared_x_max = max(shared_x_min, x_min_j), min(shared_x_max, x_max_j)\n",
        "        shared_y_min, shared_y_max = max(shared_y_min, y_min_j), min(shared_y_max, y_max_j)\n",
        "        shared_z_min, shared_z_max = max(shared_z_min, z_min_j), min(shared_z_max, z_max_j)\n",
        "    buffer_grid = grid_resolution * 2\n",
        "    shared_x_min, shared_x_max = shared_x_min - buffer_grid, shared_x_max + buffer_grid\n",
        "    shared_y_min, shared_y_max = shared_y_min - buffer_grid, shared_y_max + buffer_grid\n",
        "    shared_z_min, shared_z_max = shared_z_min - buffer_grid, shared_z_max + buffer_grid\n",
        "\n",
        "    if not (shared_x_min < shared_x_max and shared_y_min < shared_y_max and shared_z_min < shared_z_max):\n",
        "        return \"no metal\", 0, [np.nan, np.nan, np.nan]\n",
        "\n",
        "    x_range = np.arange(shared_x_min, shared_x_max, grid_resolution)\n",
        "    y_range = np.arange(shared_y_min, shared_y_max, grid_resolution)\n",
        "    z_range = np.arange(shared_z_min, shared_z_max, grid_resolution)\n",
        "    if not (x_range.size > 0 and y_range.size > 0 and z_range.size > 0):\n",
        "        return \"no metal\", 0, [np.nan, np.nan, np.nan]\n",
        "\n",
        "    best_score = 0.0\n",
        "    best_point = None\n",
        "    best_angles = [np.nan, np.nan, np.nan]\n",
        "\n",
        "    for x in x_range:\n",
        "        for y in y_range:\n",
        "            for z in z_range:\n",
        "                corner = np.array([x, y, z])\n",
        "                center = corner + grid_resolution / 2.0\n",
        "                for point in [corner, center]:\n",
        "                    dist_ca = np.linalg.norm(ca_xyz - point, axis=1)\n",
        "                    dist_cb = np.linalg.norm(cb_xyz - point, axis=1)\n",
        "                    if not (np.all((thresholds_local['ca_distances_calc'][0] <= dist_ca) & (dist_ca <= thresholds_local['ca_distances_calc'][1])) and\n",
        "                            np.all((thresholds_local['cb_distances_calc'][0] <= dist_cb) & (dist_cb <= thresholds_local['cb_distances_calc'][1]))):\n",
        "                        continue\n",
        "                    angles = calculate_angles(point, ca_xyz, cb_xyz)\n",
        "                    if np.isnan(angles).any() or not all(thresholds_local['angle'][0] <= ang <= thresholds_local['angle'][1] for ang in angles if pd.notna(ang)):\n",
        "                        continue\n",
        "                    ratios = calculate_ratio(point, ca_xyz, cb_xyz)\n",
        "                    if np.isinf(ratios).any() or not np.all((thresholds_local['ratio'][0] <= ratios) & (ratios <= thresholds_local['ratio'][1])):\n",
        "                        continue\n",
        "                    score = score_zn_predictions(dist_ca, dist_cb, angles, prob_map_3d_local, ca_bins_local, cb_bins_local, angle_bins_local)\n",
        "                    if score <= 0:\n",
        "                        continue\n",
        "                    if not proximity_filter_kdtree(kdtree, point, exclusion_radius=2.5, atom_lookup=atom_lookup):\n",
        "                        continue\n",
        "                    if score > best_score:\n",
        "                        best_score = score\n",
        "                        best_point = point\n",
        "                        best_angles = angles\n",
        "\n",
        "    if best_point is not None:\n",
        "        return best_point, best_score, best_angles\n",
        "    else:\n",
        "        return \"no metal\", 0, [np.nan, np.nan, np.nan]\n",
        "\n",
        "# --- Worker Function for Multiprocessing ---\n",
        "def process_single_site(args):\n",
        "    site_index, site_data_dict, structure_shared, thresholds_shared, \\\n",
        "        prob_map_3d_shared, ca_bins_shared, cb_bins_shared, angle_bins_shared = args\n",
        "    ca_cols = ['CA1_X', 'CA1_Y', 'CA1_Z', 'CA2_X', 'CA2_Y', 'CA2_Z', 'CA3_X', 'CA3_Y', 'CA3_Z']\n",
        "    cb_cols = ['CB1_X', 'CB1_Y', 'CB1_Z', 'CB2_X', 'CB2_Y', 'CB2_Z', 'CB3_X', 'CB3_Y', 'CB3_Z']\n",
        "    try:\n",
        "        ca_coords_flat = np.array([site_data_dict[col] for col in ca_cols], dtype=np.float64)\n",
        "        cb_coords_flat = np.array([site_data_dict[col] for col in cb_cols], dtype=np.float64)\n",
        "        zn_coords, zn_score, zn_angles = estimate_zn_iterative(\n",
        "            ca_coords_flat, cb_coords_flat, site_data_dict, structure_shared,\n",
        "            thresholds_shared, prob_map_3d_shared, ca_bins_shared,\n",
        "            cb_bins_shared, angle_bins_shared, grid_resolution=0.2\n",
        "        )\n",
        "        return site_index, zn_coords, zn_score, zn_angles\n",
        "    except Exception as e:\n",
        "        return site_index, \"error\", 0, [np.nan, np.nan, np.nan]\n",
        "\n",
        "# --- Main Execution Guard ---\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        kst = pytz.timezone('Asia/Seoul')\n",
        "        current_time_kst = datetime.datetime.now(kst)\n",
        "        print(f\"\\n--- Starting Main Process for Single PDB Site Parallelization ---\")\n",
        "        print(f\"Current Time (KST): {current_time_kst.strftime('%Y-%m-%d %H:%M:%S %Z%z')}\")\n",
        "    except ImportError:\n",
        "        print(\"\\n--- Starting Main Process for Single PDB Site Parallelization ---\")\n",
        "        print(\"Note: Could not determine KST time (pytz not installed?). Run 'pip install pytz' if needed.\")\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    # --- Prepare Tasks for Multiprocessing ---\n",
        "    tasks = []\n",
        "    required_input_cols = (\n",
        "        ['CA1_X', 'CA1_Y', 'CA1_Z', 'CA2_X', 'CA2_Y', 'CA2_Z', 'CA3_X', 'CA3_Y', 'CA3_Z'] +\n",
        "        ['CB1_X', 'CB1_Y', 'CB1_Z', 'CB2_X', 'CB2_Y', 'CB2_Z', 'CB3_X', 'CB3_Y', 'CB3_Z'] +\n",
        "        ['Coord_residue_number1', 'Coord_residue_number2', 'Coord_residue_number3', 'PDB_ID']\n",
        "    )\n",
        "    if not all(col in df_sites.columns for col in required_input_cols):\n",
        "        missing_cols = [col for col in required_input_cols if col not in df_sites.columns]\n",
        "        raise ValueError(f\"Input coordinate Excel file is missing required columns: {missing_cols}\")\n",
        "\n",
        "    # Use all columns for each row!\n",
        "    for index, row in df_sites.iterrows():\n",
        "        site_dict = row.to_dict()\n",
        "        tasks.append((\n",
        "            index,\n",
        "            site_dict,\n",
        "            structure,\n",
        "            thresholds,\n",
        "            prob_map_3d,\n",
        "            ca_bins, cb_bins, angle_bins\n",
        "        ))\n",
        "\n",
        "    print(f\"Prepared {len(tasks)} tasks for parallel processing.\")\n",
        "\n",
        "    # --- Initialize Results Columns ---\n",
        "    df_sites['Zn_X_Grid'] = np.nan\n",
        "    df_sites['Zn_Y_Grid'] = np.nan\n",
        "    df_sites['Zn_Z_Grid'] = np.nan\n",
        "    df_sites['Zn_Score'] = 0.0\n",
        "    df_sites['Angle_1'] = np.nan\n",
        "    df_sites['Angle_2'] = np.nan\n",
        "    df_sites['Angle_3'] = np.nan\n",
        "\n",
        "    num_processes = min(8, os.cpu_count())\n",
        "    print(f\"\\nInitializing multiprocessing pool with {num_processes} workers...\")\n",
        "\n",
        "    processed_count = 0\n",
        "    pool = None\n",
        "    try:\n",
        "        pool = multiprocessing.Pool(processes=num_processes)\n",
        "        print(\"Starting parallel processing of sites...\")\n",
        "        results_iterator = pool.imap_unordered(process_single_site, tasks)\n",
        "        for result in results_iterator:\n",
        "            processed_count += 1\n",
        "            try:\n",
        "                site_idx, zn_coords_res, zn_score_res, zn_angles_res = result\n",
        "                df_sites.loc[site_idx, 'Zn_Score'] = zn_score_res\n",
        "                if isinstance(zn_coords_res, np.ndarray) and zn_coords_res.shape == (3,):\n",
        "                    df_sites.loc[site_idx, 'Zn_X_Grid'] = zn_coords_res[0]\n",
        "                    df_sites.loc[site_idx, 'Zn_Y_Grid'] = zn_coords_res[1]\n",
        "                    df_sites.loc[site_idx, 'Zn_Z_Grid'] = zn_coords_res[2]\n",
        "                if isinstance(zn_angles_res, (list, np.ndarray)) and len(zn_angles_res) == 3:\n",
        "                    df_sites.loc[site_idx, 'Angle_1'] = float(zn_angles_res[0]) if pd.notna(zn_angles_res[0]) else np.nan\n",
        "                    df_sites.loc[site_idx, 'Angle_2'] = float(zn_angles_res[1]) if pd.notna(zn_angles_res[1]) else np.nan\n",
        "                    df_sites.loc[site_idx, 'Angle_3'] = float(zn_angles_res[2]) if pd.notna(zn_angles_res[2]) else np.nan\n",
        "                if processed_count % 50 == 0 or processed_count == len(tasks):\n",
        "                    print(f\"  Processed {processed_count}/{len(tasks)} sites...\")\n",
        "            except Exception as result_error:\n",
        "                print(f\"❌ Error processing result for one site: {result_error}\")\n",
        "                if isinstance(result, tuple) and len(result) > 0:\n",
        "                    print(f\"   Problem occurred for site index: {result[0]}\")\n",
        "                continue\n",
        "        print(f\"Finished processing all {processed_count} assigned tasks.\")\n",
        "    except Exception as pool_error:\n",
        "        print(f\"❌ An error occurred during multiprocessing: {pool_error}\")\n",
        "        print(traceback.format_exc())\n",
        "    finally:\n",
        "        if pool:\n",
        "            pool.close()\n",
        "            pool.join()\n",
        "\n",
        "    print(\"\\n--- Multiprocessing Pool Finished ---\")\n",
        "    print(\"Filtering results (Zn_Score > 0)...\")\n",
        "    df_sites['Zn_Score'] = pd.to_numeric(df_sites['Zn_Score'], errors='coerce').fillna(0.0)\n",
        "    df_output = df_sites[df_sites['Zn_Score'] > 0].copy()\n",
        "    total_sites_saved = len(df_output)\n",
        "\n",
        "    if not df_output.empty:\n",
        "        print(f\"Saving {total_sites_saved} site(s) with positive scores...\")\n",
        "        try:\n",
        "            for col in ['Zn_X_Grid', 'Zn_Y_Grid', 'Zn_Z_Grid', 'Angle_1', 'Angle_2', 'Angle_3']:\n",
        "                if col in df_output.columns:\n",
        "                    df_output[col] = pd.to_numeric(df_output[col], errors='coerce')\n",
        "            df_output.to_excel(Output_result2_excel_file, index=False)\n",
        "            print(f\"✅ Successfully saved results to: {Output_result2_excel_file}\")\n",
        "        except Exception as save_error:\n",
        "            print(f\"❌ Error saving results to Excel file: {save_error}\")\n",
        "            print(traceback.format_exc())\n",
        "    else:\n",
        "        print(\"⚠️ No valid Zn predictions passed filters (Score > 0). No output file created.\")\n",
        "\n",
        "    end_time = time.time()\n",
        "    print(f\"\\n🏁 Processing summary:\")\n",
        "    print(f\"  Total input sites: {len(df_sites)}\")\n",
        "    print(f\"  Total sites processed by pool: {processed_count}\")\n",
        "    print(f\"  Total result sites saved (Score > 0): {total_sites_saved}\")\n",
        "    print(f\"  Total execution time: {end_time - start_time:.2f} seconds\")\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "4oE0UEUkprTC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8VwAlZ7nSEw",
        "outputId": "ad1f02f0-be62-4d9d-e5eb-2163e4666bac",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyMOL script saved to /content/6kgy_3His_result_test3.pml\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from Bio.PDB import PDBParser\n",
        "import requests\n",
        "\n",
        "# Markdown documentation for file pathways\n",
        "\n",
        "# @markdown # Step 5: Analyze output\n",
        "\n",
        "# Load input file\n",
        "input_result2_file_path = \"/content/3tis_alanine_3His_result.xlsx\" # @param {type:\"string\"}\n",
        "df_new = pd.read_excel(input_result2_file_path)\n",
        "\n",
        "# Generate PyMOL script file\n",
        "pymol_script_commands = []\n",
        "df_new['Combination_Number'] = range(1, len(df_new) + 1)\n",
        "\n",
        "# Generate the PyMOL script for both valid and invalid Zn binding forms\n",
        "for index, row in df_new.iterrows():\n",
        "    # Retrieve chain and residue information\n",
        "    chain1, res1 = row['Coord_chain_id_number1'], row['Coord_residue_number1']\n",
        "    chain2, res2 = row['Coord_chain_id_number2'], row['Coord_residue_number2']\n",
        "    chain3, res3 = row['Coord_chain_id_number3'], row['Coord_residue_number3']\n",
        "\n",
        "    # Retrieve Zn coordinates\n",
        "    zn_x, zn_y, zn_z = row['Zn_X_Grid'], row['Zn_Y_Grid'], row['Zn_Z_Grid']\n",
        "\n",
        "    selection_name = f\"obj{row['Combination_Number']:02d}\"\n",
        "\n",
        "    # Select the residues\n",
        "    pymol_script_commands.append(f\"select {selection_name}, (chain {chain1} and resi {res1}) or (chain {chain2} and resi {res2}) or (chain {chain3} and resi {res3})\")\n",
        "\n",
        "    # Create the objects for the residues\n",
        "    pymol_script_commands.append(f\"create {selection_name}_residue1, /{row['PDB_ID']}//{chain1}/{res1}\")\n",
        "    pymol_script_commands.append(f\"create {selection_name}_residue2, /{row['PDB_ID']}//{chain2}/{res2}\")\n",
        "    pymol_script_commands.append(f\"create {selection_name}_residue3, /{row['PDB_ID']}//{chain3}/{res3}\")\n",
        "\n",
        "    # Check if Zn coordinates are available\n",
        "    if not pd.isna(zn_x) and not pd.isna(zn_y) and not pd.isna(zn_z):\n",
        "        # Zn coordinates are present, add the Zn pseudoatom\n",
        "        zn_name = f\"{selection_name}_Metal\"\n",
        "        pymol_script_commands.append(f\"pseudoatom {zn_name}, pos=[{zn_x}, {zn_y}, {zn_z}], elem=Metal, name={zn_name}\")\n",
        "        pymol_script_commands.append(f\"show sphere, {zn_name}\")\n",
        "    else:\n",
        "        # Zn coordinates are missing, mark this combination as non-binding\n",
        "        pymol_script_commands.append(f\"# {selection_name} does not bind Zn\")\n",
        "\n",
        "# Save the commands into a PyMOL script\n",
        "pymol_script_file = \"/content/3tis_alanine_3His_result.pml\" # @param {type:\"string\"}\n",
        "with open(pymol_script_file, 'w') as f:\n",
        "    f.write(\"# PyMOL script for visualizing both Zn-binding and non-binding residue combinations\\n\\n\")\n",
        "    for command in pymol_script_commands:\n",
        "        f.write(command + '\\n')\n",
        "\n",
        "print(f\"PyMOL script saved to {pymol_script_file}\")"
      ]
    }
  ]
}