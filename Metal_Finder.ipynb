{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b07083c95e434000904dcba7236257a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fdf64f8840b247f5a42ff2ca6ecb381f",
              "IPY_MODEL_c84f402c5bde428788516b5fedc4da98",
              "IPY_MODEL_0e2e289bfc5b41f29540d3b4eb166088"
            ],
            "layout": "IPY_MODEL_2c9655d41a314da0a47e4b57a014fb69"
          }
        },
        "fdf64f8840b247f5a42ff2ca6ecb381f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_12eaacccd001430bbe420f4845920d16",
            "placeholder": "​",
            "style": "IPY_MODEL_f0b2f80f6cec484da88c820c9d2f3e72",
            "value": ""
          }
        },
        "c84f402c5bde428788516b5fedc4da98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_47196fea8a884554b6a071bd0bea94e9",
            "max": 100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4cdd3bf7ef744fd2ab4d1492a6d20aa6",
            "value": 100
          }
        },
        "0e2e289bfc5b41f29540d3b4eb166088": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_54fbd3ea54d7467eacc8979a9a0e395c",
            "placeholder": "​",
            "style": "IPY_MODEL_3c0dbc5ee5ec47318c9a9f69f6e02e9e",
            "value": " 120/? [00:52&lt;00:00,  2.39it/s]"
          }
        },
        "2c9655d41a314da0a47e4b57a014fb69": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "12eaacccd001430bbe420f4845920d16": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f0b2f80f6cec484da88c820c9d2f3e72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "47196fea8a884554b6a071bd0bea94e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4cdd3bf7ef744fd2ab4d1492a6d20aa6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "54fbd3ea54d7467eacc8979a9a0e395c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3c0dbc5ee5ec47318c9a9f69f6e02e9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1SWSTdXwsn5q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "b07083c95e434000904dcba7236257a3",
            "fdf64f8840b247f5a42ff2ca6ecb381f",
            "c84f402c5bde428788516b5fedc4da98",
            "0e2e289bfc5b41f29540d3b4eb166088",
            "2c9655d41a314da0a47e4b57a014fb69",
            "12eaacccd001430bbe420f4845920d16",
            "f0b2f80f6cec484da88c820c9d2f3e72",
            "47196fea8a884554b6a071bd0bea94e9",
            "4cdd3bf7ef744fd2ab4d1492a6d20aa6",
            "54fbd3ea54d7467eacc8979a9a0e395c",
            "3c0dbc5ee5ec47318c9a9f69f6e02e9e"
          ]
        },
        "outputId": "90cb371f-1f8c-4f6e-d6a2-da461cb2eaf7"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b07083c95e434000904dcba7236257a3"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @markdown # Step 1: Install all neccessary packages\n",
        "#Download pymol in colabsystem\n",
        "from IPython.utils import io\n",
        "import tqdm.notebook\n",
        "import os\n",
        "\"\"\"The PyMOL installation is done inside two nested context managers. This approach\n",
        "was inspired by Dr. Christopher Schlicksup's (of the Phenix group at\n",
        "Lawrence Berkeley National Laboratory) method for installing cctbx\n",
        "in a Colab Notebook. He presented his work on September 1, 2021 at the IUCr\n",
        "Crystallographic Computing School. I adapted Chris's approach here. It replaces my first approach\n",
        "that requires seven steps. My approach was presentated at the SciPy2021 conference\n",
        "in July 2021 and published in the\n",
        "[proceedings](http://conference.scipy.org/proceedings/scipy2021/blaine_mooers.html).\n",
        "The new approach is easier for beginners to use. The old approach is easier to debug\n",
        "and could be used as a back-up approach.\n",
        "\n",
        "\"\"\"\n",
        "total = 100\n",
        "with tqdm.notebook.tqdm(total=total) as pbar:\n",
        "    with io.capture_output() as captured:\n",
        "\n",
        "        !pip install -q condacolab\n",
        "        import condacolab\n",
        "        condacolab.install()\n",
        "        pbar.update(10)\n",
        "\n",
        "        import sys\n",
        "        sys.path.append('/usr/local/lib/python3.7/site-packages/')\n",
        "        pbar.update(20)\n",
        "\n",
        "        # Install PyMOL\n",
        "        %shell mamba install -c schrodinger pymol-bundle --yes\n",
        "\n",
        "        pbar.update(90)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_H953hnpAYN",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @markdown # Step 2: Run prescreening (Geometric parameters applied)\n",
        "# pip install scipy\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from Bio.PDB import PDBParser\n",
        "import itertools\n",
        "import requests\n",
        "from concurrent.futures import ProcessPoolExecutor\n",
        "import traceback\n",
        "from scipy.spatial import KDTree # <-- Import KDTree\n",
        "\n",
        "# --- Constants ---\n",
        "# Input directory containing PDB files to be processed\n",
        "Target_pdb_directory = \"/content/drive/MyDrive/All/Mn_2His_1Glu\" # @param {type:\"string\"}\n",
        "# Output directory where processed Excel files will be saved\n",
        "Prescreening result_directory = \"/content/drive/MyDrive/250413_Final/Mn_2His_1Glu_1\" # @param {type:\"string\"}\n",
        "# Create the output directory if it doesn't exist\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "# --- Download threshold configuration ---\n",
        "# (This part remains the same as before)\n",
        "base_url = \"https://raw.githubusercontent.com/SNU-Songlab/Metal-Installer-code/main/Threshold\"\n",
        "Metal = 'Cu'  # @param [\"Zn\", \"Mn\", \"Cu\", \"Fe\"]\n",
        "Combinations = '3His'  # @param [\"3His\", \"2His_1Asp\", \"2His_1Glu\", \"2His_1Cys\"]\n",
        "Range = '3'  # @param [\"1\", \"2\", \"3\", \"4\",\"5\"]\n",
        "thresholds_url = f\"{base_url}/{Metal}/{Combinations}/{Range}.xlsx\"\n",
        "thresholds_file = os.path.join(output_folder, \"thresholds.xlsx\")\n",
        "\n",
        "print(f\"⬇️ Downloading thresholds from: {thresholds_url}\")\n",
        "response = requests.get(thresholds_url)\n",
        "if response.status_code == 200:\n",
        "    with open(thresholds_file, \"wb\") as file:\n",
        "        file.write(response.content)\n",
        "    print(f\"✅ Thresholds downloaded successfully to {thresholds_file}\")\n",
        "else:\n",
        "    raise ValueError(f\"❌ Failed to download thresholds from {thresholds_url}. Status code: {response.status_code}\")\n",
        "\n",
        "# --- Load thresholds ---\n",
        "# (This part remains the same as before)\n",
        "print(\"⚙️ Loading thresholds...\")\n",
        "thresholds_df = pd.read_excel(thresholds_file, sheet_name=\"Sheet1\")\n",
        "thresholds = {\n",
        "    row[\"Parameter\"]: (row[\"Min\"], row[\"Max\"])\n",
        "    for _, row in thresholds_df.iterrows()\n",
        "    if pd.notna(row[\"Min\"]) and pd.notna(row[\"Max\"])\n",
        "}\n",
        "alpha_distance_range = thresholds[\"alpha_distance_range\"]\n",
        "beta_distance_range = thresholds[\"beta_distance_range\"]\n",
        "ratio_threshold_range = thresholds[\"ratio_threshold_range\"]\n",
        "pie_threshold_range = thresholds[\"pie_threshold_range\"]\n",
        "\n",
        "print(\"📊 Thresholds loaded:\")\n",
        "for key, value in thresholds.items():\n",
        "    print(f\"  - {key}: Min={value[0]}, Max={value[1]}\")\n",
        "\n",
        "# --- Helper Functions ---\n",
        "# (These functions remain the same: calculate_pie, standardize_residue_identity)\n",
        "def calculate_pie(v1, v2):\n",
        "    \"\"\"Calculates the angle (in degrees) between two vectors.\"\"\"\n",
        "    dot = np.dot(v1, v2)\n",
        "    norm = np.linalg.norm(v1) * np.linalg.norm(v2)\n",
        "    if norm == 0: return np.nan\n",
        "    angle_rad = np.arccos(np.clip(dot / norm, -1.0, 1.0))\n",
        "    return np.degrees(angle_rad)\n",
        "\n",
        "def standardize_residue_identity(row):\n",
        "    \"\"\"Creates a standardized, sorted tuple representing the triad's residues (by name and number).\"\"\"\n",
        "    res_names = [row[f\"Coord_residue_name{i+1}\"] for i in range(3)]\n",
        "    res_numbers = [row[f\"Coord_residue_number{i+1}\"] for i in range(3)]\n",
        "    items = list(zip(res_names, res_numbers))\n",
        "    items.sort()\n",
        "    return tuple(items)\n",
        "\n",
        "# --- Main Processing Function ---\n",
        "\n",
        "def process_pdb_file(pdb_file):\n",
        "    \"\"\"\n",
        "    Processes a single PDB file to find inter/intra triads using KDTree pre-filtering.\n",
        "    \"\"\"\n",
        "    pdb_name = os.path.basename(pdb_file)\n",
        "    # Adjusted output file naming if needed, or remove if using combined output\n",
        "    output_file = os.path.join(output_folder, f\"{os.path.splitext(pdb_name)[0]}_processed.xlsx\")\n",
        "    print(f\"🔄 Processing: {pdb_name}\")\n",
        "    try:\n",
        "        parser = PDBParser(QUIET=True)\n",
        "        structure = parser.get_structure(\"protein\", pdb_file)\n",
        "        model = structure[0]\n",
        "\n",
        "        target_residues = {\"HIS\", \"ASP\", \"GLU\", \"CYS\"}\n",
        "        print(f\"  Target residue types for filtering (at least 2 required): {target_residues}\")\n",
        "\n",
        "        all_residues_full = [res for chain in model for res in chain if res.get_id()[0] == \" \"]\n",
        "        print(f\"  Found {len(all_residues_full)} standard residues initially.\")\n",
        "\n",
        "        # --- KDTree Pre-filtering Implementation ---\n",
        "\n",
        "        # 1. Prepare data for KDTree (using CA atoms) and map indices back to residues\n",
        "        # Ensure residues have CA atoms needed for the tree\n",
        "        residues_for_tree = [res for res in all_residues_full if res.has_id(\"CA\")]\n",
        "        if len(residues_for_tree) < 3:\n",
        "            print(f\"  ⚠️ Skipping {pdb_name}: Not enough residues (<3) with CA atoms.\")\n",
        "            # Return empty DataFrame if using combined output, or just skip file writing\n",
        "            return pd.DataFrame() # Important for combined output scenario\n",
        "\n",
        "        coords_ca = np.array([res[\"CA\"].coord for res in residues_for_tree])\n",
        "        # residue_map allows getting the Residue object back from its index in coords_ca\n",
        "        residue_map = residues_for_tree\n",
        "\n",
        "        # 2. Build KDTree\n",
        "        kdtree = KDTree(coords_ca)\n",
        "\n",
        "        # 3. Define max distance for neighbor search\n",
        "        # Use a value slightly larger than the max required by subsequent filters\n",
        "        # Here, using the max alpha distance + 10% buffer\n",
        "        max_dist = alpha_distance_range[1] * 1.1\n",
        "        print(f\"  KDTree Max search distance (CA-CA): {max_dist:.2f} Å\")\n",
        "\n",
        "        # 4. Find pairs within max_dist\n",
        "        # query_pairs finds all pairs (i, j) where i < j and distance(i, j) <= max_dist\n",
        "        pairs = kdtree.query_pairs(r=max_dist)\n",
        "        print(f\"  Found {len(pairs)} pairs within distance using KDTree.\")\n",
        "\n",
        "        # 5. Find potential third neighbors (k) for each pair (i, j)\n",
        "        potential_triad_indices = set() # Use a set to store unique sorted index tuples (i, j, k)\n",
        "        for i, j in pairs:\n",
        "            # Find neighbors of point i\n",
        "            indices_k_near_i = kdtree.query_ball_point(coords_ca[i], r=max_dist)\n",
        "            # Find neighbors of point j\n",
        "            indices_k_near_j = kdtree.query_ball_point(coords_ca[j], r=max_dist)\n",
        "            # Find common neighbors (potential 'k' candidates)\n",
        "            common_neighbors = set(indices_k_near_i).intersection(indices_k_near_j)\n",
        "\n",
        "            for k in common_neighbors:\n",
        "                # Ensure k is distinct from i and j\n",
        "                if k != i and k != j:\n",
        "                    # Add the sorted tuple of indices to the set to ensure uniqueness\n",
        "                    triad_indices = tuple(sorted((i, j, k)))\n",
        "                    potential_triad_indices.add(triad_indices)\n",
        "\n",
        "        print(f\"  Generated {len(potential_triad_indices)} unique potential spatial triads.\")\n",
        "\n",
        "        # 6. Map indices back to residue objects and apply the \">= 2 target residues\" filter\n",
        "        triads_to_process = []\n",
        "        for idx_i, idx_j, idx_k in potential_triad_indices:\n",
        "            # Retrieve the actual Bio.PDB Residue objects\n",
        "            comb = (residue_map[idx_i], residue_map[idx_j], residue_map[idx_k])\n",
        "            # Apply the target residue count filter\n",
        "            if sum(res.get_resname() in target_residues for res in comb) >= 2:\n",
        "                triads_to_process.append(comb)\n",
        "\n",
        "        print(f\"  Identified {len(triads_to_process)} triads meeting spatial and target residue criteria.\")\n",
        "        # --- End of KDTree Implementation ---\n",
        "\n",
        "\n",
        "        # --- Start Detailed Geometric Filtering (on the reduced 'triads_to_process' list) ---\n",
        "        def get_triad_type(comb):\n",
        "            chains = [res.get_full_id()[2] for res in comb]\n",
        "            return \"intra\" if len(set(chains)) == 1 else \"inter\"\n",
        "\n",
        "        results = []\n",
        "        # Now iterate through the much smaller, pre-filtered list\n",
        "        for comb in triads_to_process:\n",
        "            try:\n",
        "                # Check for CA and CB atoms (important, as KDTree only used CA)\n",
        "                if not all(res.has_id(\"CA\") and res.has_id(\"CB\") for res in comb):\n",
        "                    continue # Skip if any residue misses CA or CB\n",
        "\n",
        "                # Calculate pairwise distances (CA-CA and CB-CB)\n",
        "                alpha_distances, beta_distances = [], []\n",
        "                valid_distances = True\n",
        "                for res1, res2 in itertools.combinations(comb, 2):\n",
        "                    d_ca = np.linalg.norm(res1[\"CA\"].coord - res2[\"CA\"].coord)\n",
        "                    d_cb = np.linalg.norm(res1[\"CB\"].coord - res2[\"CB\"].coord)\n",
        "\n",
        "                    # Check against the *precise* thresholds loaded earlier\n",
        "                    if not (alpha_distance_range[0] <= d_ca <= alpha_distance_range[1] and \\\n",
        "                            beta_distance_range[0] <= d_cb <= beta_distance_range[1]):\n",
        "                        valid_distances = False\n",
        "                        break\n",
        "                    alpha_distances.append(d_ca)\n",
        "                    beta_distances.append(d_cb)\n",
        "\n",
        "                if valid_distances and len(alpha_distances) == 3: # Ensure all 3 pairs passed\n",
        "                    row = {\n",
        "                        \"PDB_ID\": pdb_name,\n",
        "                        \"Triad_Type\": get_triad_type(comb),\n",
        "                    }\n",
        "                    for i, res in enumerate(comb):\n",
        "                        full_id = res.get_full_id()\n",
        "                        row[f\"Coord_chain_id_number{i+1}\"] = full_id[2]\n",
        "                        row[f\"Coord_residue_number{i+1}\"] = full_id[3][1]\n",
        "                        row[f\"Coord_residue_name{i+1}\"] = res.get_resname()\n",
        "                    for i in range(3):\n",
        "                        row[f\"Alpha Distance {i+1}\"] = alpha_distances[i]\n",
        "                        row[f\"Beta Distance {i+1}\"] = beta_distances[i]\n",
        "                    results.append(row)\n",
        "            except KeyError as ke:\n",
        "                # print(f\"    Skipping triad due to missing atom: {ke} in {comb}\")\n",
        "                continue\n",
        "            except Exception as e_inner:\n",
        "                print(f\"    Error processing triad {comb}: {e_inner}\")\n",
        "                continue\n",
        "\n",
        "        print(f\"  Found {len(results)} triads passing initial distance filters.\")\n",
        "        df = pd.DataFrame(results)\n",
        "\n",
        "        # --- Apply Ratio Filter ---\n",
        "        # (This part remains the same)\n",
        "        if not df.empty:\n",
        "            def pass_ratio(row):\n",
        "                try:\n",
        "                    for i in range(3):\n",
        "                        if row[f\"Beta Distance {i+1}\"] == 0: return False\n",
        "                        ratio = row[f\"Alpha Distance {i+1}\"] / row[f\"Beta Distance {i+1}\"]\n",
        "                        if not (ratio_threshold_range[0] <= ratio <= ratio_threshold_range[1]):\n",
        "                            return False\n",
        "                    return True\n",
        "                except: return False\n",
        "            df_ratio = df[df.apply(pass_ratio, axis=1)].copy()\n",
        "            print(f\"  Found {len(df_ratio)} triads passing ratio filter.\")\n",
        "        else: df_ratio = pd.DataFrame()\n",
        "\n",
        "        # --- Apply Pie Angle Filter ---\n",
        "        # (This part remains the same, but needs all_residues_full for lookup)\n",
        "        if not df_ratio.empty:\n",
        "            res_lookup = {\n",
        "                (res.get_full_id()[2], res.get_full_id()[3][1]): res\n",
        "                for res in all_residues_full # Use the original full list for lookup\n",
        "            }\n",
        "            def compute_pie(row):\n",
        "                try:\n",
        "                    comb_ids = [(row[f\"Coord_chain_id_number{i+1}\"], row[f\"Coord_residue_number{i+1}\"]) for i in range(3)]\n",
        "                    res_objs = [res_lookup[res_id] for res_id in comb_ids]\n",
        "                    angles = []\n",
        "                    for i, j in [(0,1), (0,2), (1,2)]:\n",
        "                        # Ensure atoms exist before accessing coords (add extra check)\n",
        "                        if not (res_objs[i].has_id(\"CA\") and res_objs[i].has_id(\"CB\") and \\\n",
        "                                res_objs[j].has_id(\"CA\") and res_objs[j].has_id(\"CB\")):\n",
        "                                return pd.Series([np.nan, np.nan, np.nan], index=[\"Pie_1_2\", \"Pie_1_3\", \"Pie_2_3\"])\n",
        "                        v_ca = res_objs[j][\"CA\"].coord - res_objs[i][\"CA\"].coord\n",
        "                        v_cb = res_objs[j][\"CB\"].coord - res_objs[i][\"CB\"].coord\n",
        "                        angles.append(calculate_pie(v_ca, v_cb))\n",
        "                    return pd.Series(angles, index=[\"Pie_1_2\", \"Pie_1_3\", \"Pie_2_3\"])\n",
        "                except Exception as e_pie:\n",
        "                    # print(f\"    Error calculating pie angle for row: {e_pie}\")\n",
        "                    return pd.Series([np.nan, np.nan, np.nan], index=[\"Pie_1_2\", \"Pie_1_3\", \"Pie_2_3\"])\n",
        "\n",
        "            pie_results = df_ratio.apply(compute_pie, axis=1)\n",
        "            df_ratio[[\"Pie_1_2\", \"Pie_1_3\", \"Pie_2_3\"]] = pie_results\n",
        "            for col in [\"Pie_1_2\", \"Pie_1_3\", \"Pie_2_3\"]:\n",
        "                df_ratio[f\"{col}_Filter\"] = df_ratio[col].apply(\n",
        "                    lambda x: pie_threshold_range[0] < x < pie_threshold_range[1] if pd.notnull(x) else False)\n",
        "            df_ratio['Pie_Filter'] = df_ratio[[f'{col}_Filter' for col in ['Pie_1_2', 'Pie_1_3', 'Pie_2_3']]].all(axis=1)\n",
        "            df_final = df_ratio[df_ratio['Pie_Filter']].copy()\n",
        "            print(f\"  Found {len(df_final)} triads passing pie angle filter.\")\n",
        "        else: df_final = pd.DataFrame()\n",
        "\n",
        "        # --- Redundancy Removal ---\n",
        "        # (This part remains the same)\n",
        "        if not df_final.empty:\n",
        "            print(\"  Applying redundancy removal based on residue identity (name and number)...\")\n",
        "            df_final[\"Triad_Identity\"] = df_final.apply(standardize_residue_identity, axis=1)\n",
        "            df_deduplicated = df_final.drop_duplicates(subset=\"Triad_Identity\").drop(columns=[\"Triad_Identity\"])\n",
        "            print(f\"  Kept {len(df_deduplicated)} unique triads after deduplication.\")\n",
        "        else: df_deduplicated = pd.DataFrame()\n",
        "\n",
        "        # --- Output ---\n",
        "        # Decide whether to write individual files or return for combined output later\n",
        "        # Option 1: Write individual Excel files (as before)\n",
        "        with pd.ExcelWriter(output_file) as writer:\n",
        "             # Optionally write intermediate steps for debugging\n",
        "             # df.to_excel(writer, sheet_name=\"1_Initial_Distances_KD\", index=False)\n",
        "             # df_ratio.to_excel(writer, sheet_name=\"2_Ratio_Filtered_KD\", index=False)\n",
        "             # df_final.to_excel(writer, sheet_name=\"3_Pie_Filtered_KD\", index=False) # Before deduplication\n",
        "             df_deduplicated.to_excel(writer, sheet_name=\"4_Final_Deduplicated_KD\", index=False) # Final unique per PDB\n",
        "        print(f\"✅ Finished: {pdb_name}. Results saved to {output_file} (Unique triads: {len(df_deduplicated)})\")\n",
        "        return None # Return None if writing file here\n",
        "\n",
        "        # Option 2: Return DataFrame for combined output (Requires changes in __main__)\n",
        "        # print(f\"✅ Finished processing: {pdb_name} (Unique triads: {len(df_deduplicated)})\")\n",
        "        # return df_deduplicated\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"❌ Error: Input PDB file not found at {pdb_file}\")\n",
        "        return None # Or return empty DataFrame for combined output\n",
        "    except Exception as e:\n",
        "        print(f\"❌ An unexpected error occurred while processing {pdb_name}: {e}\")\n",
        "        traceback.print_exc()\n",
        "        return None # Or return empty DataFrame for combined output\n",
        "\n",
        "\n",
        "# --- Run Processing for All PDB Files ---\n",
        "# (This part remains largely the same, but adjust based on whether process_pdb_file returns data)\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"\\n--- Starting Batch Processing with KDTree Pre-filtering ---\")\n",
        "    pdb_files = glob.glob(os.path.join(input_folder, \"*.pdb\"))\n",
        "    print(f\"Found {len(pdb_files)} PDB files in {input_folder}\")\n",
        "\n",
        "    if not pdb_files:\n",
        "        print(\"⚠️ No PDB files found. Exiting.\")\n",
        "    else:\n",
        "        # Determine number of workers\n",
        "        num_workers = min(6, os.cpu_count() or 1) # Adjust '6' as needed\n",
        "        print(f\"🚀 Starting parallel processing with up to {num_workers} workers...\")\n",
        "\n",
        "        # --- Choose based on process_pdb_file return ---\n",
        "        # If process_pdb_file writes its own files (returns None):\n",
        "        with ProcessPoolExecutor(max_workers=num_workers) as executor:\n",
        "             executor.map(process_pdb_file, pdb_files)\n",
        "\n",
        "        # If process_pdb_file returns DataFrames for combined output:\n",
        "        # all_results_list = []\n",
        "        # with ProcessPoolExecutor(max_workers=num_workers) as executor:\n",
        "        #     results_iterator = executor.map(process_pdb_file, pdb_files)\n",
        "        #     all_results_list = [res for res in results_iterator if res is not None and not res.empty]\n",
        "        # print(\"\\n--- Combining Results ---\")\n",
        "        # if all_results_list:\n",
        "        #     final_df = pd.concat(all_results_list, ignore_index=True)\n",
        "        #     # Add optional global deduplication here if needed (see previous example)\n",
        "        #     # ... (global deduplication code) ...\n",
        "        #     output_path_csv = os.path.join(output_folder, \"ALL_RESULTS_final_deduplicated_KDTree.csv\")\n",
        "        #     final_df.to_csv(output_path_csv, index=False) # Save combined results\n",
        "        #     print(f\"✅ Combined results saved to {output_path_csv}\")\n",
        "        # else:\n",
        "        #     print(\"⚠️ No valid triads found in any PDB file.\")\n",
        "        # -------------------------------------------------\n",
        "\n",
        "    print(\"\\n🎉 All processing finished.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown # Step 3: Extraction of coordinates of the prescreened target\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from Bio.PDB import PDBParser\n",
        "\n",
        "# 🔧 Folder paths (adjust as needed)\n",
        "prescreening_directory = \"/content/drive/MyDrive/250413_Final/Zn_2His_1Glu_3\"        # @param {type:\"string\"}\n",
        "pdb_directory  = \"/content/drive/MyDrive/All/Zn_2His_1Glu\"           # @param {type:\"string\"}\n",
        "coordinate_directory = \"/content/drive/MyDrive/250413_Final/Zn_2His_1Glu_3_coordinate\"  # @param {type:\"string\"}\n",
        "\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "# 🧠 Coordinate extraction helper\n",
        "def extract_coordinates(chain, res_id, atom_name):\n",
        "    try:\n",
        "        residue = chain[res_id]\n",
        "        return residue[atom_name].coord\n",
        "    except Exception:\n",
        "        return [None, None, None]\n",
        "\n",
        "# 🔁 Loop through all Excel files\n",
        "for file in os.listdir(excel_folder):\n",
        "    if file.endswith(\"_processed.xlsx\"):\n",
        "        pdb_id = file.replace(\"_processed.xlsx\", \"\")\n",
        "        excel_path = os.path.join(excel_folder, file)\n",
        "        pdb_path = os.path.join(pdb_folder, f\"{pdb_id}.pdb\")\n",
        "        output_path = os.path.join(output_folder, f\"{pdb_id}_with_coordinates.xlsx\")\n",
        "\n",
        "        if not os.path.isfile(pdb_path):\n",
        "            print(f\"❌ Skipping {pdb_id}: PDB file not found.\")\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            # Load Excel\n",
        "            df_pie = pd.read_excel(excel_path, sheet_name=\"4_Final_Deduplicated_KD\")\n",
        "\n",
        "            # Load PDB\n",
        "            parser = PDBParser(QUIET=True)\n",
        "            structure = parser.get_structure(\"protein\", pdb_path)\n",
        "            chains = {chain.id: chain for chain in structure[0]}\n",
        "\n",
        "            ca_coords, cb_coords = [], []\n",
        "\n",
        "            for idx, row in df_pie.iterrows():\n",
        "                chain1 = chains.get(row['Coord_chain_id_number1'])\n",
        "                chain2 = chains.get(row['Coord_chain_id_number2'])\n",
        "                chain3 = chains.get(row['Coord_chain_id_number3'])\n",
        "\n",
        "                # Cα\n",
        "                ca1 = extract_coordinates(chain1, row['Coord_residue_number1'], 'CA')\n",
        "                ca2 = extract_coordinates(chain2, row['Coord_residue_number2'], 'CA')\n",
        "                ca3 = extract_coordinates(chain3, row['Coord_residue_number3'], 'CA')\n",
        "                ca_coords.append([*ca1, *ca2, *ca3])\n",
        "\n",
        "                # Cβ\n",
        "                cb1 = extract_coordinates(chain1, row['Coord_residue_number1'], 'CB')\n",
        "                cb2 = extract_coordinates(chain2, row['Coord_residue_number2'], 'CB')\n",
        "                cb3 = extract_coordinates(chain3, row['Coord_residue_number3'], 'CB')\n",
        "                cb_coords.append([*cb1, *cb2, *cb3])\n",
        "\n",
        "            # Convert to DataFrame\n",
        "            ca_cols = ['CA1_X', 'CA1_Y', 'CA1_Z', 'CA2_X', 'CA2_Y', 'CA2_Z', 'CA3_X', 'CA3_Y', 'CA3_Z']\n",
        "            cb_cols = ['CB1_X', 'CB1_Y', 'CB1_Z', 'CB2_X', 'CB2_Y', 'CB2_Z', 'CB3_X', 'CB3_Y', 'CB3_Z']\n",
        "            df_ca = pd.DataFrame(ca_coords, columns=ca_cols)\n",
        "            df_cb = pd.DataFrame(cb_coords, columns=cb_cols)\n",
        "\n",
        "            # Combine and save\n",
        "            df_pie = pd.concat([df_pie.reset_index(drop=True), df_ca, df_cb], axis=1)\n",
        "            if 'PDB_ID' in df_pie.columns:\n",
        "                df_pie['PDB_ID'] = df_pie['PDB_ID'].str.replace('.pdb', '', regex=False)\n",
        "\n",
        "            df_pie.to_excel(output_path, index=False)\n",
        "            print(f\"✅ Saved: {output_path}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error processing {pdb_id}: {e}\")\n"
      ],
      "metadata": {
        "id": "OmyrAVvhfbQl",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "3FRNkvrAmVqw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 899
        },
        "cellView": "form",
        "outputId": "35e31e4a-fe63-47cf-dbad-70c02488b3bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading probability map from: https://raw.githubusercontent.com/SNU-Songlab/Metal-Installer-code/main/probability//Zn/3His/map.xlsx\n",
            "Downloaded map data to /content/map.xlsx\n",
            "Downloading thresholds from: https://raw.githubusercontent.com/SNU-Songlab/Metal-Installer-code/main/probability//Zn/3His/threshold.xlsx\n",
            "Downloaded thresholds data to /content/threshold.xlsx\n",
            "Loading thresholds...\n",
            "Loading probability map...\n",
            "Processing thresholds...\n",
            "Processing probability map...\n",
            "Probability map processed into 3D array.\n",
            "\n",
            "--- Starting Main Process ---\n",
            "\n",
            "Searching for input Excel files in: /content/drive/MyDrive/250413_Final/Zn_3His_1_coordinate/\n",
            "Found 117 potential input files.\n",
            "\n",
            "Initializing multiprocessing pool with 8 workers...\n",
            "[PID 6353] Processing PDB ID: 1fr2_chainA[PID 6354] Processing PDB ID: 1a85_chainA[PID 6355] Processing PDB ID: 1bkc_chainA[PID 6356] Processing PDB ID: 1atl_chainA[PID 6357] Processing PDB ID: 1bmc_chainA\n",
            "\n",
            "\n",
            "[PID 6359] Processing PDB ID: 2esl_chainA\n",
            "[PID 6358] Processing PDB ID: 1txl_chainA\n",
            "\n",
            "\n",
            "[PID 6360] Processing PDB ID: 1af0_chainA\n",
            "Starting parallel processing...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/usr/lib/python3.11/multiprocessing/pool.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m                 \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_items\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopleft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    857\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: pop from an empty deque",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-70bd2c845251>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    473\u001b[0m         \u001b[0mresults_iterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimap_unordered\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_pdb_entry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0mprocessed_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m             \u001b[0mprocessed_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mpdb_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msite_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/multiprocessing/pool.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    859\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 861\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    862\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    863\u001b[0m                     \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_items\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopleft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# @markdown # Step 4: Probability density map\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import glob\n",
        "import requests\n",
        "import traceback # Import traceback for better error printing\n",
        "import multiprocessing # Import multiprocessing\n",
        "from Bio.PDB import PDBParser # PDBParser is still needed for structure loading\n",
        "from scipy.spatial import KDTree # Import KDTree\n",
        "\n",
        "# --- Configuration and Setup ---\n",
        "# Define base directories (MODIFY THESE PATHS IF NEEDED)\n",
        "# Using the paths from your last provided code\n",
        "excel_directory = \"/content/drive/MyDrive/250413_Final/Zn_3His_1_coordinate\" # @param {type:\"string\"}\n",
        "pdb_directory_base = \"/content/drive/MyDrive/All/Zn_3His\"                    # @param {type:\"string\"}\n",
        "output_result_directory = \"/content/drive/MyDrive/Benchmark_Final/Zn_3His_1\" # @param {type:\"string\"}\n",
        "\n",
        "# Check if base directories exist\n",
        "if not os.path.exists(excel_directory):\n",
        "    raise FileNotFoundError(f\"Excel directory not found: {excel_directory}\")\n",
        "if not os.path.exists(pdb_directory_base):\n",
        "    raise FileNotFoundError(f\"PDB directory not found: {pdb_directory_base}\")\n",
        "os.makedirs(output_result_directory, exist_ok=True)\n",
        "\n",
        "# Define file paths for downloaded data\n",
        "prob_map_file = '/content/map.xlsx'\n",
        "thresholds_file = '/content/threshold.xlsx'\n",
        "\n",
        "# --- Download Data from GitHub ---\n",
        "# Using the parameters from your last provided code\n",
        "base_url = \"https://raw.githubusercontent.com/SNU-Songlab/Metal-Installer-code/main/probability/\"\n",
        "Metal = 'Zn'  # @param [\"Zn\", \"Mn\", \"Cu\", \"Fe\"]\n",
        "Combinations = '3His'  # @param [\"3His\", \"2His_1Asp\", \"2His_1Glu\", \"2His_1Cys\"]\n",
        "map_url = f\"{base_url}/{Metal}/{Combinations}/map.xlsx\"\n",
        "thresholds_url = f\"{base_url}/{Metal}/{Combinations}/threshold.xlsx\"\n",
        "\n",
        "print(f\"Downloading probability map from: {map_url}\")\n",
        "response_map = requests.get(map_url)\n",
        "if response_map.status_code == 200:\n",
        "    with open(prob_map_file, 'wb') as file:\n",
        "        file.write(response_map.content)\n",
        "    print(f\"Downloaded map data to {prob_map_file}\")\n",
        "else:\n",
        "    raise ValueError(f\"Failed to download map file from {map_url}. Status code: {response_map.status_code}\")\n",
        "\n",
        "print(f\"Downloading thresholds from: {thresholds_url}\")\n",
        "response_thresh = requests.get(thresholds_url)\n",
        "if response_thresh.status_code == 200:\n",
        "    with open(thresholds_file, 'wb') as file:\n",
        "        file.write(response_thresh.content)\n",
        "    print(f\"Downloaded thresholds data to {thresholds_file}\")\n",
        "else:\n",
        "    raise ValueError(f\"Failed to download thresholds file from {thresholds_url}. Status code: {response_thresh.status_code}\")\n",
        "\n",
        "# --- Load and Process Data --- (Load data ONCE in the main process)\n",
        "print(\"Loading thresholds...\")\n",
        "thresholds_df = pd.read_excel(thresholds_file, sheet_name='Sheet1')\n",
        "print(\"Loading probability map...\")\n",
        "df_precomputed_prob_map = pd.read_excel(prob_map_file)\n",
        "\n",
        "print(\"Processing thresholds...\")\n",
        "thresholds = {}\n",
        "for _, row in thresholds_df.iterrows():\n",
        "    parameter = row['Parameter']\n",
        "    min_value = row['Min']\n",
        "    max_value = row['Max']\n",
        "    if pd.notna(min_value) and pd.notna(max_value):\n",
        "        thresholds[parameter] = (min_value, max_value)\n",
        "\n",
        "required_keys = ['ca_distances_calc', 'cb_distances_calc', 'ratio', 'angle']\n",
        "for key in required_keys:\n",
        "    if key not in thresholds:\n",
        "        raise KeyError(f\"Missing key '{key}' in thresholds file.\")\n",
        "\n",
        "print(\"Processing probability map...\")\n",
        "ca_bins = np.sort(df_precomputed_prob_map['Calpha_Zn_Dist'].unique())\n",
        "cb_bins = np.sort(df_precomputed_prob_map['Cbeta_Zn_Dist'].unique())\n",
        "angle_bins = np.sort(df_precomputed_prob_map['CA-Zn-CB_Angle'].unique())\n",
        "pivoted_prob_map = df_precomputed_prob_map.pivot_table(\n",
        "    index='Calpha_Zn_Dist', columns=['Cbeta_Zn_Dist', 'CA-Zn-CB_Angle'], values='Probability', fill_value=0\n",
        ")\n",
        "expected_shape = (len(ca_bins), len(cb_bins) * len(angle_bins))\n",
        "if pivoted_prob_map.shape == expected_shape:\n",
        "     prob_map_3d = pivoted_prob_map.values.reshape((len(ca_bins), len(cb_bins), len(angle_bins)))\n",
        "     print(\"Probability map processed into 3D array.\")\n",
        "else:\n",
        "     raise ValueError(f\"Pivoted map shape {pivoted_prob_map.shape} doesn't match expected shape {expected_shape} for reshaping.\")\n",
        "\n",
        "\n",
        "# --- Helper Function Definitions ---\n",
        "\n",
        "def calculate_ratio(current_point, ca_xyz, cb_xyz):\n",
        "    # ... (no changes needed) ...\n",
        "    ca_distances = np.linalg.norm(ca_xyz - current_point, axis=1)\n",
        "    cb_distances = np.linalg.norm(cb_xyz - current_point, axis=1)\n",
        "    ratios = np.divide(ca_distances, cb_distances, out=np.full_like(ca_distances, np.inf), where=cb_distances!=0)\n",
        "    return ratios\n",
        "\n",
        "def load_pdb_structure(entry_id, pdb_directory):\n",
        "    # ... (no changes needed) ...\n",
        "    pdb_parser = PDBParser(QUIET=True)\n",
        "    pdb_file_path = os.path.join(pdb_directory, f\"{entry_id}.pdb\")\n",
        "    try:\n",
        "        structure = pdb_parser.get_structure(entry_id, pdb_file_path)\n",
        "        return structure\n",
        "    except FileNotFoundError:\n",
        "        print(f\"❌ PDB file not found for loading: {pdb_file_path}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error loading PDB file {pdb_file_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "def score_zn_predictions(ca_distances, cb_distances, angles, prob_map_3d, ca_bins, cb_bins, angle_bins):\n",
        "    # ... (no changes needed) ...\n",
        "    ca_bin_indices = np.clip(np.digitize(ca_distances, ca_bins[1:], right=True), 0, len(ca_bins)-1)\n",
        "    cb_bin_indices = np.clip(np.digitize(cb_distances, cb_bins[1:], right=True), 0, len(cb_bins)-1)\n",
        "    angle_bin_indices = np.clip(np.digitize(angles, angle_bins[1:], right=True), 0, len(angle_bins)-1)\n",
        "    probabilities = []\n",
        "    valid = True\n",
        "    for cbin, bbin, abin in zip(ca_bin_indices, cb_bin_indices, angle_bin_indices):\n",
        "        if 0 <= cbin < prob_map_3d.shape[0] and 0 <= bbin < prob_map_3d.shape[1] and 0 <= abin < prob_map_3d.shape[2]:\n",
        "            prob_value = prob_map_3d[cbin, bbin, abin]\n",
        "            if prob_value <= 0:\n",
        "                valid = False\n",
        "                break\n",
        "            probabilities.append(prob_value)\n",
        "        else:\n",
        "            # print(f\"⚠️ Warning: Invalid bin indices generated: CA({cbin}), CB({bbin}), Angle({abin})\")\n",
        "            valid = False\n",
        "            break\n",
        "    final_score = np.prod(probabilities) if valid and probabilities else 0.0\n",
        "    return final_score\n",
        "\n",
        "def calculate_angles(zn_coords, ca_coords_triplet, cb_coords_triplet):\n",
        "    # ... (no changes needed) ...\n",
        "    angles = []\n",
        "    for i in range(3):\n",
        "        v_ca = ca_coords_triplet[i] - zn_coords\n",
        "        v_cb = cb_coords_triplet[i] - zn_coords\n",
        "        norm_v_ca = np.linalg.norm(v_ca)\n",
        "        norm_v_cb = np.linalg.norm(v_cb)\n",
        "        if norm_v_ca == 0 or norm_v_cb == 0:\n",
        "            angles.append(0.0)\n",
        "            continue\n",
        "        cos_theta = np.dot(v_ca, v_cb) / (norm_v_ca * norm_v_cb)\n",
        "        angle_rad = np.arccos(np.clip(cos_theta, -1.0, 1.0))\n",
        "        angles.append(np.degrees(angle_rad))\n",
        "    return angles\n",
        "\n",
        "def define_excluded_triads(triad_res_nums, structure):\n",
        "    # ... (no changes needed) ...\n",
        "    excluded_residues = set()\n",
        "    if structure is None: return excluded_residues\n",
        "    # Ensure triad_res_nums are integers for comparison\n",
        "    try:\n",
        "        res_nums_to_find = set(int(num) for num in triad_res_nums)\n",
        "    except (ValueError, TypeError):\n",
        "         print(f\"⚠️ Warning: Could not convert all triad residue numbers {triad_res_nums} to integers.\")\n",
        "         return excluded_residues # Return empty set if conversion fails\n",
        "\n",
        "    for model in structure:\n",
        "        for chain in model:\n",
        "            for residue in chain:\n",
        "                res_seq_num = residue.id[1]\n",
        "                if res_seq_num in res_nums_to_find:\n",
        "                    excluded_residues.add((chain.id, res_seq_num))\n",
        "    return excluded_residues\n",
        "\n",
        "# --- NEW Proximity Filter using KDTree ---\n",
        "def proximity_filter_kdtree(kdtree, zn_candidate, exclusion_radius=2.5):\n",
        "    \"\"\"\n",
        "    Checks proximity using a pre-built SciPy KDTree.\n",
        "    Returns True if valid (no atoms too close), False otherwise.\n",
        "    \"\"\"\n",
        "    if kdtree is None:\n",
        "        # If no tree was built (e.g., no non-excluded atoms), assume valid\n",
        "        return True\n",
        "\n",
        "    try:\n",
        "        # Query the KDTree to find indices of points within the radius\n",
        "        # query_ball_point is efficient for this \"are there any?\" check\n",
        "        indices_nearby = kdtree.query_ball_point(zn_candidate, r=exclusion_radius, return_length=True)\n",
        "\n",
        "        # If the length is > 0, points were found nearby\n",
        "        if indices_nearby > 0:\n",
        "            return False # Invalid: atoms are too close\n",
        "        else:\n",
        "            return True # Valid: no atoms found within the radius\n",
        "    except Exception as e:\n",
        "        # Handle potential errors during KDTree query phase\n",
        "        print(f\"❌ Error during KDTree query: {e}\")\n",
        "        return False # Treat query errors as failing the proximity check\n",
        "\n",
        "\n",
        "# --- MODIFIED Main Prediction Function (Uses KDTree) ---\n",
        "def estimate_zn_iterative(\n",
        "    ca_coords_site_flat, # Coords for ONE site - FLAT array (9,) expected\n",
        "    cb_coords_site_flat, # Coords for ONE site - FLAT array (9,) expected\n",
        "    site_info,      # DataFrame row or dict with PDB_ID and residue numbers\n",
        "    structure,      # Pass the loaded structure\n",
        "    thresholds,     # Pass the thresholds dict\n",
        "    prob_map_3d, ca_bins, cb_bins, angle_bins, # Pass map and bins\n",
        "    grid_resolution=0.2\n",
        "    ):\n",
        "    \"\"\"\n",
        "    Estimates Zn coordinate for a SINGLE site using KDTree for proximity,\n",
        "    returning the FIRST valid candidate.\n",
        "    \"\"\"\n",
        "    entry_id = site_info['PDB_ID']\n",
        "    site_index_name = site_info.name\n",
        "\n",
        "    # --- Coordinate Validation and Reshape --- (Includes fix from before)\n",
        "    try:\n",
        "        ca_coords_numeric = pd.to_numeric(np.asarray(ca_coords_site_flat), errors='coerce')\n",
        "        cb_coords_numeric = pd.to_numeric(np.asarray(cb_coords_site_flat), errors='coerce')\n",
        "        if np.isnan(ca_coords_numeric).any() or np.isnan(cb_coords_numeric).any():\n",
        "             return \"no metal\", 0, [None, None, None]\n",
        "        if ca_coords_numeric.shape != (9,) or cb_coords_numeric.shape != (9,):\n",
        "             return \"no metal\", 0, [None, None, None]\n",
        "        ca_xyz = ca_coords_numeric.astype(np.float64).reshape(3, 3)\n",
        "        cb_xyz = cb_coords_numeric.astype(np.float64).reshape(3, 3)\n",
        "    except (ValueError, TypeError) as e:\n",
        "        print(f\"❌ Error validating/reshaping coordinates for site index {site_index_name} in {entry_id}: {e}\")\n",
        "        return \"no metal\", 0, [None, None, None]\n",
        "\n",
        "    if structure is None: return \"no metal\", 0, [None, None, None]\n",
        "\n",
        "    # --- Define Excluded Residues ---\n",
        "    triad_res_nums = [\n",
        "        site_info['Coord_residue_number1'],\n",
        "        site_info['Coord_residue_number2'],\n",
        "        site_info['Coord_residue_number3']\n",
        "    ]\n",
        "    excluded_residues = define_excluded_triads(triad_res_nums, structure) # Get set of (chain, resnum)\n",
        "\n",
        "    # --- Build KD-Tree for Proximity Check (for this specific site's excluded residues) ---\n",
        "    non_excluded_coords_list = []\n",
        "    for atom in structure.get_atoms():\n",
        "        residue = atom.get_parent()\n",
        "        chain = residue.get_parent()\n",
        "        res_info = (chain.id, residue.id[1])\n",
        "        if res_info not in excluded_residues:\n",
        "            # Optional: Skip Hydrogens if needed\n",
        "            # if atom.element == 'H': continue\n",
        "            non_excluded_coords_list.append(atom.coord)\n",
        "\n",
        "    kdtree = None # Initialize kdtree\n",
        "    if non_excluded_coords_list:\n",
        "        try:\n",
        "             non_excluded_coords = np.array(non_excluded_coords_list, dtype=np.float64)\n",
        "             # Check if array is valid before building tree\n",
        "             if non_excluded_coords.ndim == 2 and non_excluded_coords.shape[1] == 3:\n",
        "                  kdtree = KDTree(non_excluded_coords)\n",
        "             # else: print(f\"⚠️ Warning: Invalid shape {non_excluded_coords.shape} for KDTree points in {entry_id}, site {site_index_name}\") # Less verbose\n",
        "        except Exception as kdtree_error:\n",
        "             print(f\"❌ Error building KDTree for {entry_id}, site {site_index_name}: {kdtree_error}\")\n",
        "             # kdtree remains None, proximity_filter_kdtree will handle this\n",
        "\n",
        "    # --- Define Search Space (Outer Box Intersection) ---\n",
        "    # ... (calculation remains the same) ...\n",
        "    shared_x_min, shared_x_max = -np.inf, np.inf\n",
        "    shared_y_min, shared_y_max = -np.inf, np.inf\n",
        "    shared_z_min, shared_z_max = -np.inf, np.inf\n",
        "    for j in range(3):\n",
        "        x_min_outer = min(ca_xyz[j, 0], cb_xyz[j, 0]) - thresholds['ca_distances_calc'][1]\n",
        "        x_max_outer = max(ca_xyz[j, 0], cb_xyz[j, 0]) + thresholds['ca_distances_calc'][1]\n",
        "        y_min_outer = min(ca_xyz[j, 1], cb_xyz[j, 1]) - thresholds['cb_distances_calc'][1]\n",
        "        y_max_outer = max(ca_xyz[j, 1], cb_xyz[j, 1]) + thresholds['cb_distances_calc'][1]\n",
        "        z_min_outer = min(ca_xyz[j, 2], cb_xyz[j, 2]) - thresholds['ca_distances_calc'][1]\n",
        "        z_max_outer = max(ca_xyz[j, 2], cb_xyz[j, 2]) + thresholds['ca_distances_calc'][1]\n",
        "        buffer = grid_resolution * 2\n",
        "        shared_x_min = max(shared_x_min, x_min_outer - buffer)\n",
        "        shared_x_max = min(shared_x_max, x_max_outer + buffer)\n",
        "        shared_y_min = max(shared_y_min, y_min_outer - buffer)\n",
        "        shared_y_max = min(shared_y_max, y_max_outer + buffer)\n",
        "        shared_z_min = max(shared_z_min, z_min_outer - buffer)\n",
        "        shared_z_max = min(shared_z_max, z_max_outer + buffer)\n",
        "\n",
        "    if shared_x_min >= shared_x_max or shared_y_min >= shared_y_max or shared_z_min >= shared_z_max:\n",
        "        return \"no metal\", 0, [None, None, None]\n",
        "\n",
        "    # --- Refined Grid Search (Find First Valid Candidate) ---\n",
        "    found_candidate_for_entry = False\n",
        "    candidate_coords = \"no metal\"\n",
        "    candidate_score = 0\n",
        "    candidate_angles = [None, None, None]\n",
        "\n",
        "    x_range = np.arange(shared_x_min, shared_x_max + 1e-9, grid_resolution)\n",
        "    y_range = np.arange(shared_y_min, shared_y_max + 1e-9, grid_resolution)\n",
        "    z_range = np.arange(shared_z_min, shared_z_max + 1e-9, grid_resolution)\n",
        "\n",
        "    if not (x_range.size > 0 and y_range.size > 0 and z_range.size > 0):\n",
        "         return \"no metal\", 0, [None, None, None]\n",
        "\n",
        "    # Grid search loops...\n",
        "    for x in x_range:\n",
        "        if found_candidate_for_entry: break\n",
        "        for y in y_range:\n",
        "            if found_candidate_for_entry: break\n",
        "            for z in z_range:\n",
        "                if found_candidate_for_entry: break\n",
        "                corner_point = np.array([x, y, z])\n",
        "                center_point = corner_point + grid_resolution / 2.0\n",
        "                points_to_check = [corner_point]\n",
        "                if np.all(center_point < [shared_x_max, shared_y_max, shared_z_max]):\n",
        "                    points_to_check.append(center_point)\n",
        "\n",
        "                for point in points_to_check:\n",
        "                    if found_candidate_for_entry: break\n",
        "                    # --- Filtering Cascade ---\n",
        "                    distances_ca = np.linalg.norm(ca_xyz - point, axis=1)\n",
        "                    distances_cb = np.linalg.norm(cb_xyz - point, axis=1)\n",
        "                    distance_ok = (np.all((thresholds['ca_distances_calc'][0] <= distances_ca) & (distances_ca <= thresholds['ca_distances_calc'][1])) and\n",
        "                                   np.all((thresholds['cb_distances_calc'][0] <= distances_cb) & (distances_cb <= thresholds['cb_distances_calc'][1])))\n",
        "                    if not distance_ok: continue\n",
        "\n",
        "                    angles = calculate_angles(point, ca_xyz, cb_xyz)\n",
        "                    angle_ok = all(thresholds['angle'][0] <= angle <= thresholds['angle'][1] for angle in angles)\n",
        "                    if not angle_ok: continue\n",
        "\n",
        "                    ratios = calculate_ratio(point, ca_xyz, cb_xyz)\n",
        "                    ratio_ok = np.all((thresholds['ratio'][0] <= ratios) & (ratios <= thresholds['ratio'][1]))\n",
        "                    if not ratio_ok: continue\n",
        "\n",
        "                    score = score_zn_predictions(distances_ca, distances_cb, angles, prob_map_3d, ca_bins, cb_bins, angle_bins)\n",
        "                    if score is None or score <= 0: continue\n",
        "\n",
        "                    # !!! Use the KDTree proximity filter !!!\n",
        "                    is_prox_valid = proximity_filter_kdtree(kdtree, point, exclusion_radius=2.5)\n",
        "                    if not is_prox_valid: continue\n",
        "\n",
        "                    # --- Candidate Found! ---\n",
        "                    candidate_coords = point\n",
        "                    candidate_score = score\n",
        "                    candidate_angles = angles\n",
        "                    found_candidate_for_entry = True\n",
        "                    break # Exit points_to_check loop\n",
        "\n",
        "    return candidate_coords, candidate_score, candidate_angles\n",
        "\n",
        "\n",
        "# --- Worker Function for Multiprocessing --- (No changes needed here)\n",
        "def process_pdb_entry(args):\n",
        "    # ... (This function remains the same as the previous multiprocessing version) ...\n",
        "    # ... (It unpacks args, loads structure, reads excel, loops through sites...) ...\n",
        "    # ... (Inside the loop, it calls the NEW estimate_zn_iterative) ...\n",
        "    # ... (It collects results, adds back to df, saves output) ...\n",
        "    # Unpack arguments\n",
        "    excel_path, pdb_directory_base, base_name, output_result_directory, \\\n",
        "    thresholds_local, prob_map_3d_local, ca_bins_local, cb_bins_local, angle_bins_local = args\n",
        "\n",
        "    process_id = os.getpid()\n",
        "    print(f\"[PID {process_id}] Processing PDB ID: {base_name}\")\n",
        "\n",
        "    pdb_path = os.path.join(pdb_directory_base, f\"{base_name}.pdb\")\n",
        "    if not os.path.exists(pdb_path):\n",
        "        print(f\"[PID {process_id}] ❌ Skipping {base_name}, PDB file not found at {pdb_path}.\")\n",
        "        return base_name, False, 0 # Return PDB ID, status, count\n",
        "\n",
        "    # Load structure ONCE for this PDB\n",
        "    structure = load_pdb_structure(base_name, pdb_directory_base)\n",
        "    if structure is None:\n",
        "        print(f\"[PID {process_id}] ❌ Failed to load structure for {base_name}, skipping.\")\n",
        "        return base_name, False, 0\n",
        "\n",
        "    try:\n",
        "        df_alanine = pd.read_excel(excel_path)\n",
        "        if df_alanine.empty:\n",
        "            print(f\"[PID {process_id}] ⚠️ Input Excel file is empty for {base_name}, skipping.\")\n",
        "            return base_name, False, 0\n",
        "\n",
        "        df_alanine['PDB_ID'] = base_name\n",
        "        df_alanine = df_alanine.reset_index(drop=True)\n",
        "\n",
        "        # Check required columns...\n",
        "        ca_cols = ['CA1_X', 'CA1_Y', 'CA1_Z', 'CA2_X', 'CA2_Y', 'CA2_Z', 'CA3_X', 'CA3_Y', 'CA3_Z']\n",
        "        cb_cols = ['CB1_X', 'CB1_Y', 'CB1_Z', 'CB2_X', 'CB2_Y', 'CB2_Z', 'CB3_X', 'CB3_Y', 'CB3_Z']\n",
        "        res_num_cols = ['Coord_residue_number1', 'Coord_residue_number2', 'Coord_residue_number3']\n",
        "        required_input_cols = ca_cols + cb_cols + res_num_cols\n",
        "        if not all(col in df_alanine.columns for col in required_input_cols):\n",
        "            missing_cols = [col for col in required_input_cols if col not in df_alanine.columns]\n",
        "            print(f\"[PID {process_id}] ❌ Missing required columns in {os.path.basename(excel_path)}: {missing_cols}, skipping {base_name}.\")\n",
        "            return base_name, False, 0\n",
        "\n",
        "        # Initialize result columns\n",
        "        df_alanine['Zn_X_Grid'] = None\n",
        "        df_alanine['Zn_Y_Grid'] = None\n",
        "        df_alanine['Zn_Z_Grid'] = None\n",
        "        df_alanine['Zn_Score'] = 0.0\n",
        "        df_alanine['Angle_1'] = None\n",
        "        df_alanine['Angle_2'] = None\n",
        "        df_alanine['Angle_3'] = None\n",
        "\n",
        "        # Iterate through each site (row) in the dataframe for this PDB\n",
        "        for index, site_info in df_alanine.iterrows():\n",
        "            # Extract coordinates for this specific site (flat arrays)\n",
        "            ca_coords_site_flat = site_info[ca_cols].values\n",
        "            cb_coords_site_flat = site_info[cb_cols].values\n",
        "\n",
        "            # Call the modified estimate_zn_iterative for this site\n",
        "            zn_coords, zn_score, zn_angles = estimate_zn_iterative(\n",
        "                ca_coords_site_flat, cb_coords_site_flat, site_info, structure, # Pass site info and loaded structure\n",
        "                thresholds_local, prob_map_3d_local, ca_bins_local, cb_bins_local, angle_bins_local, # Pass data\n",
        "                grid_resolution=0.2\n",
        "            )\n",
        "\n",
        "            # Store results directly back into the DataFrame for this index\n",
        "            df_alanine.loc[index, 'Zn_Score'] = zn_score\n",
        "            if isinstance(zn_coords, np.ndarray):\n",
        "                df_alanine.loc[index, 'Zn_X_Grid'] = zn_coords[0]\n",
        "                df_alanine.loc[index, 'Zn_Y_Grid'] = zn_coords[1]\n",
        "                df_alanine.loc[index, 'Zn_Z_Grid'] = zn_coords[2]\n",
        "            if isinstance(zn_angles, (list, np.ndarray)) and len(zn_angles) == 3:\n",
        "                 df_alanine.loc[index, 'Angle_1'] = zn_angles[0]\n",
        "                 df_alanine.loc[index, 'Angle_2'] = zn_angles[1]\n",
        "                 df_alanine.loc[index, 'Angle_3'] = zn_angles[2]\n",
        "\n",
        "        # --- Post-processing and Saving Results ---\n",
        "        # Filter results after processing all sites for this PDB\n",
        "        df_output = df_alanine[df_alanine['Zn_Score'] > 0].copy()\n",
        "\n",
        "        if not df_output.empty:\n",
        "            output_file_path = os.path.join(output_result_directory, f\"{base_name}_result.xlsx\")\n",
        "            df_output.to_excel(output_file_path, index=False)\n",
        "            print(f\"[PID {process_id}] ✅ Saved results for {len(df_output)} site(s) from {base_name} to: {os.path.basename(output_file_path)}\")\n",
        "            return base_name, True, len(df_output) # Return PDB ID, status, count\n",
        "        else:\n",
        "            print(f\"[PID {process_id}] ⚠️ No valid Zn predictions passed filters for {base_name}.\")\n",
        "            return base_name, True, 0 # Return PDB ID, status, count\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"[PID {process_id}] ❌ Error processing {base_name}: {e}\")\n",
        "        print(traceback.format_exc())\n",
        "        return base_name, False, 0 # Return PDB ID, status, count\n",
        "\n",
        "\n",
        "# --- Main Execution Guard --- (No changes needed here)\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"\\n--- Starting Main Process ---\")\n",
        "    # Data is loaded once here: thresholds, prob_map_3d, ca_bins, cb_bins, angle_bins\n",
        "\n",
        "    # Find input coordinate files using glob\n",
        "    print(f\"\\nSearching for input Excel files in: {excel_directory}\")\n",
        "    excel_files = glob.glob(os.path.join(excel_directory, '*_with_coordinates.xlsx'))\n",
        "    print(f\"Found {len(excel_files)} potential input files.\")\n",
        "\n",
        "    if not excel_files:\n",
        "        print(\"❌ No input Excel files found matching pattern '*_with_coordinates.xlsx'. Exiting.\")\n",
        "        exit()\n",
        "\n",
        "    # Prepare list of arguments for worker processes\n",
        "    tasks = []\n",
        "    for excel_path in excel_files:\n",
        "        base_name = os.path.basename(excel_path).replace('_with_coordinates.xlsx', '')\n",
        "        base_name = base_name.replace('processed_', '')\n",
        "        tasks.append((\n",
        "            excel_path, pdb_directory_base, base_name, output_result_directory,\n",
        "            thresholds, prob_map_3d, ca_bins, cb_bins, angle_bins\n",
        "        ))\n",
        "\n",
        "    # Determine number of processes\n",
        "    num_processes = 8 # Example: Manually set if needed\n",
        "    print(f\"\\nInitializing multiprocessing pool with {num_processes} workers...\")\n",
        "\n",
        "    successful_files = 0\n",
        "    failed_files = 0\n",
        "    total_sites_saved = 0\n",
        "\n",
        "    # Create and run the pool\n",
        "    with multiprocessing.Pool(processes=num_processes) as pool:\n",
        "        print(\"Starting parallel processing...\")\n",
        "        results_iterator = pool.imap_unordered(process_pdb_entry, tasks)\n",
        "        processed_count = 0\n",
        "        for result in results_iterator:\n",
        "            processed_count += 1\n",
        "            pdb_id, status, site_count = result\n",
        "            if status:\n",
        "                successful_files += 1\n",
        "                total_sites_saved += site_count\n",
        "                print(f\"  ({processed_count}/{len(tasks)}) Completed: {pdb_id} ({site_count} sites saved)\")\n",
        "            else:\n",
        "                failed_files += 1\n",
        "                print(f\"  ({processed_count}/{len(tasks)}) Failed/Skipped: {pdb_id}\")\n",
        "\n",
        "    print(\"\\n--- Multiprocessing Pool Finished ---\")\n",
        "    print(f\"\\n🏁 Batch processing summary:\")\n",
        "    print(f\"  Total input files found: {len(tasks)}\")\n",
        "    print(f\"  Successfully processed files: {successful_files}\")\n",
        "    print(f\"  Failed/Skipped files: {failed_files}\")\n",
        "    print(f\"  Total result sites saved: {total_sites_saved}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from Bio.PDB import PDBParser\n",
        "import requests\n",
        "\n",
        "# Markdown documentation for file pathways\n",
        "\n",
        "# @markdown # Step 5: Analysis the result (Apply to the PDB file)\n",
        "\n",
        "# Load input file\n",
        "input_file_path = \"/content/3ttis_coordinates_1_result.xlsx\" # @param {type:\"string\"}\n",
        "df_new = pd.read_excel(input_file_path)\n",
        "\n",
        "# Generate PyMOL script file\n",
        "pymol_script_commands = []\n",
        "df_new['Combination_Number'] = range(1, len(df_new) + 1)\n",
        "\n",
        "# Generate the PyMOL script for both valid and invalid Zn binding forms\n",
        "for index, row in df_new.iterrows():\n",
        "    # Retrieve chain and residue information\n",
        "    chain1, res1 = row['Coord_chain_id_number1'], row['Coord_residue_number1']\n",
        "    chain2, res2 = row['Coord_chain_id_number2'], row['Coord_residue_number2']\n",
        "    chain3, res3 = row['Coord_chain_id_number3'], row['Coord_residue_number3']\n",
        "\n",
        "    # Retrieve Zn coordinates\n",
        "    zn_x, zn_y, zn_z = row['Zn_X_Grid'], row['Zn_Y_Grid'], row['Zn_Z_Grid']\n",
        "\n",
        "    selection_name = f\"obj{row['Combination_Number']:02d}\"\n",
        "\n",
        "    # Select the residues\n",
        "    pymol_script_commands.append(f\"select {selection_name}, (chain {chain1} and resi {res1}) or (chain {chain2} and resi {res2}) or (chain {chain3} and resi {res3})\")\n",
        "\n",
        "    # Create the objects for the residues\n",
        "    pymol_script_commands.append(f\"create {selection_name}_residue1, /{row['PDB_ID']}//{chain1}/{res1}\")\n",
        "    pymol_script_commands.append(f\"create {selection_name}_residue2, /{row['PDB_ID']}//{chain2}/{res2}\")\n",
        "    pymol_script_commands.append(f\"create {selection_name}_residue3, /{row['PDB_ID']}//{chain3}/{res3}\")\n",
        "\n",
        "    # Check if Zn coordinates are available\n",
        "    if not pd.isna(zn_x) and not pd.isna(zn_y) and not pd.isna(zn_z):\n",
        "        # Zn coordinates are present, add the Zn pseudoatom\n",
        "        zn_name = f\"{selection_name}_Metal\"\n",
        "        pymol_script_commands.append(f\"pseudoatom {zn_name}, pos=[{zn_x}, {zn_y}, {zn_z}], elem=Metal, name={zn_name}\")\n",
        "        pymol_script_commands.append(f\"show sphere, {zn_name}\")\n",
        "    else:\n",
        "        # Zn coordinates are missing, mark this combination as non-binding\n",
        "        pymol_script_commands.append(f\"# {selection_name} does not bind Zn\")\n",
        "\n",
        "# Save the commands into a PyMOL script\n",
        "pymol_script_file = \"/content/3tis_final.pml\" # @param {type:\"string\"}\n",
        "with open(pymol_script_file, 'w') as f:\n",
        "    f.write(\"# PyMOL script for visualizing both Zn-binding and non-binding residue combinations\\n\\n\")\n",
        "    for command in pymol_script_commands:\n",
        "        f.write(command + '\\n')\n",
        "\n",
        "print(f\"PyMOL script saved to {pymol_script_file}\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "HHlEaC3XuxM8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}