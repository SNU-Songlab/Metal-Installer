{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4a791b3ac51e4f50b1bb35a25cd1dd6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8bac110eb1344e5986893d4b9e4c0857",
              "IPY_MODEL_aa40ad9cb2a144b1bcf030fa23228166",
              "IPY_MODEL_9cf1fcb561ad4556a2ef41d0f02c6256"
            ],
            "layout": "IPY_MODEL_11d9f25dafb24e508e919c8a33ca401d"
          }
        },
        "8bac110eb1344e5986893d4b9e4c0857": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8e8fe49bcdf94acc95915cea3855a015",
            "placeholder": "​",
            "style": "IPY_MODEL_32ee3e50d4a74a388fb7f0a604f43c52",
            "value": ""
          }
        },
        "aa40ad9cb2a144b1bcf030fa23228166": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e24c1851f11346349d7d13ce6b6af08a",
            "max": 100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_787f3073906944a2af1e328090d548da",
            "value": 100
          }
        },
        "9cf1fcb561ad4556a2ef41d0f02c6256": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_644234f86b4f4433ae48159716ce897e",
            "placeholder": "​",
            "style": "IPY_MODEL_f26c2743cdfa45e7b6efe507785a6f3d",
            "value": " 120/? [00:32&lt;00:00,  3.82it/s]"
          }
        },
        "11d9f25dafb24e508e919c8a33ca401d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e8fe49bcdf94acc95915cea3855a015": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "32ee3e50d4a74a388fb7f0a604f43c52": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e24c1851f11346349d7d13ce6b6af08a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "787f3073906944a2af1e328090d548da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "644234f86b4f4433ae48159716ce897e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f26c2743cdfa45e7b6efe507785a6f3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "4a791b3ac51e4f50b1bb35a25cd1dd6e",
            "8bac110eb1344e5986893d4b9e4c0857",
            "aa40ad9cb2a144b1bcf030fa23228166",
            "9cf1fcb561ad4556a2ef41d0f02c6256",
            "11d9f25dafb24e508e919c8a33ca401d",
            "8e8fe49bcdf94acc95915cea3855a015",
            "32ee3e50d4a74a388fb7f0a604f43c52",
            "e24c1851f11346349d7d13ce6b6af08a",
            "787f3073906944a2af1e328090d548da",
            "644234f86b4f4433ae48159716ce897e",
            "f26c2743cdfa45e7b6efe507785a6f3d"
          ]
        },
        "id": "jhNzixN6wkuo",
        "outputId": "7378fb4c-5308-4c6e-9e43-6fc250f7ac92"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4a791b3ac51e4f50b1bb35a25cd1dd6e"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @markdown # Step 1: Install all neccessary packages\n",
        "#Download pymol in colabsystem\n",
        "from IPython.utils import io\n",
        "import tqdm.notebook\n",
        "import os\n",
        "\"\"\"The PyMOL installation is done inside two nested context managers. This approach\n",
        "was inspired by Dr. Christopher Schlicksup's (of the Phenix group at\n",
        "Lawrence Berkeley National Laboratory) method for installing cctbx\n",
        "in a Colab Notebook. He presented his work on September 1, 2021 at the IUCr\n",
        "Crystallographic Computing School. I adapted Chris's approach here. It replaces my first approach\n",
        "that requires seven steps. My approach was presentated at the SciPy2021 conference\n",
        "in July 2021 and published in the\n",
        "[proceedings](http://conference.scipy.org/proceedings/scipy2021/blaine_mooers.html).\n",
        "The new approach is easier for beginners to use. The old approach is easier to debug\n",
        "and could be used as a back-up approach.\n",
        "\n",
        "\"\"\"\n",
        "total = 100\n",
        "with tqdm.notebook.tqdm(total=total) as pbar:\n",
        "    with io.capture_output() as captured:\n",
        "\n",
        "        !pip install -q condacolab\n",
        "        import condacolab\n",
        "        condacolab.install()\n",
        "        pbar.update(10)\n",
        "\n",
        "        import sys\n",
        "        sys.path.append('/usr/local/lib/python3.7/site-packages/')\n",
        "        pbar.update(20)\n",
        "\n",
        "        # Install PyMOL\n",
        "        %shell mamba install -c schrodinger pymol-bundle --yes\n",
        "\n",
        "        pbar.update(90)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "r9SNE0lbDaJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ne2VAUmTFILs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown # Run prescreening & Extract Coordinates\n",
        "# pip install scipy pytz pandas openpyxl BioPython\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from Bio.PDB import PDBParser # For PDB parsing\n",
        "from Bio.PDB.Residue import Residue # For type hinting if needed\n",
        "from Bio.PDB.Chain import Chain # For type hinting if needed\n",
        "import itertools\n",
        "import requests\n",
        "import traceback\n",
        "from scipy.spatial import KDTree\n",
        "import datetime\n",
        "import pytz\n",
        "from typing import List, Optional, Tuple # For type hinting\n",
        "\n",
        "# --- Configuration ---\n",
        "# Specify the SINGLE PDB file path\n",
        "Target_pdb_file = \"/content/1EP0_alanine_dimer.pdb\" # @param {type:\"string\"}\n",
        "\n",
        "# Specify Specific Residue Number\n",
        "# @markdown ### Specify a residue number (filters for triads containing AT LEAST ONE residue with this number). Use 0 for no specific number filtering.\n",
        "specific_residue_number = 0  # @param {type:\"integer\"}\n",
        "\n",
        "# --- Specify Exact Output Excel File Path ---\n",
        "Output_excel_file_path = \"/content/1EP0_full_Coords.xlsx\"# @param {type:\"string\"}\n",
        "\n",
        "# --- Determine Output Directory from Output File Path ---\n",
        "output_directory = os.path.dirname(Output_excel_file_path)\n",
        "if output_directory:\n",
        "    os.makedirs(output_directory, exist_ok=True)\n",
        "    print(f\"➡️ Output directory: {output_directory}\")\n",
        "else:\n",
        "    output_directory = \".\"\n",
        "    print(f\"➡️ Output directory: Current directory\")\n",
        "\n",
        "print(f\"➡️ Output Excel file with coordinates will be saved as: {Output_excel_file_path}\")\n",
        "\n",
        "if specific_residue_number != 0:\n",
        "    print(f\"⚠️ WARNING: Filtering by residue number '{specific_residue_number}'.\")\n",
        "\n",
        "\n",
        "# --- Download threshold configuration ---\n",
        "# Thresholds file will be saved in the same directory as the output Excel file\n",
        "base_url = \"https://raw.githubusercontent.com/SNU-Songlab/Metal-Installer-code/main/Threshold\"\n",
        "Metal = 'Cu'  # @param [\"Zn\", \"Mn\", \"Cu\", \"Fe\"]\n",
        "Threshold_Download_Set = '3His'  # @param [\"3His\", \"2His_1Asp\", \"2His_1Glu\", \"2His_1Cys\"]\n",
        "Range = '3'  # @param [\"1\", \"2\", \"3\", \"4\",\"5\"]\n",
        "thresholds_url = f\"{base_url}/{Metal}/{Threshold_Download_Set}/{Range}.xlsx\"\n",
        "thresholds_file = os.path.join(output_directory, f\"thresholds_{Metal}_{Threshold_Download_Set}_R{Range}.xlsx\")\n",
        "\n",
        "print(f\"⬇️ Downloading threshold set '{Threshold_Download_Set}' for '{Metal}' (Range {Range}) from: {thresholds_url}\")\n",
        "print(f\"⚠️ NOTE: These thresholds will be applied to any found triad (regardless of residue type).\")\n",
        "\n",
        "response = requests.get(thresholds_url)\n",
        "if response.status_code == 200:\n",
        "    with open(thresholds_file, \"wb\") as file:\n",
        "        file.write(response.content)\n",
        "    print(f\"✅ Thresholds downloaded successfully to {thresholds_file}\")\n",
        "else:\n",
        "    raise ValueError(f\"❌ Failed to download thresholds from {thresholds_url}. Status code: {response.status_code}\")\n",
        "\n",
        "# --- Load thresholds ---\n",
        "print(\"⚙️ Loading thresholds...\")\n",
        "try:\n",
        "    thresholds_df = pd.read_excel(thresholds_file, sheet_name=\"Sheet1\")\n",
        "except FileNotFoundError:\n",
        "     print(f\"❌ Error: Thresholds file not found at {thresholds_file}. Cannot load thresholds.\")\n",
        "     raise # Re-raise the error to stop execution\n",
        "thresholds = {\n",
        "    row[\"Parameter\"]: (row[\"Min\"], row[\"Max\"])\n",
        "    for _, row in thresholds_df.iterrows()\n",
        "    if pd.notna(row[\"Min\"]) and pd.notna(row[\"Max\"])\n",
        "}\n",
        "required_thresholds = [\"alpha_distance_range\", \"beta_distance_range\", \"ratio_threshold_range\", \"pie_threshold_range\"]\n",
        "if not all(key in thresholds for key in required_thresholds):\n",
        "    missing = [key for key in required_thresholds if key not in thresholds]\n",
        "    # Allow script to continue but warn if only geometric coords are needed?\n",
        "    # For now, raise error as geometry checks depend on them.\n",
        "    raise ValueError(f\"❌ Missing required thresholds in the downloaded file: {missing}\")\n",
        "\n",
        "# Only define if present, otherwise geometry checks later will fail if needed\n",
        "alpha_distance_range = thresholds.get(\"alpha_distance_range\")\n",
        "beta_distance_range = thresholds.get(\"beta_distance_range\")\n",
        "ratio_threshold_range = thresholds.get(\"ratio_threshold_range\")\n",
        "pie_threshold_range = thresholds.get(\"pie_threshold_range\")\n",
        "\n",
        "\n",
        "print(\"📊 Thresholds loaded:\")\n",
        "# Check if thresholds were actually loaded before printing\n",
        "if all(t is not None for t in [alpha_distance_range, beta_distance_range, ratio_threshold_range, pie_threshold_range]):\n",
        "    for key, value in thresholds.items():\n",
        "        print(f\"   - {key}: Min={value[0]}, Max={value[1]}\")\n",
        "else:\n",
        "    print(\"   ⚠️ Warning: Could not load all required thresholds.\")\n",
        "\n",
        "\n",
        "# --- Helper Functions ---\n",
        "def calculate_pie(v1, v2):\n",
        "    \"\"\"Calculates the angle (in degrees) between two vectors.\"\"\"\n",
        "    dot = np.dot(v1, v2)\n",
        "    norm = np.linalg.norm(v1) * np.linalg.norm(v2)\n",
        "    if norm == 0: return np.nan\n",
        "    # Clip to avoid domain errors with arccos due to floating point inaccuracies\n",
        "    angle_rad = np.arccos(np.clip(dot / norm, -1.0, 1.0))\n",
        "    return np.degrees(angle_rad)\n",
        "\n",
        "# --- NEW Coordinate Extraction Helper ---\n",
        "def extract_coordinates(chain: Optional[Chain], res_id: int, atom_name: str) -> List[Optional[float]]:\n",
        "    \"\"\"Safely extracts coordinates for a given atom in a residue.\"\"\"\n",
        "    # Return None values immediately if chain object is None\n",
        "    if chain is None:\n",
        "        # print(f\"Debug: Chain object is None for res_id {res_id}, atom {atom_name}\") # Optional Debug\n",
        "        return [np.nan, np.nan, np.nan]\n",
        "    try:\n",
        "        # Use the standard way to access residues in BioPython: tuple key\n",
        "        # Assumes standard residues (no HETATM flag) and no insertion code (' ')\n",
        "        residue_key = (' ', res_id, ' ')\n",
        "        if residue_key not in chain:\n",
        "             # Try accessing just by number for simplicity if the tuple key fails\n",
        "             if res_id not in chain:\n",
        "                  # print(f\"Debug: Residue {res_id} not found in chain {chain.id}\") # Optional Debug\n",
        "                  return [np.nan, np.nan, np.nan]\n",
        "             else:\n",
        "                  residue = chain[res_id] # Fallback access\n",
        "        else:\n",
        "             residue = chain[residue_key] # Preferred access\n",
        "\n",
        "        # Check if the atom exists\n",
        "        if atom_name not in residue:\n",
        "            # print(f\"Debug: Atom {atom_name} not found in residue {res_id} of chain {chain.id}\") # Optional Debug\n",
        "            return [np.nan, np.nan, np.nan]\n",
        "\n",
        "        # Extract coordinates\n",
        "        coord = residue[atom_name].coord\n",
        "        # Ensure coord is a numpy array of size 3, convert Nones to NaN\n",
        "        if coord is None or len(coord) != 3:\n",
        "             return [np.nan, np.nan, np.nan]\n",
        "        # Return list of floats\n",
        "        return [float(c) for c in coord]\n",
        "\n",
        "    except Exception as e:\n",
        "        # Catch any other unexpected errors during lookup/extraction\n",
        "        # print(f\"❌ Error extracting {atom_name} for Res {res_id} in Chain {chain.id if chain else 'N/A'}: {e}\") # Optional Debug\n",
        "        return [np.nan, np.nan, np.nan] # Return list of NaNs on error\n",
        "\n",
        "\n",
        "# --- Main Processing Function ---\n",
        "def process_pdb_file(pdb_file_path, output_excel_path):\n",
        "    \"\"\"\n",
        "    Processes a single PDB file. Finds triads meeting spatial, optional number,\n",
        "    and geometric criteria. Extracts CA/CB coordinates. Saves combined results.\n",
        "    REDUNDANCY REMOVAL IS DISABLED.\n",
        "    \"\"\"\n",
        "    pdb_name = os.path.basename(pdb_file_path)\n",
        "    print(f\"🔄 Processing: {pdb_name} (Number Filter: {specific_residue_number if specific_residue_number != 0 else 'OFF'}, No Redundancy Removal)\")\n",
        "\n",
        "    # --- Check if thresholds loaded ---\n",
        "    if not all(t is not None for t in [alpha_distance_range, beta_distance_range, ratio_threshold_range, pie_threshold_range]):\n",
        "         print(f\"   ❌ Cannot proceed: Required geometric thresholds were not loaded successfully.\")\n",
        "         return # Stop processing this file if thresholds are missing\n",
        "\n",
        "    try:\n",
        "        parser = PDBParser(QUIET=True)\n",
        "        structure = parser.get_structure(\"protein\", pdb_file_path)\n",
        "        # Ensure only one model is processed if multiple exist\n",
        "        if len(structure) > 1:\n",
        "            print(f\"   ⚠️ Warning: Multiple models found in {pdb_name}. Using only the first model (ID: {structure[0].id}).\")\n",
        "        model = structure[0]\n",
        "\n",
        "        # --- Create Chain Lookup ---\n",
        "        # Store chains in a dictionary for faster access\n",
        "        chains_dict = {chain.id: chain for chain in model}\n",
        "        if not chains_dict:\n",
        "             print(f\"   ❌ Error: No chains found in model {model.id} of {pdb_name}.\")\n",
        "             return\n",
        "\n",
        "        all_residues_full = [res for chain in chains_dict.values() for res in chain if res.get_id()[0] == \" \"]\n",
        "        print(f\"   Found {len(all_residues_full)} standard residues.\")\n",
        "\n",
        "        # --- KDTree Pre-filtering ---\n",
        "        residues_for_tree = [res for res in all_residues_full if res.has_id(\"CA\")]\n",
        "        if len(residues_for_tree) < 3:\n",
        "            print(f\"   ⚠️ Skipping {pdb_name}: Not enough residues (<3) with CA atoms.\")\n",
        "            return\n",
        "        coords_ca = np.array([res[\"CA\"].coord for res in residues_for_tree])\n",
        "        residue_map = residues_for_tree # Map KDTree index back to Residue object\n",
        "        kdtree = KDTree(coords_ca)\n",
        "        max_dist = alpha_distance_range[1] * 1.1\n",
        "        pairs = kdtree.query_pairs(r=max_dist)\n",
        "        potential_triad_indices = set()\n",
        "        for i, j in pairs:\n",
        "            indices_k_near_i = kdtree.query_ball_point(coords_ca[i], r=max_dist)\n",
        "            indices_k_near_j = kdtree.query_ball_point(coords_ca[j], r=max_dist)\n",
        "            common_neighbors = set(indices_k_near_i).intersection(indices_k_near_j)\n",
        "            for k in common_neighbors:\n",
        "                if k != i and k != j:\n",
        "                    triad_indices = tuple(sorted((i, j, k)))\n",
        "                    potential_triad_indices.add(triad_indices)\n",
        "        print(f\"   Generated {len(potential_triad_indices)} unique potential spatial triads.\")\n",
        "\n",
        "        # --- Map Indices to Residues ---\n",
        "        all_spatial_triads = []\n",
        "        for idx_i, idx_j, idx_k in potential_triad_indices:\n",
        "             # Ensure indices are within bounds of residue_map\n",
        "             if all(idx < len(residue_map) for idx in [idx_i, idx_j, idx_k]):\n",
        "                 comb = (residue_map[idx_i], residue_map[idx_j], residue_map[idx_k])\n",
        "                 all_spatial_triads.append(comb)\n",
        "             else:\n",
        "                  print(f\"   ⚠️ Warning: KDTree index out of bounds. Skipping triad indices {(idx_i, idx_j, idx_k)}.\")\n",
        "        print(f\"   Mapped {len(all_spatial_triads)} spatial triads.\")\n",
        "\n",
        "\n",
        "        # --- Apply Residue NUMBER Filter ---\n",
        "        triads_meeting_number_criteria = []\n",
        "        if specific_residue_number == 0:\n",
        "            print(f\"   Specific Residue Number Filter: OFF\")\n",
        "            triads_meeting_number_criteria = all_spatial_triads\n",
        "        else:\n",
        "            print(f\"   Filtering {len(all_spatial_triads)} triads: At least one residue must have number {specific_residue_number}\")\n",
        "            for comb in all_spatial_triads:\n",
        "                if any(res.get_id()[1] == specific_residue_number for res in comb):\n",
        "                    triads_meeting_number_criteria.append(comb)\n",
        "            print(f\"   {len(triads_meeting_number_criteria)} triads have at least one residue with number {specific_residue_number}.\")\n",
        "\n",
        "        # --- Final list for geometric checks ---\n",
        "        final_triads_to_process = triads_meeting_number_criteria\n",
        "        print(f\"   Proceeding to detailed geometric checks for {len(final_triads_to_process)} triads.\")\n",
        "\n",
        "        # --- Detailed Geometric Filtering & Initial DataFrame Creation ---\n",
        "        def get_triad_type(comb):\n",
        "            chains = [res.get_full_id()[2] for res in comb]\n",
        "            return \"intra\" if len(set(chains)) == 1 else \"inter\"\n",
        "\n",
        "        results = []\n",
        "        for comb in final_triads_to_process:\n",
        "            try:\n",
        "                # Ensure CA and CB atoms exist for distance calculations\n",
        "                if not all(res.has_id(\"CA\") and res.has_id(\"CB\") for res in comb): continue\n",
        "\n",
        "                alpha_distances, beta_distances = [], []\n",
        "                valid_distances = True\n",
        "                for res1, res2 in itertools.combinations(comb, 2):\n",
        "                     d_ca = np.linalg.norm(res1[\"CA\"].coord - res2[\"CA\"].coord)\n",
        "                     d_cb = np.linalg.norm(res1[\"CB\"].coord - res2[\"CB\"].coord)\n",
        "                     # Apply distance thresholds\n",
        "                     if not (alpha_distance_range[0] <= d_ca <= alpha_distance_range[1] and \\\n",
        "                             beta_distance_range[0] <= d_cb <= beta_distance_range[1]):\n",
        "                         valid_distances = False; break\n",
        "                     alpha_distances.append(d_ca); beta_distances.append(d_cb)\n",
        "\n",
        "                if valid_distances and len(alpha_distances) == 3:\n",
        "                     row = {\"PDB_ID\": pdb_name, \"Triad_Type\": get_triad_type(comb)}\n",
        "                     for i, res in enumerate(comb):\n",
        "                         full_id = res.get_full_id()\n",
        "                         row[f\"Coord_chain_id_number{i+1}\"] = full_id[2]\n",
        "                         row[f\"Coord_residue_number{i+1}\"] = res.get_id()[1]\n",
        "                         row[f\"Coord_residue_name{i+1}\"] = res.get_resname()\n",
        "                     for i in range(3):\n",
        "                         row[f\"Alpha Distance {i+1}\"] = alpha_distances[i]\n",
        "                         row[f\"Beta Distance {i+1}\"] = beta_distances[i]\n",
        "                     results.append(row) # Add row if distances are valid\n",
        "\n",
        "            except KeyError: continue # Skip if atoms are missing during coord access\n",
        "            except Exception as e_inner: print(f\"     Error processing triad {comb} during geometry checks: {e_inner}\"); continue\n",
        "\n",
        "        print(f\"   Found {len(results)} triads passing initial distance filters.\")\n",
        "        df = pd.DataFrame(results) # DataFrame of triads passing distance filter\n",
        "\n",
        "        # --- Apply Ratio Filter ---\n",
        "        if not df.empty:\n",
        "             def pass_ratio(row):\n",
        "                 try:\n",
        "                     for i in range(3):\n",
        "                         # Check for zero beta distance before division\n",
        "                         if row[f\"Beta Distance {i+1}\"] is None or row[f\"Beta Distance {i+1}\"] == 0: return False\n",
        "                         ratio = row[f\"Alpha Distance {i+1}\"] / row[f\"Beta Distance {i+1}\"]\n",
        "                         if not (ratio_threshold_range[0] <= ratio <= ratio_threshold_range[1]): return False\n",
        "                     return True\n",
        "                 except (TypeError, KeyError): return False # Handle potential None values or missing keys\n",
        "             df_ratio = df[df.apply(pass_ratio, axis=1)].copy()\n",
        "             print(f\"   Found {len(df_ratio)} triads passing ratio filter.\")\n",
        "        else: df_ratio = pd.DataFrame()\n",
        "\n",
        "        # --- Apply Pie Angle Filter ---\n",
        "        if not df_ratio.empty:\n",
        "            def compute_pie(row):\n",
        "                try:\n",
        "                    # Use chains_dict (created earlier from the model) for lookup\n",
        "                    chain1 = chains_dict.get(row['Coord_chain_id_number1'])\n",
        "                    chain2 = chains_dict.get(row['Coord_chain_id_number2'])\n",
        "                    chain3 = chains_dict.get(row['Coord_chain_id_number3'])\n",
        "                    if not all([chain1, chain2, chain3]): # Check if all chains were found\n",
        "                         return pd.Series([np.nan]*3, index=[\"Pie_1_2\", \"Pie_1_3\", \"Pie_2_3\"])\n",
        "\n",
        "                    # Retrieve residue objects (handle potential errors if res number not in chain)\n",
        "                    res1 = chain1[(' ', row['Coord_residue_number1'], ' ')] if (' ', row['Coord_residue_number1'], ' ') in chain1 else None\n",
        "                    res2 = chain2[(' ', row['Coord_residue_number2'], ' ')] if (' ', row['Coord_residue_number2'], ' ') in chain2 else None\n",
        "                    res3 = chain3[(' ', row['Coord_residue_number3'], ' ')] if (' ', row['Coord_residue_number3'], ' ') in chain3 else None\n",
        "                    if not all([res1, res2, res3]):\n",
        "                        return pd.Series([np.nan]*3, index=[\"Pie_1_2\", \"Pie_1_3\", \"Pie_2_3\"])\n",
        "\n",
        "                    res_objs = [res1, res2, res3]\n",
        "                    angles = []\n",
        "                    for i, j in [(0,1), (0,2), (1,2)]:\n",
        "                         # Check atoms exist before calculating vectors\n",
        "                         if not (res_objs[i].has_id(\"CA\") and res_objs[i].has_id(\"CB\") and \\\n",
        "                                 res_objs[j].has_id(\"CA\") and res_objs[j].has_id(\"CB\")):\n",
        "                             angles.append(np.nan) # Append NaN if atoms missing\n",
        "                             continue\n",
        "                         v_ca = res_objs[j][\"CA\"].coord - res_objs[i][\"CA\"].coord\n",
        "                         v_cb = res_objs[j][\"CB\"].coord - res_objs[i][\"CB\"].coord\n",
        "                         angles.append(calculate_pie(v_ca, v_cb))\n",
        "                    # Ensure we always return a Series of length 3\n",
        "                    while len(angles) < 3: angles.append(np.nan)\n",
        "                    return pd.Series(angles, index=[\"Pie_1_2\", \"Pie_1_3\", \"Pie_2_3\"])\n",
        "\n",
        "                except Exception: # Catch any other error during residue lookup or calculation\n",
        "                    return pd.Series([np.nan]*3, index=[\"Pie_1_2\", \"Pie_1_3\", \"Pie_2_3\"])\n",
        "\n",
        "\n",
        "            pie_results = df_ratio.apply(compute_pie, axis=1)\n",
        "            df_ratio[[\"Pie_1_2\", \"Pie_1_3\", \"Pie_2_3\"]] = pie_results\n",
        "            # Filter based on calculated pie angles\n",
        "            for col in [\"Pie_1_2\", \"Pie_1_3\", \"Pie_2_3\"]:\n",
        "                 # Apply filter only if pie angle is not NaN\n",
        "                 df_ratio[f\"{col}_Filter\"] = df_ratio[col].apply(\n",
        "                     lambda x: pie_threshold_range[0] < x < pie_threshold_range[1] if pd.notnull(x) else False)\n",
        "            df_ratio['Pie_Filter'] = df_ratio[[f'{col}_Filter' for col in ['Pie_1_2', 'Pie_1_3', 'Pie_2_3']]].all(axis=1)\n",
        "            df_final = df_ratio[df_ratio['Pie_Filter']].copy() # This is the final DataFrame before coordinate extraction\n",
        "            print(f\"   Found {len(df_final)} triads passing pie angle filter.\")\n",
        "        else: df_final = pd.DataFrame() # df_final is empty if df_ratio was empty\n",
        "\n",
        "\n",
        "        # --- Coordinate Extraction (Integrated Step 3) ---\n",
        "        df_with_coords = pd.DataFrame() # Initialize empty DataFrame for results\n",
        "        if not df_final.empty:\n",
        "            print(f\"   Extracting CA and CB coordinates for {len(df_final)} final triads...\")\n",
        "            ca_coords, cb_coords = [], []\n",
        "\n",
        "            # Use the chains_dict created earlier\n",
        "            for idx, row in df_final.iterrows():\n",
        "                # Get chain objects using the dictionary (safer than re-parsing)\n",
        "                # Use .get() to handle potential missing chain IDs gracefully\n",
        "                chain1 = chains_dict.get(row['Coord_chain_id_number1'])\n",
        "                chain2 = chains_dict.get(row['Coord_chain_id_number2'])\n",
        "                chain3 = chains_dict.get(row['Coord_chain_id_number3'])\n",
        "\n",
        "                # Extract Cα coordinates\n",
        "                ca1 = extract_coordinates(chain1, row['Coord_residue_number1'], 'CA')\n",
        "                ca2 = extract_coordinates(chain2, row['Coord_residue_number2'], 'CA')\n",
        "                ca3 = extract_coordinates(chain3, row['Coord_residue_number3'], 'CA')\n",
        "                ca_coords.append([*ca1, *ca2, *ca3]) # Flatten coordinates [x1,y1,z1, x2,y2,z2, x3,y3,z3]\n",
        "\n",
        "                # Extract Cβ coordinates\n",
        "                cb1 = extract_coordinates(chain1, row['Coord_residue_number1'], 'CB')\n",
        "                cb2 = extract_coordinates(chain2, row['Coord_residue_number2'], 'CB')\n",
        "                cb3 = extract_coordinates(chain3, row['Coord_residue_number3'], 'CB')\n",
        "                cb_coords.append([*cb1, *cb2, *cb3]) # Flatten coordinates\n",
        "\n",
        "            # Define column names for coordinate DataFrames\n",
        "            ca_cols = ['CA1_X', 'CA1_Y', 'CA1_Z', 'CA2_X', 'CA2_Y', 'CA2_Z', 'CA3_X', 'CA3_Y', 'CA3_Z']\n",
        "            cb_cols = ['CB1_X', 'CB1_Y', 'CB1_Z', 'CB2_X', 'CB2_Y', 'CB2_Z', 'CB3_X', 'CB3_Y', 'CB3_Z']\n",
        "\n",
        "            # Create DataFrames from the extracted coordinates\n",
        "            df_ca = pd.DataFrame(ca_coords, columns=ca_cols)\n",
        "            df_cb = pd.DataFrame(cb_coords, columns=cb_cols)\n",
        "\n",
        "            # Combine the original filtered data with the new coordinate data\n",
        "            # Use reset_index to ensure indices align correctly during concatenation\n",
        "            df_with_coords = pd.concat([df_final.reset_index(drop=True), df_ca, df_cb], axis=1)\n",
        "            print(f\"   Coordinate extraction complete.\")\n",
        "\n",
        "        else:\n",
        "            print(\"   No triads passed all filters, skipping coordinate extraction.\")\n",
        "            df_with_coords = df_final # Assign empty df_final if no triads\n",
        "\n",
        "\n",
        "        # --- Redundancy Removal DISABLED ---\n",
        "\n",
        "        # --- Output ---\n",
        "        # Save the DataFrame that includes coordinates (df_with_coords)\n",
        "        if not df_with_coords.empty:\n",
        "             print(f\"   Saving final results with coordinates...\")\n",
        "             # Use the exact output path provided\n",
        "             with pd.ExcelWriter(output_excel_path) as writer:\n",
        "                  num_suffix = f\"_Num{specific_residue_number}\" if specific_residue_number != 0 else \"_ANY_Num\"\n",
        "                  # Update sheet name to reflect content\n",
        "                  sheet_name = f\"Final_Coords{num_suffix}\"\n",
        "                  # Save the DataFrame with coordinates\n",
        "                  df_with_coords.to_excel(writer, sheet_name=sheet_name, index=False)\n",
        "             print(f\"✅ Finished: {pdb_name}. Results with coordinates saved to {output_excel_path} (Total triads found: {len(df_with_coords)})\")\n",
        "        else:\n",
        "             print(f\"✅ Finished: {pdb_name}. No triads passed all filters. No output file created.\")\n",
        "\n",
        "    except FileNotFoundError: print(f\"❌ Error: Input PDB file not found at {pdb_file_path}\")\n",
        "    except Exception as e: print(f\"❌ An unexpected error occurred while processing {pdb_name}: {e}\"); traceback.print_exc()\n",
        "\n",
        "\n",
        "# --- Run Processing for the Single PDB File ---\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        kst = pytz.timezone('Asia/Seoul')\n",
        "        current_time_kst = datetime.datetime.now(kst)\n",
        "        print(f\"\\n--- Starting Single File Processing & Coordinate Extraction ---\") # Updated title\n",
        "        print(f\"Current Time (KST): {current_time_kst.strftime('%Y-%m-%d %H:%M:%S %Z%z')}\")\n",
        "    except ImportError:\n",
        "        print(\"\\n--- Starting Single File Processing & Coordinate Extraction ---\")\n",
        "        print(\"Note: Could not determine KST time (pytz not installed?). Run 'pip install pytz' if needed.\")\n",
        "\n",
        "    print(f\"Input PDB File: {Target_pdb_file}\")\n",
        "    print(f\"Specific Number Filter: {specific_residue_number if specific_residue_number != 0 else 'OFF'}\")\n",
        "    print(f\"Threshold Set Used (for download): {Metal} / {Threshold_Download_Set} / Range {Range}\")\n",
        "    print(f\"Outputting results with coordinates to file: {Output_excel_file_path}\") # Updated description\n",
        "    print(f\"(Thresholds file saved in: {output_directory})\")\n",
        "    print(\"Redundancy Removal: DISABLED\")\n",
        "\n",
        "    if not os.path.isfile(Target_pdb_file):\n",
        "        print(f\"⚠️ Error: Input PDB file not found at '{Target_pdb_file}'. Please check the path.\")\n",
        "    else:\n",
        "        # Call the combined processing function\n",
        "        process_pdb_file(Target_pdb_file, Output_excel_file_path)\n",
        "\n",
        "    print(\"\\n🎉 Processing finished.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "y6OrfuPIGA29",
        "outputId": "c02ec887-93bb-41f3-f576-0e4cc14bd601"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Finished: 1EP0_alanine_dimer.pdb. Results with coordinates saved to /content/1EP0_full_Coords.xlsx (Total triads found: 4314)\n",
            "\n",
            "🎉 Processing finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown # Step 4: Probability density map (Parallel Processing WITHIN a Single PDB)\n",
        "# pip install scipy pandas openpyxl requests BioPython numpy pytz\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import requests\n",
        "import traceback # Import traceback for better error printing\n",
        "import multiprocessing # Import multiprocessing\n",
        "from Bio.PDB import PDBParser # PDBParser is still needed for structure loading\n",
        "from scipy.spatial import KDTree # Import KDTree\n",
        "import time # For timing if desired\n",
        "import datetime\n",
        "import pytz\n",
        "from typing import List, Optional, Tuple # For type hinting\n",
        "from Bio.PDB.Chain import Chain # For type hinting if needed\n",
        "\n",
        "\n",
        "# --- Configuration and Setup ---\n",
        "# Define paths for the SINGLE input coordinate Excel file and the corresponding PDB file\n",
        "input_coords_file = '/content/1EP0_full_Coords.xlsx' # @param {type:\"string\"}\n",
        "Input_pdb_file = '/content/1EP0_alanine_dimer.pdb' # @param {type:\"string\"}\n",
        "Output_result_excel_file = '/content/Full_result.xlsx' # @param {type:\"string\"}\n",
        "# Create output directory if it doesn't exist\n",
        "output_result_directory = os.path.dirname(Output_result_excel_file)\n",
        "if output_result_directory:\n",
        "    os.makedirs(output_result_directory, exist_ok=True)\n",
        "    print(f\"➡️ Output directory: {output_result_directory}\")\n",
        "else:\n",
        "    # If no directory specified, output files go in the current directory\n",
        "    output_result_directory = \".\"\n",
        "    print(f\"➡️ Output directory: Current directory\")\n",
        "\n",
        "\n",
        "# Check if input files exist\n",
        "if not os.path.isfile(input_coords_file):\n",
        "    raise FileNotFoundError(f\"Input coordinate Excel file not found: {Input_coordinate_excel_file}\")\n",
        "if not os.path.isfile(Input_pdb_file):\n",
        "    raise FileNotFoundError(f\"Input PDB file not found: {Input_pdb_file}\")\n",
        "\n",
        "# Define local file paths for downloaded data\n",
        "prob_map_file = os.path.join(output_result_directory, 'map.xlsx') # Save downloads in output dir\n",
        "thresholds_file = os.path.join(output_result_directory, 'threshold.xlsx')\n",
        "\n",
        "# --- Download Data from GitHub ---\n",
        "# Using the parameters from your previous examples (adjust if needed)\n",
        "base_url = \"https://raw.githubusercontent.com/SNU-Songlab/Metal-Installer-code/main/probability/\"\n",
        "Metal = 'Cu'  # @param [\"Zn\", \"Mn\", \"Cu\", \"Fe\"]\n",
        "Combinations = '3His'  # @param [\"3His\", \"2His_1Asp\", \"2His_1Glu\", \"2His_1Cys\"]\n",
        "map_url = f\"{base_url}/{Metal}/{Combinations}/map.xlsx\"\n",
        "thresholds_url = f\"{base_url}/{Metal}/{Combinations}/threshold.xlsx\"\n",
        "\n",
        "print(f\"Downloading probability map from: {map_url}\")\n",
        "response_map = requests.get(map_url)\n",
        "if response_map.status_code == 200:\n",
        "    with open(prob_map_file, 'wb') as file:\n",
        "        file.write(response_map.content)\n",
        "    print(f\"Downloaded map data to {prob_map_file}\")\n",
        "else:\n",
        "    raise ValueError(f\"Failed to download map file from {map_url}. Status code: {response_map.status_code}\")\n",
        "\n",
        "print(f\"Downloading thresholds from: {thresholds_url}\")\n",
        "response_thresh = requests.get(thresholds_url)\n",
        "if response_thresh.status_code == 200:\n",
        "    with open(thresholds_file, 'wb') as file:\n",
        "        file.write(response_thresh.content)\n",
        "    print(f\"Downloaded thresholds data to {thresholds_file}\")\n",
        "else:\n",
        "    raise ValueError(f\"Failed to download thresholds file from {thresholds_url}. Status code: {response_thresh.status_code}\")\n",
        "\n",
        "\n",
        "# --- Load and Process Data (Load ONCE in the main process) ---\n",
        "print(\"\\n--- Loading Shared Data ---\")\n",
        "# Load PDB Structure\n",
        "pdb_id = os.path.splitext(os.path.basename(Input_pdb_file))[0]\n",
        "print(f\"Loading PDB structure: {pdb_id}...\")\n",
        "pdb_parser = PDBParser(QUIET=True)\n",
        "try:\n",
        "    structure = pdb_parser.get_structure(pdb_id, Input_pdb_file)\n",
        "    print(f\"Loaded structure.\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error loading PDB file {Input_pdb_file}: {e}\")\n",
        "    raise # Stop if structure cannot be loaded\n",
        "\n",
        "# Load Thresholds\n",
        "print(\"Loading thresholds...\")\n",
        "try:\n",
        "    thresholds_df = pd.read_excel(thresholds_file, sheet_name='Sheet1')\n",
        "except FileNotFoundError:\n",
        "    print(f\"❌ Error: Thresholds file not found at {thresholds_file}\")\n",
        "    raise\n",
        "thresholds = {}\n",
        "for _, row in thresholds_df.iterrows():\n",
        "    parameter = row['Parameter']\n",
        "    min_value = row['Min']\n",
        "    max_value = row['Max']\n",
        "    if pd.notna(min_value) and pd.notna(max_value):\n",
        "        thresholds[parameter] = (min_value, max_value)\n",
        "\n",
        "required_keys = ['ca_distances_calc', 'cb_distances_calc', 'ratio', 'angle']\n",
        "if not all(key in thresholds for key in required_keys):\n",
        "    missing_keys = [key for key in required_keys if key not in thresholds]\n",
        "    raise KeyError(f\"Missing key(s) {missing_keys} in thresholds file.\")\n",
        "print(\"Thresholds loaded.\")\n",
        "\n",
        "# Load and Process Probability Map\n",
        "print(\"Loading probability map...\")\n",
        "try:\n",
        "    df_precomputed_prob_map = pd.read_excel(prob_map_file)\n",
        "except FileNotFoundError:\n",
        "    print(f\"❌ Error: Probability map file not found at {prob_map_file}\")\n",
        "    raise\n",
        "\n",
        "print(\"Processing probability map...\")\n",
        "map_req_cols = ['Calpha_Zn_Dist', 'Cbeta_Zn_Dist', 'CA-Zn-CB_Angle', 'Probability']\n",
        "if not all(col in df_precomputed_prob_map.columns for col in map_req_cols):\n",
        "     missing_map_cols = [col for col in map_req_cols if col not in df_precomputed_prob_map.columns]\n",
        "     raise ValueError(f\"Missing required columns in map file: {missing_map_cols}\")\n",
        "\n",
        "ca_bins = np.sort(df_precomputed_prob_map['Calpha_Zn_Dist'].unique())\n",
        "cb_bins = np.sort(df_precomputed_prob_map['Cbeta_Zn_Dist'].unique())\n",
        "angle_bins = np.sort(df_precomputed_prob_map['CA-Zn-CB_Angle'].unique())\n",
        "# --- Check Indentation Carefully Here ---\n",
        "try:\n",
        "    pivoted_prob_map = df_precomputed_prob_map.pivot_table(\n",
        "        index='Calpha_Zn_Dist', columns=['Cbeta_Zn_Dist', 'CA-Zn-CB_Angle'], values='Probability', fill_value=0\n",
        "    ) # Line ~119\n",
        "    expected_shape = (len(ca_bins), len(cb_bins) * len(angle_bins))\n",
        "    if pivoted_prob_map.shape == expected_shape:\n",
        "        # Correct indentation (4 spaces relative to 'if')\n",
        "        prob_map_3d = pivoted_prob_map.values.reshape((len(ca_bins), len(cb_bins), len(angle_bins)))\n",
        "        print(\"Probability map processed into 3D array.\")\n",
        "    else:\n",
        "        # Correct indentation (4 spaces relative to 'else')\n",
        "        raise ValueError(f\"Pivoted map shape {pivoted_prob_map.shape} doesn't match expected shape {expected_shape} for reshaping.\") # Likely Line 126 area\n",
        "# Correct indentation (aligned with 'try')\n",
        "except Exception as e:\n",
        "    # Correct indentation (4 spaces relative to 'except')\n",
        "    print(f\"❌ Error processing probability map: {e}\")\n",
        "    raise # Re-raise the caught exception\n",
        "\n",
        "# Load Input Coordinate Data\n",
        "print(f\"Loading input coordinate data from: {input_coords_file}...\")\n",
        "try:\n",
        "    df_sites = pd.read_excel(input_coords_file)\n",
        "except FileNotFoundError:\n",
        "     print(f\"❌ Error: Input coordinate file not found at {input_coords_file}\")\n",
        "     raise\n",
        "if df_sites.empty:\n",
        "    print(\"⚠️ Input coordinate file is empty. Nothing to process.\")\n",
        "    exit()\n",
        "# Ensure PDB_ID column exists or add it based on filename\n",
        "if 'PDB_ID' not in df_sites.columns:\n",
        "     df_sites['PDB_ID'] = pdb_id\n",
        "print(f\"Loaded {len(df_sites)} candidate sites.\")\n",
        "\n",
        "# --- Helper Function Definitions ---\n",
        "# (calculate_ratio, calculate_angles, score_zn_predictions, define_excluded_triads,\n",
        "#  proximity_filter_kdtree, estimate_zn_iterative remain unchanged from previous version)\n",
        "def calculate_ratio(current_point, ca_xyz, cb_xyz):\n",
        "    ca_distances = np.linalg.norm(ca_xyz - current_point, axis=1)\n",
        "    cb_distances = np.linalg.norm(cb_xyz - current_point, axis=1)\n",
        "    # Handle potential division by zero\n",
        "    ratios = np.divide(ca_distances, cb_distances, out=np.full_like(ca_distances, np.inf), where=cb_distances!=0)\n",
        "    return ratios\n",
        "\n",
        "def calculate_angles(zn_coords, ca_coords_triplet, cb_coords_triplet):\n",
        "    angles = []\n",
        "    for i in range(3):\n",
        "        v_ca = ca_coords_triplet[i] - zn_coords\n",
        "        v_cb = cb_coords_triplet[i] - zn_coords\n",
        "        norm_v_ca = np.linalg.norm(v_ca)\n",
        "        norm_v_cb = np.linalg.norm(v_cb)\n",
        "        if norm_v_ca == 0 or norm_v_cb == 0:\n",
        "            angles.append(np.nan) # Use NaN for undefined angles\n",
        "            continue\n",
        "        # Clip argument to avoid domain errors due to floating point inaccuracies\n",
        "        cos_theta = np.clip(np.dot(v_ca, v_cb) / (norm_v_ca * norm_v_cb), -1.0, 1.0)\n",
        "        angle_rad = np.arccos(cos_theta)\n",
        "        angles.append(np.degrees(angle_rad))\n",
        "    # Ensure list always has 3 elements, padding with NaN if necessary\n",
        "    while len(angles) < 3: angles.append(np.nan)\n",
        "    return angles\n",
        "\n",
        "def score_zn_predictions(ca_distances, cb_distances, angles, prob_map_3d, ca_bins, cb_bins, angle_bins):\n",
        "    # Check for NaN inputs\n",
        "    if np.isnan(ca_distances).any() or np.isnan(cb_distances).any() or np.isnan(angles).any():\n",
        "        return 0.0\n",
        "\n",
        "    # Digitize finds the index of the bin each value belongs to.\n",
        "    # right=True means bins are [left, right)\n",
        "    ca_bin_indices = np.clip(np.digitize(ca_distances, ca_bins[1:], right=True), 0, len(ca_bins)-1)\n",
        "    cb_bin_indices = np.clip(np.digitize(cb_distances, cb_bins[1:], right=True), 0, len(cb_bins)-1)\n",
        "    angle_bin_indices = np.clip(np.digitize(angles, angle_bins[1:], right=True), 0, len(angle_bins)-1)\n",
        "\n",
        "    probabilities = []\n",
        "    valid = True\n",
        "    try:\n",
        "        # Use advanced indexing to get probabilities for all 3 residues at once\n",
        "        probs = prob_map_3d[ca_bin_indices, cb_bin_indices, angle_bin_indices]\n",
        "        # Check if any probability is non-positive\n",
        "        if np.any(probs <= 0):\n",
        "            valid = False\n",
        "        else:\n",
        "            probabilities = probs\n",
        "    except IndexError:\n",
        "        valid = False # Indices out of bounds\n",
        "    except Exception:\n",
        "        valid = False # Other potential errors\n",
        "\n",
        "    # Calculate final score only if all 3 probabilities were valid (positive)\n",
        "    final_score = np.prod(probabilities) if valid and len(probabilities) == 3 else 0.0\n",
        "    return final_score\n",
        "\n",
        "def define_excluded_triads(triad_res_nums, structure):\n",
        "    excluded_residues = set()\n",
        "    if structure is None: return excluded_residues\n",
        "    try:\n",
        "        # Ensure input numbers are valid integers\n",
        "        res_nums_to_find = set(int(num) for num in triad_res_nums if pd.notna(num))\n",
        "    except (ValueError, TypeError):\n",
        "        print(f\"⚠️ Warning: Could not convert all triad residue numbers {triad_res_nums} to integers for exclusion.\")\n",
        "        return excluded_residues\n",
        "\n",
        "    if not res_nums_to_find: return excluded_residues # No valid numbers provided\n",
        "\n",
        "    for model in structure:\n",
        "        for chain in model:\n",
        "            for residue in chain:\n",
        "                try:\n",
        "                    # residue.id format is (hetfield, sequence_identifier, insertion_code)\n",
        "                    res_seq_num = residue.id[1]\n",
        "                    if res_seq_num in res_nums_to_find:\n",
        "                        excluded_residues.add((chain.id, res_seq_num))\n",
        "                except (TypeError, IndexError):\n",
        "                    continue # Skip if residue ID format is unexpected\n",
        "    return excluded_residues\n",
        "\n",
        "def proximity_filter_kdtree(kdtree, zn_candidate, exclusion_radius=2.5):\n",
        "    if kdtree is None: return True # Assume valid if no tree was built\n",
        "    try:\n",
        "        # Find indices of points within the exclusion radius\n",
        "        indices_nearby = kdtree.query_ball_point(zn_candidate, r=exclusion_radius, return_length=True)\n",
        "        # Return True if no points are found nearby (length is 0)\n",
        "        return indices_nearby == 0\n",
        "    except Exception as e:\n",
        "        # Log error and return False (fail safe) if KDTree query fails\n",
        "        # print(f\"❌ Error during KDTree query: {e}\") # Optional Debug\n",
        "        return False\n",
        "\n",
        "def estimate_zn_iterative(\n",
        "    ca_coords_site_flat,\n",
        "    cb_coords_site_flat,\n",
        "    site_info,\n",
        "    structure_local,\n",
        "    thresholds_local,\n",
        "    prob_map_3d_local, ca_bins_local, cb_bins_local, angle_bins_local,\n",
        "    grid_resolution=0.2\n",
        "    ):\n",
        "    \"\"\"Estimates Zn coordinate for a SINGLE site.\"\"\"\n",
        "    # --- Coordinate Validation ---\n",
        "    try:\n",
        "        ca_coords_numeric = pd.to_numeric(np.asarray(ca_coords_site_flat), errors='coerce')\n",
        "        cb_coords_numeric = pd.to_numeric(np.asarray(cb_coords_site_flat), errors='coerce')\n",
        "        if np.isnan(ca_coords_numeric).any() or np.isnan(cb_coords_numeric).any():\n",
        "            return \"no metal\", 0, [np.nan, np.nan, np.nan]\n",
        "        if ca_coords_numeric.shape != (9,) or cb_coords_numeric.shape != (9,):\n",
        "             return \"no metal\", 0, [np.nan, np.nan, np.nan]\n",
        "        ca_xyz = ca_coords_numeric.astype(np.float64).reshape(3, 3)\n",
        "        cb_xyz = cb_coords_numeric.astype(np.float64).reshape(3, 3)\n",
        "    except (ValueError, TypeError):\n",
        "        return \"no metal\", 0, [np.nan, np.nan, np.nan]\n",
        "\n",
        "    if structure_local is None: return \"no metal\", 0, [np.nan, np.nan, np.nan]\n",
        "\n",
        "    # --- Excluded Residues & KDTree ---\n",
        "    triad_res_nums = [\n",
        "        site_info.get('Coord_residue_number1'), # Use .get() for safety\n",
        "        site_info.get('Coord_residue_number2'),\n",
        "        site_info.get('Coord_residue_number3')\n",
        "    ]\n",
        "    excluded_residues_set = define_excluded_triads(triad_res_nums, structure_local)\n",
        "\n",
        "    non_excluded_coords_list = []\n",
        "    try:\n",
        "        for atom in structure_local.get_atoms():\n",
        "            residue = atom.get_parent()\n",
        "            chain = residue.get_parent()\n",
        "            if residue is None or chain is None: continue\n",
        "            res_info = (chain.id, residue.id[1])\n",
        "            if res_info not in excluded_residues_set:\n",
        "                 # Check if coord is valid ndarray\n",
        "                 if isinstance(atom.coord, np.ndarray) and atom.coord.shape == (3,):\n",
        "                      non_excluded_coords_list.append(atom.coord)\n",
        "    except Exception as atom_iter_err:\n",
        "        print(f\"Warning: Error iterating atoms for KDTree build: {atom_iter_err}\")\n",
        "\n",
        "    kdtree = None\n",
        "    if non_excluded_coords_list:\n",
        "        try:\n",
        "            non_excluded_coords = np.array(non_excluded_coords_list, dtype=np.float64)\n",
        "            if non_excluded_coords.ndim == 2 and non_excluded_coords.shape[1] == 3 and non_excluded_coords.shape[0] > 0:\n",
        "                 kdtree = KDTree(non_excluded_coords)\n",
        "        except Exception:\n",
        "             pass # kdtree remains None\n",
        "\n",
        "    # --- Search Space ---\n",
        "    shared_x_min, shared_x_max = -np.inf, np.inf\n",
        "    shared_y_min, shared_y_max = -np.inf, np.inf\n",
        "    shared_z_min, shared_z_max = -np.inf, np.inf\n",
        "    buffer_dist = max(thresholds_local['ca_distances_calc'][1], thresholds_local['cb_distances_calc'][1])\n",
        "    for j in range(3):\n",
        "        x_min_j, x_max_j = ca_xyz[j, 0] - buffer_dist, ca_xyz[j, 0] + buffer_dist\n",
        "        y_min_j, y_max_j = ca_xyz[j, 1] - buffer_dist, ca_xyz[j, 1] + buffer_dist\n",
        "        z_min_j, z_max_j = ca_xyz[j, 2] - buffer_dist, ca_xyz[j, 2] + buffer_dist\n",
        "        shared_x_min, shared_x_max = max(shared_x_min, x_min_j), min(shared_x_max, x_max_j)\n",
        "        shared_y_min, shared_y_max = max(shared_y_min, y_min_j), min(shared_y_max, y_max_j)\n",
        "        shared_z_min, shared_z_max = max(shared_z_min, z_min_j), min(shared_z_max, z_max_j)\n",
        "\n",
        "    buffer_grid = grid_resolution * 2\n",
        "    shared_x_min, shared_x_max = shared_x_min - buffer_grid, shared_x_max + buffer_grid\n",
        "    shared_y_min, shared_y_max = shared_y_min - buffer_grid, shared_y_max + buffer_grid\n",
        "    shared_z_min, shared_z_max = shared_z_min - buffer_grid, shared_z_max + buffer_grid\n",
        "\n",
        "    if not (shared_x_min < shared_x_max and shared_y_min < shared_y_max and shared_z_min < shared_z_max):\n",
        "        return \"no metal\", 0, [np.nan, np.nan, np.nan]\n",
        "\n",
        "    # --- Grid Search ---\n",
        "    x_range = np.arange(shared_x_min, shared_x_max, grid_resolution)\n",
        "    y_range = np.arange(shared_y_min, shared_y_max, grid_resolution)\n",
        "    z_range = np.arange(shared_z_min, shared_z_max, grid_resolution)\n",
        "    if not (x_range.size > 0 and y_range.size > 0 and z_range.size > 0):\n",
        "        return \"no metal\", 0, [np.nan, np.nan, np.nan]\n",
        "\n",
        "    for x in x_range:\n",
        "        for y in y_range:\n",
        "            z_coords = z_range\n",
        "            num_z = len(z_coords)\n",
        "            points = np.column_stack([np.full(num_z, x), np.full(num_z, y), z_coords])\n",
        "\n",
        "            dist_ca = np.linalg.norm(ca_xyz[np.newaxis, :, :] - points[:, np.newaxis, :], axis=2)\n",
        "            dist_cb = np.linalg.norm(cb_xyz[np.newaxis, :, :] - points[:, np.newaxis, :], axis=2)\n",
        "            dist_ca_ok = np.all((thresholds_local['ca_distances_calc'][0] <= dist_ca) & (dist_ca <= thresholds_local['ca_distances_calc'][1]), axis=1)\n",
        "            dist_cb_ok = np.all((thresholds_local['cb_distances_calc'][0] <= dist_cb) & (dist_cb <= thresholds_local['cb_distances_calc'][1]), axis=1)\n",
        "            dist_ok_mask = dist_ca_ok & dist_cb_ok\n",
        "            if not np.any(dist_ok_mask): continue\n",
        "\n",
        "            points_dist_ok = points[dist_ok_mask]\n",
        "            dist_ca_filt = dist_ca[dist_ok_mask]\n",
        "            dist_cb_filt = dist_cb[dist_ok_mask]\n",
        "\n",
        "            for i, point in enumerate(points_dist_ok):\n",
        "                current_dist_ca = dist_ca_filt[i]; current_dist_cb = dist_cb_filt[i]\n",
        "                angles = calculate_angles(point, ca_xyz, cb_xyz)\n",
        "                if np.isnan(angles).any() or not all(thresholds_local['angle'][0] <= ang <= thresholds_local['angle'][1] for ang in angles if pd.notna(ang)): continue\n",
        "\n",
        "                ratios = calculate_ratio(point, ca_xyz, cb_xyz)\n",
        "                if np.isinf(ratios).any() or not np.all((thresholds_local['ratio'][0] <= ratios) & (ratios <= thresholds_local['ratio'][1])): continue\n",
        "\n",
        "                score = score_zn_predictions(current_dist_ca, current_dist_cb, angles, prob_map_3d_local, ca_bins_local, cb_bins_local, angle_bins_local)\n",
        "                if score <= 0: continue\n",
        "\n",
        "                if not proximity_filter_kdtree(kdtree, point, exclusion_radius=2.0): continue\n",
        "\n",
        "                return point, score, angles # Return first valid point\n",
        "\n",
        "    return \"no metal\", 0, [np.nan, np.nan, np.nan] # No point found\n",
        "\n",
        "# --- Worker Function for Multiprocessing ---\n",
        "def process_single_site(args):\n",
        "    \"\"\"Worker function to process a single candidate site (row).\"\"\"\n",
        "    site_index, site_data_dict, structure_shared, thresholds_shared, \\\n",
        "    prob_map_3d_shared, ca_bins_shared, cb_bins_shared, angle_bins_shared = args\n",
        "\n",
        "    ca_cols = ['CA1_X', 'CA1_Y', 'CA1_Z', 'CA2_X', 'CA2_Y', 'CA2_Z', 'CA3_X', 'CA3_Y', 'CA3_Z']\n",
        "    cb_cols = ['CB1_X', 'CB1_Y', 'CB1_Z', 'CB2_X', 'CB2_Y', 'CB2_Z', 'CB3_X', 'CB3_Y', 'CB3_Z']\n",
        "\n",
        "    try:\n",
        "        # Extract data directly from the dictionary\n",
        "        ca_coords_flat = np.array([site_data_dict[col] for col in ca_cols], dtype=np.float64)\n",
        "        cb_coords_flat = np.array([site_data_dict[col] for col in cb_cols], dtype=np.float64)\n",
        "\n",
        "        zn_coords, zn_score, zn_angles = estimate_zn_iterative(\n",
        "            ca_coords_flat, cb_coords_flat, site_data_dict, structure_shared,\n",
        "            thresholds_shared, prob_map_3d_shared, ca_bins_shared,\n",
        "            cb_bins_shared, angle_bins_shared,\n",
        "            grid_resolution=0.2\n",
        "        )\n",
        "        return site_index, zn_coords, zn_score, zn_angles\n",
        "\n",
        "    except Exception as e:\n",
        "        # print(f\"❌ Error in worker processing site index {site_index}: {e}\") # Optional Debug\n",
        "        return site_index, \"error\", 0, [np.nan, np.nan, np.nan]\n",
        "\n",
        "# --- Main Execution Guard ---\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        kst = pytz.timezone('Asia/Seoul')\n",
        "        current_time_kst = datetime.datetime.now(kst)\n",
        "        print(f\"\\n--- Starting Main Process for Single PDB Site Parallelization ---\")\n",
        "        print(f\"Current Time (KST): {current_time_kst.strftime('%Y-%m-%d %H:%M:%S %Z%z')}\")\n",
        "    except ImportError:\n",
        "        print(\"\\n--- Starting Main Process for Single PDB Site Parallelization ---\")\n",
        "        print(\"Note: Could not determine KST time (pytz not installed?). Run 'pip install pytz' if needed.\")\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    # --- Prepare Tasks for Multiprocessing ---\n",
        "    tasks = []\n",
        "    required_input_cols = (\n",
        "        ['CA1_X', 'CA1_Y', 'CA1_Z', 'CA2_X', 'CA2_Y', 'CA2_Z', 'CA3_X', 'CA3_Y', 'CA3_Z'] +\n",
        "        ['CB1_X', 'CB1_Y', 'CB1_Z', 'CB2_X', 'CB2_Y', 'CB2_Z', 'CB3_X', 'CB3_Y', 'CB3_Z'] +\n",
        "        ['Coord_residue_number1', 'Coord_residue_number2', 'Coord_residue_number3', 'PDB_ID']\n",
        "    )\n",
        "    if not all(col in df_sites.columns for col in required_input_cols):\n",
        "         missing_cols = [col for col in required_input_cols if col not in df_sites.columns]\n",
        "         raise ValueError(f\"Input coordinate Excel file is missing required columns: {missing_cols}\")\n",
        "\n",
        "    # Convert relevant parts of DataFrame to list of dicts for potentially better pickling\n",
        "    sites_data_list = df_sites[required_input_cols].to_dict('records')\n",
        "\n",
        "    for index, site_dict in enumerate(sites_data_list):\n",
        "        tasks.append((\n",
        "            df_sites.index[index], # Original DataFrame index\n",
        "            site_dict,            # Dictionary for this site\n",
        "            structure,\n",
        "            thresholds,\n",
        "            prob_map_3d,\n",
        "            ca_bins, cb_bins, angle_bins\n",
        "        ))\n",
        "\n",
        "    print(f\"Prepared {len(tasks)} tasks for parallel processing.\")\n",
        "\n",
        "    # --- Initialize Results Columns ---\n",
        "    df_sites['Zn_X_Grid'] = np.nan\n",
        "    df_sites['Zn_Y_Grid'] = np.nan\n",
        "    df_sites['Zn_Z_Grid'] = np.nan\n",
        "    df_sites['Zn_Score'] = 0.0\n",
        "    df_sites['Angle_1'] = np.nan\n",
        "    df_sites['Angle_2'] = np.nan\n",
        "    df_sites['Angle_3'] = np.nan\n",
        "\n",
        "    num_processes = min(8, os.cpu_count())\n",
        "    print(f\"\\nInitializing multiprocessing pool with {num_processes} workers...\")\n",
        "\n",
        "    processed_count = 0\n",
        "    # --- Run Multiprocessing ---\n",
        "    # Using try/finally to ensure pool cleanup\n",
        "    pool = None # Initialize pool variable\n",
        "    try:\n",
        "        pool = multiprocessing.Pool(processes=num_processes)\n",
        "        print(\"Starting parallel processing of sites...\")\n",
        "        results_iterator = pool.imap_unordered(process_single_site, tasks)\n",
        "\n",
        "        for result in results_iterator:\n",
        "            processed_count += 1\n",
        "            try:\n",
        "                site_idx, zn_coords_res, zn_score_res, zn_angles_res = result\n",
        "\n",
        "                df_sites.loc[site_idx, 'Zn_Score'] = zn_score_res\n",
        "                if isinstance(zn_coords_res, np.ndarray) and zn_coords_res.shape == (3,):\n",
        "                     df_sites.loc[site_idx, 'Zn_X_Grid'] = zn_coords_res[0]\n",
        "                     df_sites.loc[site_idx, 'Zn_Y_Grid'] = zn_coords_res[1]\n",
        "                     df_sites.loc[site_idx, 'Zn_Z_Grid'] = zn_coords_res[2]\n",
        "                if isinstance(zn_angles_res, (list, np.ndarray)) and len(zn_angles_res) == 3:\n",
        "                    # Convert potential numpy floats to python floats before assigning if needed\n",
        "                    df_sites.loc[site_idx, 'Angle_1'] = float(zn_angles_res[0]) if pd.notna(zn_angles_res[0]) else np.nan\n",
        "                    df_sites.loc[site_idx, 'Angle_2'] = float(zn_angles_res[1]) if pd.notna(zn_angles_res[1]) else np.nan\n",
        "                    df_sites.loc[site_idx, 'Angle_3'] = float(zn_angles_res[2]) if pd.notna(zn_angles_res[2]) else np.nan\n",
        "\n",
        "                if processed_count % 50 == 0 or processed_count == len(tasks):\n",
        "                     print(f\"  Processed {processed_count}/{len(tasks)} sites...\")\n",
        "\n",
        "            except Exception as result_error:\n",
        "                 print(f\"❌ Error processing result for one site: {result_error}\")\n",
        "                 if isinstance(result, tuple) and len(result) > 0: print(f\"   Problem occurred for site index: {result[0]}\")\n",
        "                 continue\n",
        "\n",
        "        print(f\"Finished processing all {processed_count} assigned tasks.\")\n",
        "\n",
        "    except Exception as pool_error:\n",
        "        print(f\"❌ An error occurred during multiprocessing: {pool_error}\")\n",
        "        print(traceback.format_exc())\n",
        "    finally:\n",
        "        # Ensure pool resources are released\n",
        "        if pool:\n",
        "            pool.close()\n",
        "            pool.join()\n",
        "\n",
        "    print(\"\\n--- Multiprocessing Pool Finished ---\")\n",
        "\n",
        "    # --- Post-processing and Saving Results ---\n",
        "    print(\"Filtering results (Zn_Score > 0)...\")\n",
        "    # Ensure Zn_Score is numeric before filtering\n",
        "    df_sites['Zn_Score'] = pd.to_numeric(df_sites['Zn_Score'], errors='coerce').fillna(0.0)\n",
        "    df_output = df_sites[df_sites['Zn_Score'] > 0].copy()\n",
        "\n",
        "    total_sites_saved = len(df_output)\n",
        "\n",
        "    if not df_output.empty:\n",
        "        print(f\"Saving {total_sites_saved} site(s) with positive scores...\")\n",
        "        try:\n",
        "            # Convert coordinate columns to float before saving if they are object type\n",
        "            for col in ['Zn_X_Grid', 'Zn_Y_Grid', 'Zn_Z_Grid', 'Angle_1', 'Angle_2', 'Angle_3']:\n",
        "                 if col in df_output.columns:\n",
        "                      df_output[col] = pd.to_numeric(df_output[col], errors='coerce')\n",
        "            df_output.to_excel(Output_result_excel_file, index=False)\n",
        "            print(f\"✅ Successfully saved results to: {Output_result_excel_file}\")\n",
        "        except Exception as save_error:\n",
        "            print(f\"❌ Error saving results to Excel file: {save_error}\")\n",
        "            print(traceback.format_exc()) # Print traceback for saving errors\n",
        "    else:\n",
        "        print(\"⚠️ No valid Zn predictions passed filters (Score > 0). No output file created.\")\n",
        "\n",
        "    end_time = time.time()\n",
        "    print(f\"\\n🏁 Processing summary:\")\n",
        "    print(f\"  Total input sites: {len(df_sites)}\")\n",
        "    print(f\"  Total sites processed by pool: {processed_count}\") # Note: Might be less than total if errors occur early\n",
        "    print(f\"  Total result sites saved (Score > 0): {total_sites_saved}\")\n",
        "    print(f\"  Total execution time: {end_time - start_time:.2f} seconds\")\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "0xckYLFVHUZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown # Step 4: Probability density map (Parallel Processing WITHIN a Single PDB - Find BEST Score)\n",
        "# pip install scipy pandas openpyxl requests BioPython numpy pytz\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import requests\n",
        "import traceback # Import traceback for better error printing\n",
        "import multiprocessing # Import multiprocessing\n",
        "from Bio.PDB import PDBParser # PDBParser is still needed for structure loading\n",
        "from scipy.spatial import KDTree # Import KDTree\n",
        "import time # For timing if desired\n",
        "import datetime\n",
        "import pytz\n",
        "from typing import List, Optional, Tuple # For type hinting\n",
        "from Bio.PDB.Chain import Chain # For type hinting if needed\n",
        "\n",
        "\n",
        "# --- Configuration and Setup ---\n",
        "# Define paths for the SINGLE input coordinate Excel file and the corresponding PDB file\n",
        "input_coords_file = '/content/1EP0_full_Coords.xlsx' # @param {type:\"string\"}\n",
        "Input_pdb_file = '/content/1EP0_alanine_dimer.pdb' # @param {type:\"string\"}\n",
        "Output_result_excel_file = '/content/1EP0_full_best_score.xlsx' # @param {type:\"string\"}\n",
        "# Create output directory if it doesn't exist\n",
        "output_result_directory = os.path.dirname(Output_result_excel_file)\n",
        "if output_result_directory:\n",
        "    os.makedirs(output_result_directory, exist_ok=True)\n",
        "    print(f\"➡️ Output directory: {output_result_directory}\")\n",
        "else:\n",
        "    output_result_directory = \".\"\n",
        "    print(f\"➡️ Output directory: Current directory\")\n",
        "\n",
        "\n",
        "# Check if input files exist\n",
        "if not os.path.isfile(input_coords_file):\n",
        "    # Corrected variable name in error message\n",
        "    raise FileNotFoundError(f\"Input coordinate Excel file not found: {input_coords_file}\")\n",
        "if not os.path.isfile(Input_pdb_file):\n",
        "    raise FileNotFoundError(f\"Input PDB file not found: {Input_pdb_file}\")\n",
        "\n",
        "# Define local file paths for downloaded data\n",
        "prob_map_file = os.path.join(output_result_directory, 'map.xlsx')\n",
        "thresholds_file = os.path.join(output_result_directory, 'threshold.xlsx')\n",
        "\n",
        "# --- Download Data from GitHub ---\n",
        "base_url = \"https://raw.githubusercontent.com/SNU-Songlab/Metal-Installer-code/main/probability/\"\n",
        "Metal = 'Cu'  # @param [\"Zn\", \"Mn\", \"Cu\", \"Fe\"]\n",
        "Combinations = '3His'  # @param [\"3His\", \"2His_1Asp\", \"2His_1Glu\", \"2His_1Cys\"]\n",
        "map_url = f\"{base_url}/{Metal}/{Combinations}/map.xlsx\"\n",
        "thresholds_url = f\"{base_url}/{Metal}/{Combinations}/threshold.xlsx\"\n",
        "\n",
        "print(f\"Downloading probability map from: {map_url}\")\n",
        "response_map = requests.get(map_url)\n",
        "if response_map.status_code == 200:\n",
        "    with open(prob_map_file, 'wb') as file:\n",
        "        file.write(response_map.content)\n",
        "    print(f\"Downloaded map data to {prob_map_file}\")\n",
        "else:\n",
        "    raise ValueError(f\"Failed to download map file from {map_url}. Status code: {response_map.status_code}\")\n",
        "\n",
        "print(f\"Downloading thresholds from: {thresholds_url}\")\n",
        "response_thresh = requests.get(thresholds_url)\n",
        "if response_thresh.status_code == 200:\n",
        "    with open(thresholds_file, 'wb') as file:\n",
        "        file.write(response_thresh.content)\n",
        "    print(f\"Downloaded thresholds data to {thresholds_file}\")\n",
        "else:\n",
        "    raise ValueError(f\"Failed to download thresholds file from {thresholds_url}. Status code: {response_thresh.status_code}\")\n",
        "\n",
        "\n",
        "# --- Load and Process Data (Load ONCE in the main process) ---\n",
        "print(\"\\n--- Loading Shared Data ---\")\n",
        "# Load PDB Structure\n",
        "pdb_id = os.path.splitext(os.path.basename(Input_pdb_file))[0]\n",
        "print(f\"Loading PDB structure: {pdb_id}...\")\n",
        "pdb_parser = PDBParser(QUIET=True)\n",
        "try:\n",
        "    structure = pdb_parser.get_structure(pdb_id, Input_pdb_file)\n",
        "    print(f\"Loaded structure.\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error loading PDB file {Input_pdb_file}: {e}\")\n",
        "    raise\n",
        "\n",
        "# Load Thresholds\n",
        "print(\"Loading thresholds...\")\n",
        "try:\n",
        "    thresholds_df = pd.read_excel(thresholds_file, sheet_name='Sheet1')\n",
        "except FileNotFoundError:\n",
        "    print(f\"❌ Error: Thresholds file not found at {thresholds_file}\")\n",
        "    raise\n",
        "thresholds = {}\n",
        "for _, row in thresholds_df.iterrows():\n",
        "    parameter = row['Parameter']\n",
        "    min_value = row['Min']\n",
        "    max_value = row['Max']\n",
        "    if pd.notna(min_value) and pd.notna(max_value):\n",
        "        thresholds[parameter] = (min_value, max_value)\n",
        "\n",
        "required_keys = ['ca_distances_calc', 'cb_distances_calc', 'ratio', 'angle']\n",
        "if not all(key in thresholds for key in required_keys):\n",
        "    missing_keys = [key for key in required_keys if key not in thresholds]\n",
        "    raise KeyError(f\"Missing key(s) {missing_keys} in thresholds file.\")\n",
        "print(\"Thresholds loaded.\")\n",
        "\n",
        "# Load and Process Probability Map\n",
        "print(\"Loading probability map...\")\n",
        "try:\n",
        "    df_precomputed_prob_map = pd.read_excel(prob_map_file)\n",
        "except FileNotFoundError:\n",
        "    print(f\"❌ Error: Probability map file not found at {prob_map_file}\")\n",
        "    raise\n",
        "\n",
        "print(\"Processing probability map...\")\n",
        "map_req_cols = ['Calpha_Zn_Dist', 'Cbeta_Zn_Dist', 'CA-Zn-CB_Angle', 'Probability']\n",
        "if not all(col in df_precomputed_prob_map.columns for col in map_req_cols):\n",
        "     missing_map_cols = [col for col in map_req_cols if col not in df_precomputed_prob_map.columns]\n",
        "     raise ValueError(f\"Missing required columns in map file: {missing_map_cols}\")\n",
        "\n",
        "ca_bins = np.sort(df_precomputed_prob_map['Calpha_Zn_Dist'].unique())\n",
        "cb_bins = np.sort(df_precomputed_prob_map['Cbeta_Zn_Dist'].unique())\n",
        "angle_bins = np.sort(df_precomputed_prob_map['CA-Zn-CB_Angle'].unique())\n",
        "\n",
        "try:\n",
        "    pivoted_prob_map = df_precomputed_prob_map.pivot_table(\n",
        "        index='Calpha_Zn_Dist', columns=['Cbeta_Zn_Dist', 'CA-Zn-CB_Angle'], values='Probability', fill_value=0\n",
        "    )\n",
        "    expected_shape = (len(ca_bins), len(cb_bins) * len(angle_bins))\n",
        "    if pivoted_prob_map.shape == expected_shape:\n",
        "        prob_map_3d = pivoted_prob_map.values.reshape((len(ca_bins), len(cb_bins), len(angle_bins)))\n",
        "        print(\"Probability map processed into 3D array.\")\n",
        "    else:\n",
        "        raise ValueError(f\"Pivoted map shape {pivoted_prob_map.shape} doesn't match expected shape {expected_shape} for reshaping.\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error processing probability map: {e}\")\n",
        "    raise\n",
        "\n",
        "# Load Input Coordinate Data\n",
        "print(f\"Loading input coordinate data from: {input_coords_file}...\")\n",
        "try:\n",
        "    # Make sure to use the correct variable name here\n",
        "    df_sites = pd.read_excel(input_coords_file)\n",
        "except FileNotFoundError:\n",
        "     print(f\"❌ Error: Input coordinate file not found at {input_coords_file}\")\n",
        "     raise\n",
        "if df_sites.empty:\n",
        "    print(\"⚠️ Input coordinate file is empty. Nothing to process.\")\n",
        "    exit()\n",
        "# Ensure PDB_ID column exists or add it based on filename\n",
        "if 'PDB_ID' not in df_sites.columns:\n",
        "     df_sites['PDB_ID'] = pdb_id\n",
        "print(f\"Loaded {len(df_sites)} candidate sites.\")\n",
        "\n",
        "# --- Helper Function Definitions ---\n",
        "# (calculate_ratio, calculate_angles, score_zn_predictions, define_excluded_triads,\n",
        "#  proximity_filter_kdtree remain unchanged)\n",
        "def calculate_ratio(current_point, ca_xyz, cb_xyz):\n",
        "    ca_distances = np.linalg.norm(ca_xyz - current_point, axis=1)\n",
        "    cb_distances = np.linalg.norm(cb_xyz - current_point, axis=1)\n",
        "    ratios = np.divide(ca_distances, cb_distances, out=np.full_like(ca_distances, np.inf), where=cb_distances!=0)\n",
        "    return ratios\n",
        "\n",
        "def calculate_angles(zn_coords, ca_coords_triplet, cb_coords_triplet):\n",
        "    angles = []\n",
        "    for i in range(3):\n",
        "        v_ca = ca_coords_triplet[i] - zn_coords\n",
        "        v_cb = cb_coords_triplet[i] - zn_coords\n",
        "        norm_v_ca = np.linalg.norm(v_ca)\n",
        "        norm_v_cb = np.linalg.norm(v_cb)\n",
        "        if norm_v_ca == 0 or norm_v_cb == 0:\n",
        "            angles.append(np.nan); continue\n",
        "        cos_theta = np.clip(np.dot(v_ca, v_cb) / (norm_v_ca * norm_v_cb), -1.0, 1.0)\n",
        "        angle_rad = np.arccos(cos_theta)\n",
        "        angles.append(np.degrees(angle_rad))\n",
        "    while len(angles) < 3: angles.append(np.nan)\n",
        "    return angles\n",
        "\n",
        "def score_zn_predictions(ca_distances, cb_distances, angles, prob_map_3d, ca_bins, cb_bins, angle_bins):\n",
        "    if np.isnan(ca_distances).any() or np.isnan(cb_distances).any() or np.isnan(angles).any(): return 0.0\n",
        "    ca_bin_indices = np.clip(np.digitize(ca_distances, ca_bins[1:], right=True), 0, len(ca_bins)-1)\n",
        "    cb_bin_indices = np.clip(np.digitize(cb_distances, cb_bins[1:], right=True), 0, len(cb_bins)-1)\n",
        "    angle_bin_indices = np.clip(np.digitize(angles, angle_bins[1:], right=True), 0, len(angle_bins)-1)\n",
        "    probabilities = []\n",
        "    valid = True\n",
        "    try:\n",
        "        probs = prob_map_3d[ca_bin_indices, cb_bin_indices, angle_bin_indices]\n",
        "        if np.any(probs <= 0): valid = False\n",
        "        else: probabilities = probs\n",
        "    except IndexError: valid = False\n",
        "    except Exception: valid = False\n",
        "    final_score = np.prod(probabilities) if valid and len(probabilities) == 3 else 0.0\n",
        "    return final_score\n",
        "\n",
        "def define_excluded_triads(triad_res_nums, structure):\n",
        "    excluded_residues = set()\n",
        "    if structure is None: return excluded_residues\n",
        "    try:\n",
        "        res_nums_to_find = set(int(num) for num in triad_res_nums if pd.notna(num))\n",
        "    except (ValueError, TypeError): return excluded_residues\n",
        "    if not res_nums_to_find: return excluded_residues\n",
        "    for model in structure:\n",
        "        for chain in model:\n",
        "            for residue in chain:\n",
        "                try:\n",
        "                    res_seq_num = residue.id[1]\n",
        "                    if res_seq_num in res_nums_to_find: excluded_residues.add((chain.id, res_seq_num))\n",
        "                except (TypeError, IndexError): continue\n",
        "    return excluded_residues\n",
        "\n",
        "def proximity_filter_kdtree(kdtree, zn_candidate, exclusion_radius=2.5):\n",
        "    if kdtree is None: return True\n",
        "    try:\n",
        "        indices_nearby = kdtree.query_ball_point(zn_candidate, r=exclusion_radius, return_length=True)\n",
        "        return indices_nearby == 0\n",
        "    except Exception: return False\n",
        "\n",
        "# --- MODIFIED Zn Estimation Function (Finds BEST Score) ---\n",
        "def estimate_zn_iterative(\n",
        "    ca_coords_site_flat,\n",
        "    cb_coords_site_flat,\n",
        "    site_info,\n",
        "    structure_local,\n",
        "    thresholds_local,\n",
        "    prob_map_3d_local, ca_bins_local, cb_bins_local, angle_bins_local,\n",
        "    grid_resolution=0.2\n",
        "    ):\n",
        "    \"\"\"\n",
        "    Estimates Zn coordinate for a SINGLE site by searching the entire grid\n",
        "    and returning the position with the HIGHEST score.\n",
        "    \"\"\"\n",
        "    # --- Coordinate Validation ---\n",
        "    try:\n",
        "        ca_coords_numeric = pd.to_numeric(np.asarray(ca_coords_site_flat), errors='coerce')\n",
        "        cb_coords_numeric = pd.to_numeric(np.asarray(cb_coords_site_flat), errors='coerce')\n",
        "        if np.isnan(ca_coords_numeric).any() or np.isnan(cb_coords_numeric).any():\n",
        "            return \"no metal\", 0, [np.nan, np.nan, np.nan]\n",
        "        if ca_coords_numeric.shape != (9,) or cb_coords_numeric.shape != (9,):\n",
        "             return \"no metal\", 0, [np.nan, np.nan, np.nan]\n",
        "        ca_xyz = ca_coords_numeric.astype(np.float64).reshape(3, 3)\n",
        "        cb_xyz = cb_coords_numeric.astype(np.float64).reshape(3, 3)\n",
        "    except (ValueError, TypeError):\n",
        "        return \"no metal\", 0, [np.nan, np.nan, np.nan]\n",
        "\n",
        "    if structure_local is None: return \"no metal\", 0, [np.nan, np.nan, np.nan]\n",
        "\n",
        "    # --- Excluded Residues & KDTree ---\n",
        "    triad_res_nums = [site_info.get(f'Coord_residue_number{i+1}') for i in range(3)]\n",
        "    excluded_residues_set = define_excluded_triads(triad_res_nums, structure_local)\n",
        "    non_excluded_coords_list = []\n",
        "    try:\n",
        "        for atom in structure_local.get_atoms():\n",
        "            residue = atom.get_parent(); chain = residue.get_parent()\n",
        "            if residue is None or chain is None: continue\n",
        "            res_info = (chain.id, residue.id[1])\n",
        "            if res_info not in excluded_residues_set:\n",
        "                 if isinstance(atom.coord, np.ndarray) and atom.coord.shape == (3,):\n",
        "                      non_excluded_coords_list.append(atom.coord)\n",
        "    except Exception as atom_iter_err:\n",
        "        print(f\"Warning: Error iterating atoms for KDTree build: {atom_iter_err}\")\n",
        "    kdtree = None\n",
        "    if non_excluded_coords_list:\n",
        "        try:\n",
        "            non_excluded_coords = np.array(non_excluded_coords_list, dtype=np.float64)\n",
        "            if non_excluded_coords.ndim == 2 and non_excluded_coords.shape[1] == 3 and non_excluded_coords.shape[0] > 0:\n",
        "                 kdtree = KDTree(non_excluded_coords)\n",
        "        except Exception: pass\n",
        "\n",
        "    # --- Search Space ---\n",
        "    shared_x_min, shared_x_max = -np.inf, np.inf; shared_y_min, shared_y_max = -np.inf, np.inf; shared_z_min, shared_z_max = -np.inf, np.inf\n",
        "    buffer_dist = max(thresholds_local['ca_distances_calc'][1], thresholds_local['cb_distances_calc'][1])\n",
        "    for j in range(3):\n",
        "        x_min_j, x_max_j = ca_xyz[j, 0] - buffer_dist, ca_xyz[j, 0] + buffer_dist\n",
        "        y_min_j, y_max_j = ca_xyz[j, 1] - buffer_dist, ca_xyz[j, 1] + buffer_dist\n",
        "        z_min_j, z_max_j = ca_xyz[j, 2] - buffer_dist, ca_xyz[j, 2] + buffer_dist\n",
        "        shared_x_min, shared_x_max = max(shared_x_min, x_min_j), min(shared_x_max, x_max_j)\n",
        "        shared_y_min, shared_y_max = max(shared_y_min, y_min_j), min(shared_y_max, y_max_j)\n",
        "        shared_z_min, shared_z_max = max(shared_z_min, z_min_j), min(shared_z_max, z_max_j)\n",
        "    buffer_grid = grid_resolution * 2\n",
        "    shared_x_min, shared_x_max = shared_x_min - buffer_grid, shared_x_max + buffer_grid\n",
        "    shared_y_min, shared_y_max = shared_y_min - buffer_grid, shared_y_max + buffer_grid\n",
        "    shared_z_min, shared_z_max = shared_z_min - buffer_grid, shared_z_max + buffer_grid\n",
        "    if not (shared_x_min < shared_x_max and shared_y_min < shared_y_max and shared_z_min < shared_z_max): return \"no metal\", 0, [np.nan, np.nan, np.nan]\n",
        "\n",
        "    # --- Grid Search Initialization ---\n",
        "    x_range = np.arange(shared_x_min, shared_x_max, grid_resolution)\n",
        "    y_range = np.arange(shared_y_min, shared_y_max, grid_resolution)\n",
        "    z_range = np.arange(shared_z_min, shared_z_max, grid_resolution)\n",
        "    if not (x_range.size > 0 and y_range.size > 0 and z_range.size > 0): return \"no metal\", 0, [np.nan, np.nan, np.nan]\n",
        "\n",
        "    # ***** MODIFICATION START *****\n",
        "    best_score = 0.0\n",
        "    best_coords = \"no metal\"\n",
        "    best_angles = [np.nan, np.nan, np.nan]\n",
        "    # ***** MODIFICATION END *****\n",
        "\n",
        "    # --- Grid Search Loop ---\n",
        "    for x in x_range:\n",
        "        for y in y_range:\n",
        "            z_coords = z_range\n",
        "            num_z = len(z_coords)\n",
        "            points = np.column_stack([np.full(num_z, x), np.full(num_z, y), z_coords])\n",
        "\n",
        "            # Vectorized distance check\n",
        "            dist_ca = np.linalg.norm(ca_xyz[np.newaxis, :, :] - points[:, np.newaxis, :], axis=2)\n",
        "            dist_cb = np.linalg.norm(cb_xyz[np.newaxis, :, :] - points[:, np.newaxis, :], axis=2)\n",
        "            dist_ca_ok = np.all((thresholds_local['ca_distances_calc'][0] <= dist_ca) & (dist_ca <= thresholds_local['ca_distances_calc'][1]), axis=1)\n",
        "            dist_cb_ok = np.all((thresholds_local['cb_distances_calc'][0] <= dist_cb) & (dist_cb <= thresholds_local['cb_distances_calc'][1]), axis=1)\n",
        "            dist_ok_mask = dist_ca_ok & dist_cb_ok\n",
        "            if not np.any(dist_ok_mask): continue\n",
        "\n",
        "            # Filter points that passed distance check\n",
        "            points_dist_ok = points[dist_ok_mask]\n",
        "            dist_ca_filt = dist_ca[dist_ok_mask]\n",
        "            dist_cb_filt = dist_cb[dist_ok_mask]\n",
        "\n",
        "            # Iterate through potentially valid points\n",
        "            for i, point in enumerate(points_dist_ok):\n",
        "                current_dist_ca = dist_ca_filt[i]; current_dist_cb = dist_cb_filt[i]\n",
        "\n",
        "                # Angle Filter\n",
        "                angles = calculate_angles(point, ca_xyz, cb_xyz)\n",
        "                # Check if angles are valid and within range\n",
        "                if np.isnan(angles).any() or not all(thresholds_local['angle'][0] <= ang <= thresholds_local['angle'][1] for ang in angles if pd.notna(ang)): continue\n",
        "\n",
        "                # Ratio Filter\n",
        "                ratios = calculate_ratio(point, ca_xyz, cb_xyz)\n",
        "                if np.isinf(ratios).any() or not np.all((thresholds_local['ratio'][0] <= ratios) & (ratios <= thresholds_local['ratio'][1])): continue\n",
        "\n",
        "                # Probability Score Filter\n",
        "                score = score_zn_predictions(current_dist_ca, current_dist_cb, angles, prob_map_3d_local, ca_bins_local, cb_bins_local, angle_bins_local)\n",
        "                if score <= 0: continue # Only consider positive scores\n",
        "\n",
        "                # Proximity Filter\n",
        "                if not proximity_filter_kdtree(kdtree, point, exclusion_radius=2.5): continue\n",
        "\n",
        "                # ***** MODIFICATION START *****\n",
        "                # Candidate passed all filters, check if it's the best score so far\n",
        "                if score > best_score:\n",
        "                    best_score = score\n",
        "                    best_coords = point # Store the numpy array\n",
        "                    best_angles = angles\n",
        "                # ***** MODIFICATION END *****\n",
        "                # Do NOT return here, continue searching the rest of the grid\n",
        "\n",
        "    # ***** MODIFICATION START *****\n",
        "    # After checking all points, return the best one found (or defaults if none found)\n",
        "    return best_coords, best_score, best_angles\n",
        "    # ***** MODIFICATION END *****\n",
        "\n",
        "\n",
        "# --- Worker Function for Multiprocessing ---\n",
        "# (process_single_site remains unchanged, it calls the modified estimate_zn_iterative)\n",
        "def process_single_site(args):\n",
        "    \"\"\"Worker function to process a single candidate site (row).\"\"\"\n",
        "    site_index, site_data_dict, structure_shared, thresholds_shared, \\\n",
        "    prob_map_3d_shared, ca_bins_shared, cb_bins_shared, angle_bins_shared = args\n",
        "    ca_cols = ['CA1_X', 'CA1_Y', 'CA1_Z', 'CA2_X', 'CA2_Y', 'CA2_Z', 'CA3_X', 'CA3_Y', 'CA3_Z']\n",
        "    cb_cols = ['CB1_X', 'CB1_Y', 'CB1_Z', 'CB2_X', 'CB2_Y', 'CB2_Z', 'CB3_X', 'CB3_Y', 'CB3_Z']\n",
        "    try:\n",
        "        ca_coords_flat = np.array([site_data_dict[col] for col in ca_cols], dtype=np.float64)\n",
        "        cb_coords_flat = np.array([site_data_dict[col] for col in cb_cols], dtype=np.float64)\n",
        "        # Call the modified estimate_zn_iterative which now finds the BEST score\n",
        "        zn_coords, zn_score, zn_angles = estimate_zn_iterative(\n",
        "            ca_coords_flat, cb_coords_flat, site_data_dict, structure_shared,\n",
        "            thresholds_shared, prob_map_3d_shared, ca_bins_shared,\n",
        "            cb_bins_shared, angle_bins_shared,\n",
        "            grid_resolution=0.2\n",
        "        )\n",
        "        return site_index, zn_coords, zn_score, zn_angles\n",
        "    except Exception as e:\n",
        "        return site_index, \"error\", 0, [np.nan, np.nan, np.nan]\n",
        "\n",
        "\n",
        "# --- Main Execution Guard ---\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        kst = pytz.timezone('Asia/Seoul')\n",
        "        current_time_kst = datetime.datetime.now(kst)\n",
        "        print(f\"\\n--- Starting Main Process for Single PDB Site Parallelization (Find Best Score) ---\") # Updated Title\n",
        "        print(f\"Current Time (KST): {current_time_kst.strftime('%Y-%m-%d %H:%M:%S %Z%z')}\")\n",
        "    except ImportError:\n",
        "        print(\"\\n--- Starting Main Process for Single PDB Site Parallelization (Find Best Score) ---\")\n",
        "        print(\"Note: Could not determine KST time (pytz not installed?). Run 'pip install pytz' if needed.\")\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    # --- Prepare Tasks for Multiprocessing ---\n",
        "    tasks = []\n",
        "    required_input_cols = (\n",
        "        ['CA1_X', 'CA1_Y', 'CA1_Z', 'CA2_X', 'CA2_Y', 'CA2_Z', 'CA3_X', 'CA3_Y', 'CA3_Z'] +\n",
        "        ['CB1_X', 'CB1_Y', 'CB1_Z', 'CB2_X', 'CB2_Y', 'CB2_Z', 'CB3_X', 'CB3_Y', 'CB3_Z'] +\n",
        "        ['Coord_residue_number1', 'Coord_residue_number2', 'Coord_residue_number3', 'PDB_ID']\n",
        "    )\n",
        "    if not all(col in df_sites.columns for col in required_input_cols):\n",
        "         missing_cols = [col for col in required_input_cols if col not in df_sites.columns]\n",
        "         raise ValueError(f\"Input coordinate Excel file is missing required columns: {missing_cols}\")\n",
        "    sites_data_list = df_sites[required_input_cols].to_dict('records')\n",
        "    for index, site_dict in enumerate(sites_data_list):\n",
        "        tasks.append((df_sites.index[index], site_dict, structure, thresholds, prob_map_3d, ca_bins, cb_bins, angle_bins))\n",
        "    print(f\"Prepared {len(tasks)} tasks for parallel processing.\")\n",
        "\n",
        "    # --- Initialize Results Columns ---\n",
        "    df_sites['Zn_X_Grid'] = np.nan; df_sites['Zn_Y_Grid'] = np.nan; df_sites['Zn_Z_Grid'] = np.nan\n",
        "    df_sites['Zn_Score'] = 0.0\n",
        "    df_sites['Angle_1'] = np.nan; df_sites['Angle_2'] = np.nan; df_sites['Angle_3'] = np.nan\n",
        "\n",
        "    num_processes = min(8, os.cpu_count()) # Adjust as needed\n",
        "    print(f\"\\nInitializing multiprocessing pool with {num_processes} workers...\")\n",
        "\n",
        "    processed_count = 0\n",
        "    pool = None\n",
        "    try:\n",
        "        pool = multiprocessing.Pool(processes=num_processes)\n",
        "        print(\"Starting parallel processing of sites...\")\n",
        "        results_iterator = pool.imap_unordered(process_single_site, tasks)\n",
        "        for result in results_iterator:\n",
        "            processed_count += 1\n",
        "            try:\n",
        "                site_idx, zn_coords_res, zn_score_res, zn_angles_res = result\n",
        "                df_sites.loc[site_idx, 'Zn_Score'] = zn_score_res\n",
        "                if isinstance(zn_coords_res, np.ndarray) and zn_coords_res.shape == (3,):\n",
        "                     df_sites.loc[site_idx, 'Zn_X_Grid'] = zn_coords_res[0]\n",
        "                     df_sites.loc[site_idx, 'Zn_Y_Grid'] = zn_coords_res[1]\n",
        "                     df_sites.loc[site_idx, 'Zn_Z_Grid'] = zn_coords_res[2]\n",
        "                if isinstance(zn_angles_res, (list, np.ndarray)) and len(zn_angles_res) == 3:\n",
        "                    df_sites.loc[site_idx, 'Angle_1'] = float(zn_angles_res[0]) if pd.notna(zn_angles_res[0]) else np.nan\n",
        "                    df_sites.loc[site_idx, 'Angle_2'] = float(zn_angles_res[1]) if pd.notna(zn_angles_res[1]) else np.nan\n",
        "                    df_sites.loc[site_idx, 'Angle_3'] = float(zn_angles_res[2]) if pd.notna(zn_angles_res[2]) else np.nan\n",
        "                if processed_count % 50 == 0 or processed_count == len(tasks): print(f\"  Processed {processed_count}/{len(tasks)} sites...\")\n",
        "            except Exception as result_error:\n",
        "                 print(f\"❌ Error processing result for one site: {result_error}\")\n",
        "                 if isinstance(result, tuple) and len(result) > 0: print(f\"   Problem occurred for site index: {result[0]}\")\n",
        "                 continue\n",
        "        print(f\"Finished processing all {processed_count} assigned tasks.\")\n",
        "    except Exception as pool_error:\n",
        "        print(f\"❌ An error occurred during multiprocessing: {pool_error}\")\n",
        "        print(traceback.format_exc())\n",
        "    finally:\n",
        "        if pool: pool.close(); pool.join()\n",
        "    print(\"\\n--- Multiprocessing Pool Finished ---\")\n",
        "\n",
        "    # --- Post-processing and Saving Results ---\n",
        "    print(\"Filtering results (Zn_Score > 0)...\")\n",
        "    df_sites['Zn_Score'] = pd.to_numeric(df_sites['Zn_Score'], errors='coerce').fillna(0.0)\n",
        "    df_output = df_sites[df_sites['Zn_Score'] > 0].copy()\n",
        "    total_sites_saved = len(df_output)\n",
        "    if not df_output.empty:\n",
        "        print(f\"Saving {total_sites_saved} site(s) with positive scores...\")\n",
        "        try:\n",
        "            for col in ['Zn_X_Grid', 'Zn_Y_Grid', 'Zn_Z_Grid', 'Angle_1', 'Angle_2', 'Angle_3']:\n",
        "                 if col in df_output.columns: df_output[col] = pd.to_numeric(df_output[col], errors='coerce')\n",
        "            df_output.to_excel(Output_result_excel_file, index=False)\n",
        "            print(f\"✅ Successfully saved results to: {Output_result_excel_file}\")\n",
        "        except Exception as save_error:\n",
        "            print(f\"❌ Error saving results to Excel file: {save_error}\")\n",
        "            print(traceback.format_exc())\n",
        "    else:\n",
        "        print(\"⚠️ No valid Zn predictions passed filters (Score > 0). No output file created.\")\n",
        "\n",
        "    end_time = time.time()\n",
        "    print(f\"\\n🏁 Processing summary:\")\n",
        "    print(f\"  Total input sites: {len(df_sites)}\")\n",
        "    print(f\"  Total sites processed by pool: {processed_count}\")\n",
        "    print(f\"  Total result sites saved (Score > 0): {total_sites_saved}\")\n",
        "    print(f\"  Total execution time: {end_time - start_time:.2f} seconds\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "GaiC_6jMWzXX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1EKdDrBMc3IQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8VwAlZ7nSEw",
        "outputId": "b63ae97b-404f-47e9-94d3-9a1d4e03fac9",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyMOL script saved to /content/1EP0_full_best_score.pml\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from Bio.PDB import PDBParser\n",
        "import requests\n",
        "\n",
        "# Markdown documentation for file pathways\n",
        "\n",
        "# @markdown # Step 5: Analysis the result (Apply to the PDB file)\n",
        "\n",
        "# Load input file\n",
        "input_file_path = \"/content/1EP0_full_best_score.xlsx\" # @param {type:\"string\"}\n",
        "df_new = pd.read_excel(input_file_path)\n",
        "\n",
        "# Generate PyMOL script file\n",
        "pymol_script_commands = []\n",
        "df_new['Combination_Number'] = range(1, len(df_new) + 1)\n",
        "\n",
        "# Generate the PyMOL script for both valid and invalid Zn binding forms\n",
        "for index, row in df_new.iterrows():\n",
        "    # Retrieve chain and residue information\n",
        "    chain1, res1 = row['Coord_chain_id_number1'], row['Coord_residue_number1']\n",
        "    chain2, res2 = row['Coord_chain_id_number2'], row['Coord_residue_number2']\n",
        "    chain3, res3 = row['Coord_chain_id_number3'], row['Coord_residue_number3']\n",
        "\n",
        "    # Retrieve Zn coordinates\n",
        "    zn_x, zn_y, zn_z = row['Zn_X_Grid'], row['Zn_Y_Grid'], row['Zn_Z_Grid']\n",
        "\n",
        "    selection_name = f\"obj{row['Combination_Number']:02d}\"\n",
        "\n",
        "    # Select the residues\n",
        "    pymol_script_commands.append(f\"select {selection_name}, (chain {chain1} and resi {res1}) or (chain {chain2} and resi {res2}) or (chain {chain3} and resi {res3})\")\n",
        "\n",
        "    # Create the objects for the residues\n",
        "    pymol_script_commands.append(f\"create {selection_name}_residue1, /{row['PDB_ID']}//{chain1}/{res1}\")\n",
        "    pymol_script_commands.append(f\"create {selection_name}_residue2, /{row['PDB_ID']}//{chain2}/{res2}\")\n",
        "    pymol_script_commands.append(f\"create {selection_name}_residue3, /{row['PDB_ID']}//{chain3}/{res3}\")\n",
        "\n",
        "    # Check if Zn coordinates are available\n",
        "    if not pd.isna(zn_x) and not pd.isna(zn_y) and not pd.isna(zn_z):\n",
        "        # Zn coordinates are present, add the Zn pseudoatom\n",
        "        zn_name = f\"{selection_name}_Metal\"\n",
        "        pymol_script_commands.append(f\"pseudoatom {zn_name}, pos=[{zn_x}, {zn_y}, {zn_z}], elem=Metal, name={zn_name}\")\n",
        "        pymol_script_commands.append(f\"show sphere, {zn_name}\")\n",
        "    else:\n",
        "        # Zn coordinates are missing, mark this combination as non-binding\n",
        "        pymol_script_commands.append(f\"# {selection_name} does not bind Zn\")\n",
        "\n",
        "# Save the commands into a PyMOL script\n",
        "pymol_script_file = \"/content/1EP0_full_best_score.pml\" # @param {type:\"string\"}\n",
        "with open(pymol_script_file, 'w') as f:\n",
        "    f.write(\"# PyMOL script for visualizing both Zn-binding and non-binding residue combinations\\n\\n\")\n",
        "    for command in pymol_script_commands:\n",
        "        f.write(command + '\\n')\n",
        "\n",
        "print(f\"PyMOL script saved to {pymol_script_file}\")"
      ]
    }
  ]
}